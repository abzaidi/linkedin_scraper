2025-02-11 00:43:47 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 00:43:47 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 00:43:47 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 00:43:47 [scrapy.extensions.telnet] INFO: Telnet Password: f985a3bc1756aa16
2025-02-11 00:43:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 00:43:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 00:43:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 00:43:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 00:43:47 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 00:43:47 [scrapy.core.engine] INFO: Spider opened
2025-02-11 00:43:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 00:43:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 00:43:56 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 00:45:21 [linkedin_job] INFO: Saved job search page as job_search_page.html
2025-02-11 00:46:01 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 00:47:07 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 00:47:57 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 00:48:51 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 00:49:06 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 00:50:16 [linkedin_job] INFO: Timeout: No job title found.
2025-02-11 00:50:30 [linkedin_job] INFO: Saved job page as job_page_1739217030.9372926.html
2025-02-11 00:50:45 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2025-02-11 00:50:52 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 1 items (at 0 items/min)
2025-02-11 00:51:03 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 00:52:42 [linkedin_job] INFO: Saved job page as job_page_1739217162.0795588.html
2025-02-11 00:53:01 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 1 pages/min), scraped 2 items (at 1 items/min)
2025-02-11 00:53:18 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 00:54:29 [linkedin_job] INFO: Saved job page as job_page_1739217269.6737442.html
2025-02-11 00:54:41 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 1 pages/min), scraped 3 items (at 1 items/min)
2025-02-11 00:54:47 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2025-02-11 00:54:58 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 00:56:14 [linkedin_job] INFO: Saved job page as job_page_1739217374.453386.html
2025-02-11 00:56:31 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 1 pages/min), scraped 4 items (at 1 items/min)
2025-02-11 00:56:47 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 00:58:04 [linkedin_job] INFO: Saved job page as job_page_1739217484.3461225.html
2025-02-11 00:58:17 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 1 pages/min), scraped 5 items (at 1 items/min)
2025-02-11 00:58:23 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.linkedin.com/jobs/view/4131239990/?eBP=CwEAAAGU8WQ-gXeifqr1KbT00jQkpen_vMain3MLhvqe06UfaOhq_dWhW7ftwWv-DmCLMyP22x6rDeiGUUbNYogFrLT7LHVXiHQlIFz9_Tu_8l4Tf8Dy1ymLPTAlKu1W81nHIdv47IHMlSibGx6O6_j8SnNj2e5m7AtI08NiArl87bfzb_gK1fTm_SAUq_AspKn3YICSPw2L0ac8pxkP0eyMRtudx2sJMVtOXBxd6Su34dJSrM2Nd5MucLT1G8WpSQlhkMnv2C_4QyctJFlRwnLkQhNkpyiVVHHc-KggN0-apMlmFtP8vre1fn4PeL-8wCCXvRnN-G5g10bJkq2gYlKhPgh2ZOPpRIeCACuao7MlgSVzi05eal4eYzkB5b6j1FklbvBqftOMTqA-EpRRhEg11ROz7NNHjzaNw3kaWix688XUnZGkluLqAaUJCJUmXBczXATp0JALu7uQP7hHMIZtogXHla3Vu_7DSM4EoA_r&refId=0Df6s9dIMjZ2ywbWy9T%2Bbw%3D%3D&trackingId=%2BHVjU0Z2KD9DJU8dBekukw%3D%3D&trk=flagship3_search_srp_jobs. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2025-02-11 00:58:34 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 01:00:02 [linkedin_job] INFO: Timeout: No job title found.
2025-02-11 01:00:19 [linkedin_job] INFO: Saved job page as job_page_1739217619.252197.html
2025-02-11 01:00:36 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 1 pages/min), scraped 6 items (at 1 items/min)
2025-02-11 01:00:52 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 01:01:55 [linkedin_job] INFO: Saved job page as job_page_1739217715.5048122.html
2025-02-11 01:02:06 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 1 pages/min), scraped 7 items (at 1 items/min)
2025-02-11 01:02:21 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 01:03:52 [linkedin_job] INFO: Saved job page as job_page_1739217832.4565005.html
2025-02-11 01:04:10 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 1 pages/min), scraped 8 items (at 1 items/min)
2025-02-11 01:04:19 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-11 01:04:19 [scrapy.extensions.feedexport] INFO: Stored json feed (8 items) in: output2.json
2025-02-11 01:04:19 [scrapy.extensions.feedexport] INFO: Stored csv feed (8 items) in: linkedin_scraper/data/linkedin_profiles.csv
2025-02-11 01:04:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 12341,
 'downloader/request_count': 11,
 'downloader/request_method_count/GET': 11,
 'downloader/response_bytes': 1338553,
 'downloader/response_count': 9,
 'downloader/response_status_count/200': 9,
 'elapsed_time_seconds': 1232.333367,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 10, 20, 4, 19, 549078, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 11549321,
 'httpcompression/response_count': 9,
 'item_scraped_count': 8,
 'items_per_minute': None,
 'log_count/INFO': 46,
 'log_count/WARNING': 1,
 'memusage/max': 139255808,
 'memusage/startup': 73932800,
 'request_depth_max': 1,
 'response_received_count': 9,
 'responses_per_minute': None,
 'retry/count': 2,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'scheduler/dequeued': 11,
 'scheduler/dequeued/memory': 11,
 'scheduler/enqueued': 11,
 'scheduler/enqueued/memory': 11,
 'start_time': datetime.datetime(2025, 2, 10, 19, 43, 47, 215711, tzinfo=datetime.timezone.utc)}
2025-02-11 01:04:19 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-11 01:10:02 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 01:10:02 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 01:10:02 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 01:10:02 [scrapy.extensions.telnet] INFO: Telnet Password: 4d41572e8ac5a51f
2025-02-11 01:10:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 01:10:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 01:10:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 01:10:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 01:10:02 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 01:10:02 [scrapy.core.engine] INFO: Spider opened
2025-02-11 01:10:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 01:10:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 01:10:14 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 01:12:06 [linkedin_job] INFO: Timeout: No job links found within 15 seconds.
2025-02-11 01:12:26 [linkedin_job] INFO: Saved job search page as job_search_page.html
2025-02-11 01:13:05 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 01:13:06 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 01:13:06 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 01:13:06 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: output3.json
2025-02-11 01:13:06 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/linkedin_profiles.csv
2025-02-11 01:13:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 467,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 151466,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 183.806305,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2025, 2, 10, 20, 13, 6, 26071, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1304689,
 'httpcompression/response_count': 1,
 'items_per_minute': None,
 'log_count/INFO': 17,
 'memusage/max': 123002880,
 'memusage/startup': 73936896,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 10, 20, 10, 2, 219766, tzinfo=datetime.timezone.utc)}
2025-02-11 01:13:06 [scrapy.core.engine] INFO: Spider closed (shutdown)
2025-02-11 01:13:10 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 01:13:10 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 01:13:10 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 01:13:10 [scrapy.extensions.telnet] INFO: Telnet Password: d43c5ec9de2a9702
2025-02-11 01:13:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 01:13:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 01:13:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 01:13:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 01:13:10 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 01:13:10 [scrapy.core.engine] INFO: Spider opened
2025-02-11 01:13:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 01:13:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 01:13:20 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 01:14:53 [linkedin_job] INFO: Timeout: No job links found within 15 seconds.
2025-02-11 01:15:08 [linkedin_job] INFO: Saved job search page as job_search_page.html
2025-02-11 01:15:37 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 01:15:37 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-11 01:15:37 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: output3.json
2025-02-11 01:15:37 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/linkedin_profiles.csv
2025-02-11 01:15:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 467,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 151592,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 146.536372,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 10, 20, 15, 37, 410312, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1304677,
 'httpcompression/response_count': 1,
 'items_per_minute': None,
 'log_count/INFO': 16,
 'memusage/max': 122830848,
 'memusage/startup': 73801728,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 10, 20, 13, 10, 873940, tzinfo=datetime.timezone.utc)}
2025-02-11 01:15:37 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-11 01:17:49 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 01:17:49 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 01:20:07 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 01:20:07 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 01:20:07 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 01:20:07 [scrapy.extensions.telnet] INFO: Telnet Password: c82aee8ca7fcd8e5
2025-02-11 01:20:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 01:20:07 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 01:20:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 01:20:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 01:20:07 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 01:20:07 [scrapy.core.engine] INFO: Spider opened
2025-02-11 01:20:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 01:20:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 01:20:15 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 01:21:40 [linkedin_job] INFO: Timeout: No job links found within 15 seconds.
2025-02-11 01:21:51 [linkedin_job] INFO: Saved job search page as job_search_page.html
2025-02-11 01:22:13 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 01:22:13 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-11 01:22:13 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: output3.json
2025-02-11 01:22:13 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/linkedin_profiles.csv
2025-02-11 01:22:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 471,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 151601,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 126.105587,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 10, 20, 22, 13, 723490, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1304659,
 'httpcompression/response_count': 1,
 'items_per_minute': None,
 'log_count/INFO': 16,
 'memusage/max': 122884096,
 'memusage/startup': 73801728,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 10, 20, 20, 7, 617903, tzinfo=datetime.timezone.utc)}
2025-02-11 01:22:13 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-11 01:29:39 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 01:29:39 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 01:29:39 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 01:29:39 [scrapy.extensions.telnet] INFO: Telnet Password: aa16e7babbcacee2
2025-02-11 01:29:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 01:29:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 01:29:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 01:29:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 01:29:39 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 01:29:39 [scrapy.core.engine] INFO: Spider opened
2025-02-11 01:29:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 01:29:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 01:29:47 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 01:31:56 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 01:32:45 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 01:33:58 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 01:34:38 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 01:35:59 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2025-02-11 01:36:15 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 01:37:57 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 1 pages/min), scraped 2 items (at 1 items/min)
2025-02-11 01:38:05 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.linkedin.com/jobs/view/4001451324/?eBP=CwEAAAGU8Y4igMFW79pv4uYQPH9qlsV4XZo2rNrL1uslmycjRsc5CHv7QFsmLpIbDNy8v466jATTZ4DL5XDEg8xgoZ8OgBK7th5k-9M1cMssZNDZ9aXk-dgK7tBrdWYxhbzdzxCZHyBBQtiXyi2tf2OrhD-hQORcJXO9W2fAxtyg1EH0On94URFXfN9F6H4a1vC5dDWMpFYnyFUHo9HYydsRIoI5zhJT7IxErQpx3eMRQtto8YpeMCibRbInl46DKkXEjhElH7UgTZtb7s1lLLqCH0rj7IsG1CWblC1_lzOm6ZRAaJ_Ce0Ejsj8dU2ZECpQKEsVpt3y8qrvOQauqUdy1wsDjX4jR1XFB8PmHft8Wpk6jnVlrUZODrh-LG6oGjCd6V37nJi9JyjRn-OPKIw9TqVwHGo0YPOyU8YomVGCEi7XIWkRZ_hLWDaMCuj8CtHfyNVMw_DgaVDC8LCo&refId=foWBN7kbGN2Hf39umxU1eg%3D%3D&trackingId=fcK%2Bjjg6CQh8kw2KCi%2FKgg%3D%3D&trk=flagship3_search_srp_jobs. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2025-02-11 01:38:13 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 01:39:33 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 1 pages/min), scraped 3 items (at 1 items/min)
2025-02-11 01:39:39 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 0 pages/min), scraped 3 items (at 0 items/min)
2025-02-11 01:39:47 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 01:41:07 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 1 pages/min), scraped 4 items (at 1 items/min)
2025-02-11 01:41:20 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 01:42:46 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 1 pages/min), scraped 5 items (at 1 items/min)
2025-02-11 01:43:00 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 01:44:25 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 1 pages/min), scraped 6 items (at 1 items/min)
2025-02-11 01:44:43 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 01:46:36 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 1 pages/min), scraped 7 items (at 1 items/min)
2025-02-11 01:46:46 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 7 items (at 0 items/min)
2025-02-11 01:46:46 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-11 01:46:46 [scrapy.extensions.feedexport] INFO: Stored json feed (7 items) in: output3.json
2025-02-11 01:46:46 [scrapy.extensions.feedexport] INFO: Stored csv feed (7 items) in: linkedin_scraper/data/linkedin_profiles.csv
2025-02-11 01:46:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 1,
 'downloader/request_bytes': 9808,
 'downloader/request_count': 9,
 'downloader/request_method_count/GET': 9,
 'downloader/response_bytes': 1198750,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 8,
 'elapsed_time_seconds': 1027.259152,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 10, 20, 46, 46, 610624, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 10359720,
 'httpcompression/response_count': 8,
 'item_scraped_count': 7,
 'items_per_minute': None,
 'log_count/INFO': 32,
 'log_count/WARNING': 1,
 'memusage/max': 133578752,
 'memusage/startup': 73801728,
 'request_depth_max': 1,
 'response_received_count': 8,
 'responses_per_minute': None,
 'retry/count': 1,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 1,
 'scheduler/dequeued': 9,
 'scheduler/dequeued/memory': 9,
 'scheduler/enqueued': 9,
 'scheduler/enqueued/memory': 9,
 'start_time': datetime.datetime(2025, 2, 10, 20, 29, 39, 351472, tzinfo=datetime.timezone.utc)}
2025-02-11 01:46:46 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-11 18:49:43 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 18:49:43 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 18:49:43 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 18:49:43 [scrapy.extensions.telnet] INFO: Telnet Password: 9d48e95efc5d01b4
2025-02-11 18:49:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 18:49:43 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 18:49:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 18:49:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 18:49:43 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 18:49:43 [scrapy.core.engine] INFO: Spider opened
2025-02-11 18:49:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 18:49:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 18:49:55 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 18:50:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 115, in parse
    self.scroll_to_load_jobs(driver, max_scrolls=5)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 287, in scroll_to_load_jobs
    job_list_container.send_keys(Keys.END)
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webelement.py", line 301, in send_keys
    self._execute(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webelement.py", line 570, in _execute
    return self._parent.execute(command, params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 429, in execute
    self.error_handler.check_response(response)
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py", line 232, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.ElementNotInteractableException: Message: element not interactable
  (Session info: chrome=133.0.6943.53)
Stacktrace:
#0 0x5b096258009a <unknown>
#1 0x5b0962047703 <unknown>
#2 0x5b0962092c3c <unknown>
#3 0x5b09620910e8 <unknown>
#4 0x5b09620be982 <unknown>
#5 0x5b096208c50a <unknown>
#6 0x5b09620beb4e <unknown>
#7 0x5b09620e4b8b <unknown>
#8 0x5b09620be753 <unknown>
#9 0x5b096208a38e <unknown>
#10 0x5b096208bb51 <unknown>
#11 0x5b096254976b <unknown>
#12 0x5b096254d6f2 <unknown>
#13 0x5b09625358fc <unknown>
#14 0x5b096254e2e4 <unknown>
#15 0x5b09625199cf <unknown>
#16 0x5b096256ecd8 <unknown>
#17 0x5b096256eeb6 <unknown>
#18 0x5b096257ef16 <unknown>
#19 0x7eeaf629caa4 <unknown>
#20 0x7eeaf6329c3c <unknown>

2025-02-11 18:50:31 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-11 18:50:31 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: output4.json
2025-02-11 18:50:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 434,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 151465,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 47.973207,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 11, 13, 50, 31, 570709, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1304110,
 'httpcompression/response_count': 1,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 12,
 'memusage/max': 73220096,
 'memusage/startup': 73220096,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ElementNotInteractableException': 1,
 'start_time': datetime.datetime(2025, 2, 11, 13, 49, 43, 597502, tzinfo=datetime.timezone.utc)}
2025-02-11 18:50:31 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-11 18:57:33 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 18:57:33 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 18:57:33 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 18:57:33 [scrapy.extensions.telnet] INFO: Telnet Password: a683a99b252ee47e
2025-02-11 18:57:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 18:57:33 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 18:57:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 18:57:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 18:57:33 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 18:57:33 [scrapy.core.engine] INFO: Spider opened
2025-02-11 18:57:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 18:57:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 18:57:37 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 18:59:15 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 18:59:37 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 19:00:04 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 19:00:04 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-11 19:00:08 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x727ab1247650>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/6c0638a67d77bf7953e56e0bf059b5b8/execute/sync
2025-02-11 19:00:08 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x727ab12614f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/6c0638a67d77bf7953e56e0bf059b5b8/execute/sync
2025-02-11 19:00:08 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x727ab1261850>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/6c0638a67d77bf7953e56e0bf059b5b8/execute/sync
2025-02-11 19:00:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x727ab1261a30>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 158, in parse
    self.human_scroll(driver)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 241, in human_scroll
    driver.execute_script("window.scrollBy(0, 500);")  # Scroll down
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=57631): Max retries exceeded with url: /session/6c0638a67d77bf7953e56e0bf059b5b8/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x727ab1261a30>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 19:00:08 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 19:05:28 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 19:05:28 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 19:05:28 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 19:05:28 [scrapy.extensions.telnet] INFO: Telnet Password: 4cdb51bc8aa4a436
2025-02-11 19:05:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2025-02-11 19:05:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOGSTATS_INTERVAL': 0,
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 19:05:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 19:05:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 19:05:28 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 19:05:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 19:05:28 [scrapy.core.engine] INFO: Spider opened
2025-02-11 19:05:34 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 19:05:34 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 19:05:34 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 19:05:34 [scrapy.extensions.telnet] INFO: Telnet Password: ec38772d9b4a410b
2025-02-11 19:05:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2025-02-11 19:05:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOGSTATS_INTERVAL': 0,
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 19:05:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 19:05:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 19:05:34 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 19:05:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 19:13:26 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 19:13:26 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 19:13:26 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 19:13:26 [scrapy.extensions.telnet] INFO: Telnet Password: 0efd089391d1403e
2025-02-11 19:13:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 19:13:26 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 19:13:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 19:13:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 19:13:26 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 19:13:26 [scrapy.core.engine] INFO: Spider opened
2025-02-11 19:13:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 19:13:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 19:13:29 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 19:15:05 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 19:15:07 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-11 19:15:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e299ea979b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/1551c9f109fa6f84260ec32b94c08cff/execute/sync
2025-02-11 19:15:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e299ea97620>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/1551c9f109fa6f84260ec32b94c08cff/execute/sync
2025-02-11 19:15:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e299eab0290>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/1551c9f109fa6f84260ec32b94c08cff/execute/sync
2025-02-11 19:15:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7e299eab0470>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 158, in parse
    self.human_scroll(driver)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 241, in human_scroll
    driver.execute_script("window.scrollBy(0, 500);")  # Scroll down
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=49475): Max retries exceeded with url: /session/1551c9f109fa6f84260ec32b94c08cff/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e299eab0470>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 19:15:09 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 19:15:09 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 19:19:24 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 19:19:24 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 19:19:24 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 19:19:24 [scrapy.extensions.telnet] INFO: Telnet Password: 0ae18feeef5a935e
2025-02-11 19:19:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 19:19:24 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 19:19:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 19:19:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 19:19:24 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 19:19:24 [scrapy.core.engine] INFO: Spider opened
2025-02-11 19:19:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 19:19:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 19:19:28 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 19:20:50 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 19:20:50 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-11 19:20:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c907efb7fe0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/d312259759547980e7a0d6ab34c4f11e/execute/sync
2025-02-11 19:20:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c907efb76e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/d312259759547980e7a0d6ab34c4f11e/execute/sync
2025-02-11 19:20:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c907efd4110>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/d312259759547980e7a0d6ab34c4f11e/execute/sync
2025-02-11 19:20:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7c907efd43e0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 158, in parse
    self.human_scroll(driver)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 241, in human_scroll
    driver.execute_script("window.scrollBy(0, 500);")  # Scroll down
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=38571): Max retries exceeded with url: /session/d312259759547980e7a0d6ab34c4f11e/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c907efd43e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 19:20:50 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 19:20:50 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 19:43:02 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 19:43:02 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 19:43:02 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 19:43:02 [scrapy.extensions.telnet] INFO: Telnet Password: d02af74f3225a25a
2025-02-11 19:43:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 19:43:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 19:43:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 19:43:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 19:43:02 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 19:43:02 [scrapy.core.engine] INFO: Spider opened
2025-02-11 19:43:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 19:43:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 19:43:07 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 19:44:36 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 19:45:18 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 19:46:18 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 19:47:22 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 19:48:19 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 19:48:57 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 19:50:08 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea1b620>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:08 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea1b6b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:08 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3c080>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3c590>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3c7a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3c9e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:11 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea1bf80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:11 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea1b1a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:11 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea1b5f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:12 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3c1a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:12 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3ce90>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:12 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3d0d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3d610>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3d820>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3da60>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:14 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3dfa0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:14 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cf4f1190>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:14 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea1b860>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/actions
2025-02-11 19:50:15 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 1 items (at 1 items/min)
2025-02-11 19:50:19 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3da60>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/execute/sync
2025-02-11 19:50:19 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3d7f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/execute/sync
2025-02-11 19:50:19 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3c290>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e0131592d6437d5964a143e015529d/execute/sync
2025-02-11 19:50:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x79a9cea3d2b0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 157, in parse
    self.human_scroll(driver)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 240, in human_scroll
    driver.execute_script("window.scrollBy(0, 500);")  # Scroll down
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=51965): Max retries exceeded with url: /session/c4e0131592d6437d5964a143e015529d/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3d2b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 19:50:21 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 19:51:07 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 1 pages/min), scraped 2 items (at 1 items/min)
2025-02-11 19:51:13 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 19:51:49 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 19:51:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea1a5a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/72166767b577bc92054584aaff45d259/actions
2025-02-11 19:51:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea182c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/72166767b577bc92054584aaff45d259/actions
2025-02-11 19:51:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea19340>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/72166767b577bc92054584aaff45d259/actions
2025-02-11 19:51:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea1a840>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/72166767b577bc92054584aaff45d259/actions
2025-02-11 19:51:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea1a1b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/72166767b577bc92054584aaff45d259/actions
2025-02-11 19:51:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea19760>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/72166767b577bc92054584aaff45d259/actions
2025-02-11 19:51:52 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea19f70>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/72166767b577bc92054584aaff45d259/element
2025-02-11 19:51:52 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea18a10>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/72166767b577bc92054584aaff45d259/element
2025-02-11 19:51:52 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea18800>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/72166767b577bc92054584aaff45d259/element
2025-02-11 19:51:52 [linkedin_job] INFO: Timeout: No job title found.
2025-02-11 19:51:54 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-11 19:51:56 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3d7f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/72166767b577bc92054584aaff45d259/source
2025-02-11 19:51:56 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3d5e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/72166767b577bc92054584aaff45d259/source
2025-02-11 19:51:56 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3d070>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/72166767b577bc92054584aaff45d259/source
2025-02-11 19:51:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4147195577/?eBP=CwEAAAGU9XbypnSvi21a5gHNSasi-V2hdQlADIuAmvRtwpDD5WkgcyMbQMJun8XXGfdydB9Bq84Zc-doPeBNEzoAmB1E3I-LhzbUH9r9NUrAVfWdgaEYXuVzrGni8xO8j7Fpi2qgwgpvCweiQSKSuzSK_Mmp8iZoMOVRQwlSE9yPcZul7gf-du42u_OTJODTf1PCCUdLgof0HbDGgCWRFpN1pNiRaRarvTSa0W1YC9oilVPPcsE5GmoNwjYEm47bHddyZlxCUhfmSqJefBIUpw3q_D_v9zNUJRBS0eoJr-OJEa91zCbrLIoJBQ_zRO5hipEpeOiKvvqkHYJ05UslsBddxQCi-ivVA0xe6k7FC_6Dcq0rMR_NO8iR35H-0vWcmSjoUORy01lJ1A1CHKnOYJ2pGZDokadxQyzLWSxxAoFPsEPAOf6t2ssecPuF-1iCnIgfg4oasgnrqMVleCOGsaP5y0vmv22eGM-27jPdPWDx6uLsjZEGBojRwNGgNurweONn7MmUg8pgJpBzOrhbEsTN47Rc&refId=2uFLNlrajn%2FmWkPfrJbrdA%3D%3D&trackingId=ljUyjozEtzy3gG%2Fj9P9SQA%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x79a9cea3df10>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 204, in parse_job_page
    job_page_html = driver.page_source
                    ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=47153): Max retries exceeded with url: /session/72166767b577bc92054584aaff45d259/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79a9cea3df10>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 19:51:56 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 19:51:56 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.linkedin.com/jobs/view/4083811055/?eBP=CwEAAAGU9Xbypgx4MN8HVL3wHb4Q-kDMLrBLlv9i-QYHR8znyoCD_7eHJLlQPsJiKwRm1EJMS1aO0aB8kcWgsLjpX83tGa8Mjnr5nDVzFsR-9HXk6gbkJGl9Fq9Obcf01xkY0GDeJoFx93-8lwF6w2nDyU8-iDRdqXpy4oPFIupM0yJxTF0-niEoNWpM_xp7A-49ml-ppY2WjeZ4KxJ2nnWgQE5NiPJRWAOQXLo5inRFZHtPv5zqLfauJRGVbpMijd4JN6Tpqj6JkbBTLEr8D1EpgxaYKbprrXd0HLEMwMVSk_SZ7VT5C9DBhnTQpI3IYoqUHRkp_NrH0eUx_jcQaPsdfpSkm6FiX0Aga-rLqf98tFQ5YpOUa58eo9TsN0bvHHPC8tqwW7cosw3XA57wNqQxb68zWvL8UW9SuJzmbw-sYcJva5l1ahbuSSjj6noLO308Jf8pXk-IH_KecihWXCvYPdrxcwSMTjqcSW7WbgICsfyMgplOBaKcS993ag&refId=2uFLNlrajn%2FmWkPfrJbrdA%3D%3D&trackingId=nDWMERKncRakh1IqaFqk0Q%3D%3D&trk=flagship3_search_srp_jobs. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2025-02-11 19:55:01 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 19:55:02 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 19:55:02 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 19:55:02 [scrapy.extensions.telnet] INFO: Telnet Password: c1d6369e95373ef1
2025-02-11 19:55:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 19:55:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 19:55:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 19:55:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 19:55:02 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 19:55:02 [scrapy.core.engine] INFO: Spider opened
2025-02-11 19:55:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 19:55:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 19:55:04 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 19:55:47 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 19:55:47 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-11 19:55:48 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ec95b865cd0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f10b3ed11d8ef70b8b56d071b18e9627/execute/sync
2025-02-11 19:55:48 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ec95b094470>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f10b3ed11d8ef70b8b56d071b18e9627/execute/sync
2025-02-11 19:55:48 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ec95b0940b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f10b3ed11d8ef70b8b56d071b18e9627/execute/sync
2025-02-11 19:55:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7ec95b094710>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 156, in parse
    self.human_scroll(driver)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 239, in human_scroll
    driver.execute_script("window.scrollBy(0, 500);")  # Scroll down
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=58655): Max retries exceeded with url: /session/f10b3ed11d8ef70b8b56d071b18e9627/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ec95b094710>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 19:55:48 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 19:56:11 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 19:56:11 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 19:56:11 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 19:56:11 [scrapy.extensions.telnet] INFO: Telnet Password: b8c97daef58b3c51
2025-02-11 19:56:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 19:56:11 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 19:56:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 19:56:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 19:56:11 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 19:56:11 [scrapy.core.engine] INFO: Spider opened
2025-02-11 19:56:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 19:56:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 19:56:14 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 19:57:14 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 19:57:14 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-11 19:57:16 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x77fc10447f20>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/b1b841ab8d51475b8f2eed576ba76042/execute/sync
2025-02-11 19:57:16 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x77fc104601a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/b1b841ab8d51475b8f2eed576ba76042/execute/sync
2025-02-11 19:57:16 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x77fc10460410>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/b1b841ab8d51475b8f2eed576ba76042/execute/sync
2025-02-11 19:57:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x77fc10460650>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 156, in parse
    self.human_scroll(driver)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 239, in human_scroll
    driver.execute_script("window.scrollBy(0, 500);")  # Scroll down
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56587): Max retries exceeded with url: /session/b1b841ab8d51475b8f2eed576ba76042/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x77fc10460650>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 19:57:16 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 19:57:16 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 19:58:21 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 19:58:21 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 19:58:21 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 19:58:21 [scrapy.extensions.telnet] INFO: Telnet Password: f46ec61408d6afaf
2025-02-11 19:58:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 19:58:21 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 19:58:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 19:58:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 19:58:21 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 19:58:21 [scrapy.core.engine] INFO: Spider opened
2025-02-11 19:58:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 19:58:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 19:58:25 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 19:59:29 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 19:59:29 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-11 19:59:31 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b09f1dbf7a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/4f5ca720c8533f8fd851a5a0b346d789/execute/sync
2025-02-11 19:59:31 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b09f1dd84a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/4f5ca720c8533f8fd851a5a0b346d789/execute/sync
2025-02-11 19:59:31 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b09f1dd8200>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/4f5ca720c8533f8fd851a5a0b346d789/execute/sync
2025-02-11 19:59:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7b09f1dd86e0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 156, in parse
    self.human_scroll(driver)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 239, in human_scroll
    driver.execute_script("window.scrollBy(0, 500);")  # Scroll down
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=33097): Max retries exceeded with url: /session/4f5ca720c8533f8fd851a5a0b346d789/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b09f1dd86e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 19:59:31 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 19:59:31 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 20:00:51 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 20:00:51 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 20:00:51 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 20:00:51 [scrapy.extensions.telnet] INFO: Telnet Password: dbd3130dd98aded8
2025-02-11 20:00:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 20:00:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 20:00:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 20:00:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 20:00:51 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 20:00:51 [scrapy.core.engine] INFO: Spider opened
2025-02-11 20:00:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:00:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 20:00:54 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 20:01:44 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 20:01:44 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-11 20:01:44 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a15a306b860>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/0aeabedfb85ac5e0c546361b78a56e04/execute/sync
2025-02-11 20:01:44 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a15a3084440>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/0aeabedfb85ac5e0c546361b78a56e04/execute/sync
2025-02-11 20:01:44 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a15a30841a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/0aeabedfb85ac5e0c546361b78a56e04/execute/sync
2025-02-11 20:01:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7a15a30845f0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 156, in parse
    self.human_scroll(driver)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 239, in human_scroll
    driver.execute_script("window.scrollBy(0, 500);")  # Scroll down
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=34365): Max retries exceeded with url: /session/0aeabedfb85ac5e0c546361b78a56e04/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a15a30845f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 20:01:44 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 20:03:32 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 20:03:32 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 20:03:32 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 20:03:32 [scrapy.extensions.telnet] INFO: Telnet Password: d42ae06a32318696
2025-02-11 20:03:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 20:03:32 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 20:03:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 20:03:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 20:03:32 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 20:03:32 [scrapy.core.engine] INFO: Spider opened
2025-02-11 20:03:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:03:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-11 20:03:35 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 20:04:28 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 20:04:28 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-11 20:04:32 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7dde50e5fbc0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/035c730e09950a974dc12bcf9a2980ac/execute/sync
2025-02-11 20:04:32 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7dde50e7c140>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/035c730e09950a974dc12bcf9a2980ac/execute/sync
2025-02-11 20:04:32 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7dde50e7c380>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/035c730e09950a974dc12bcf9a2980ac/execute/sync
2025-02-11 20:04:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7dde50e7c5c0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 156, in parse
    self.human_scroll(driver)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 239, in human_scroll
    driver.execute_script("window.scrollBy(0, 500);")  # Scroll down
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=42581): Max retries exceeded with url: /session/035c730e09950a974dc12bcf9a2980ac/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7dde50e7c5c0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 20:04:32 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 20:11:19 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 20:11:19 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 20:11:19 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 20:11:19 [scrapy.extensions.telnet] INFO: Telnet Password: 7c7d1ec6e212f19b
2025-02-11 20:11:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 20:11:19 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 20:11:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 20:11:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 20:11:19 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 20:11:19 [scrapy.core.engine] INFO: Spider opened
2025-02-11 20:11:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:11:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-11 20:11:23 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 20:12:43 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:13:27 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:14:09 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 20:14:20 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 20:14:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4131737705/?eBP=CwEAAAGU9ZDWqa4d6T93pHIZlxdxQZHe4mysTdQXrgkPpWaUpVytuDtz3usdQtrFCeKaHdpPafpXTVE8Msfc4GWy8hK9hZJAvkUd7PHPWSEHEkQOlpPZSALDdcVSpvG_V9bFHIDFI8PWftxTSnwk3qDX6ZnWFfZmcJVzsSVtVM3M0MHZzqhU_xXyQ3rV5sMQX3J8PY-M0gXfV1FJnAy-8dliWQFAF55TcnDgH5q6MrUwxmoI9RPdBdyswqTZ8qqJiOPa1hTUqsqaiKOQ8fhHfKCeY49ou_mbR5HlYokcuH2GFFhO9RpNjuKkAfy8qcUn8juNIqBOzoI6zt177WE1a_SJjeqrxO4lGS7bJDy4RWLjdd0kB1aLkrKz2Mr1fEGk-gzxUrBA8-pIht2Ia1UAmg84cukICdCJ7yBuYeZe3ALk9jvvWvv22RIXnaUgPW98HtLeh23JCgMDgNuoyd8TTqEu&refId=a%2BzUi32VCrYA50XWdpH0vA%3D%3D&trackingId=4o9Euqd9jK0PTECgkvD82w%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 185, in parse_job_page
    driver.get(response.url)
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 665, in get
    return super().get(url)
           ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 454, in get
    self.execute(Command.GET, {"url": url})
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/util.py", line 38, in reraise
    raise value.with_traceback(tb)
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2025-02-11 20:14:20 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:14:20 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 20:14:20 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-11 20:14:21 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 20:25:01 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 20:25:01 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 20:25:01 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 20:25:01 [scrapy.extensions.telnet] INFO: Telnet Password: db8b17245dd88c61
2025-02-11 20:25:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 20:25:01 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 20:25:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 20:25:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 20:25:01 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 20:25:01 [scrapy.core.engine] INFO: Spider opened
2025-02-11 20:25:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:25:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-11 20:25:05 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 20:26:14 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:27:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:27:11 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 20:27:11 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-11 20:27:14 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7321d608fb30>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/28def8675922649585483d3246df3f68/execute/sync
2025-02-11 20:27:14 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7321d60a4830>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/28def8675922649585483d3246df3f68/execute/sync
2025-02-11 20:27:14 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7321d60a5a30>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/28def8675922649585483d3246df3f68/execute/sync
2025-02-11 20:27:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7321d60a5c40>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 143, in parse
    self.human_scroll(driver)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 227, in human_scroll
    driver.execute_script("window.scrollBy(0, 500);")  # Scroll down
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=58687): Max retries exceeded with url: /session/28def8675922649585483d3246df3f68/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7321d60a5c40>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 20:27:14 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 20:29:31 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 20:29:31 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 20:29:31 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 20:29:31 [scrapy.extensions.telnet] INFO: Telnet Password: 09323d68b9ba3093
2025-02-11 20:29:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 20:29:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 20:29:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 20:29:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 20:29:31 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 20:29:31 [scrapy.core.engine] INFO: Spider opened
2025-02-11 20:29:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:29:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-11 20:29:34 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 20:30:24 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 20:30:24 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-11 20:30:26 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a1f7a763d70>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/b677d78797310e5c8038979a1e908960/window/rect
2025-02-11 20:30:26 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a1f7a780a70>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/b677d78797310e5c8038979a1e908960/window/rect
2025-02-11 20:30:26 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a1f7a7816a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/b677d78797310e5c8038979a1e908960/window/rect
2025-02-11 20:30:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7a1f7a781850>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 144, in parse
    self.human_mouse_movements(driver)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 235, in human_mouse_movements
    window_size = driver.get_window_size()
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 1032, in get_window_size
    size = self.get_window_rect()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 1083, in get_window_rect
    return self.execute(Command.GET_WINDOW_RECT)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=35631): Max retries exceeded with url: /session/b677d78797310e5c8038979a1e908960/window/rect (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a1f7a781850>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 20:30:26 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 20:39:35 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 20:39:35 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 20:39:35 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 20:39:35 [scrapy.extensions.telnet] INFO: Telnet Password: f4292db9ed49c155
2025-02-11 20:39:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 20:39:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 20:39:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 20:39:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 20:39:35 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 20:39:35 [scrapy.core.engine] INFO: Spider opened
2025-02-11 20:39:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:39:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-11 20:39:38 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 20:40:37 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 20:40:37 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7559b5eb7c50>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/2e6ea9246f017a284314c38a049ffe55/actions
2025-02-11 20:40:37 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7559b5ecd3a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/2e6ea9246f017a284314c38a049ffe55/actions
2025-02-11 20:40:37 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7559b5ecd640>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/2e6ea9246f017a284314c38a049ffe55/actions
2025-02-11 20:40:38 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-11 20:40:38 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7559b5ecdc40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/2e6ea9246f017a284314c38a049ffe55/actions
2025-02-11 20:40:38 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7559b5ecde50>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/2e6ea9246f017a284314c38a049ffe55/actions
2025-02-11 20:40:38 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7559b5ece090>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/2e6ea9246f017a284314c38a049ffe55/actions
2025-02-11 20:40:38 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:40:38 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7559b5ece600>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/2e6ea9246f017a284314c38a049ffe55/execute/sync
2025-02-11 20:40:38 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7559b5ece7e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/2e6ea9246f017a284314c38a049ffe55/execute/sync
2025-02-11 20:40:38 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7559b5ece9c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/2e6ea9246f017a284314c38a049ffe55/execute/sync
2025-02-11 20:40:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7559b5ecec00>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 145, in parse
    self.human_scroll(driver)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 229, in human_scroll
    driver.execute_script("window.scrollBy(0, 500);")  # Scroll down
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=50483): Max retries exceeded with url: /session/2e6ea9246f017a284314c38a049ffe55/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7559b5ecec00>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 20:40:38 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 20:50:49 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 20:50:49 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 20:50:49 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 20:50:49 [scrapy.extensions.telnet] INFO: Telnet Password: 0eec1ac39d92ab58
2025-02-11 20:50:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 20:50:49 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 20:50:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 20:50:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 20:50:49 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 20:50:49 [scrapy.core.engine] INFO: Spider opened
2025-02-11 20:50:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:50:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-11 20:50:52 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 20:52:15 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 20:52:16 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-11 20:52:18 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x778532663860>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/837f6806bc1cf566e7de3a8728f95604/execute/sync
2025-02-11 20:52:18 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x778532663980>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/837f6806bc1cf566e7de3a8728f95604/execute/sync
2025-02-11 20:52:18 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x778532698110>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/837f6806bc1cf566e7de3a8728f95604/execute/sync
2025-02-11 20:52:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x778532698350>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 145, in parse
    self.human_scroll(driver)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 229, in human_scroll
    driver.execute_script("window.scrollBy(0, 500);")  # Scroll down
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=54875): Max retries exceeded with url: /session/837f6806bc1cf566e7de3a8728f95604/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x778532698350>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 20:52:18 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:52:18 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 20:52:55 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 20:52:55 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 20:52:55 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 20:52:55 [scrapy.extensions.telnet] INFO: Telnet Password: 26beb22ceb722cc4
2025-02-11 20:52:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 20:52:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 20:52:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 20:52:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 20:52:55 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 20:52:55 [scrapy.core.engine] INFO: Spider opened
2025-02-11 20:52:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:52:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-11 20:52:59 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 20:53:56 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 20:53:57 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a7afceb3830>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/1f6429800f30baf47f25852b40fc3db8/window/rect
2025-02-11 20:53:57 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a7afcee4e00>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/1f6429800f30baf47f25852b40fc3db8/window/rect
2025-02-11 20:53:57 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a7afcee4f80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/1f6429800f30baf47f25852b40fc3db8/window/rect
2025-02-11 20:53:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7a7afcee5130>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 146, in parse
    self.human_mouse_movements(driver)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 237, in human_mouse_movements
    window_size = driver.get_window_size()
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 1032, in get_window_size
    size = self.get_window_rect()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 1083, in get_window_rect
    return self.execute(Command.GET_WINDOW_RECT)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=53203): Max retries exceeded with url: /session/1f6429800f30baf47f25852b40fc3db8/window/rect (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a7afcee5130>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 20:53:57 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:53:57 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 20:53:57 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-11 20:58:34 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 20:58:34 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 20:58:34 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 20:58:34 [scrapy.extensions.telnet] INFO: Telnet Password: aeaa7d2dd030e412
2025-02-11 20:58:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 20:58:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 20:58:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 20:58:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 20:58:34 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 20:58:34 [scrapy.core.engine] INFO: Spider opened
2025-02-11 20:58:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 20:58:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-11 20:58:37 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:00:06 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 21:00:38 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 21:00:56 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-11 21:00:56 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-11 21:00:57 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7390849bb6b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/697b821ed0127c563ab766293651f142/execute/sync
2025-02-11 21:00:57 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7390849d4bf0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/697b821ed0127c563ab766293651f142/execute/sync
2025-02-11 21:00:57 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7390849d4440>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/697b821ed0127c563ab766293651f142/execute/sync
2025-02-11 21:00:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=fullstack%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7390849d5970>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 147, in parse
    self.human_scroll(driver)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 231, in human_scroll
    driver.execute_script("window.scrollBy(0, 500);")  # Scroll down
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=39901): Max retries exceeded with url: /session/697b821ed0127c563ab766293651f142/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7390849d5970>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 21:00:57 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-11 21:01:36 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 21:01:36 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 21:01:36 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 21:01:36 [scrapy.extensions.telnet] INFO: Telnet Password: 2f332c7e930a8d96
2025-02-11 21:01:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 21:01:36 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 21:01:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 21:01:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 21:01:36 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 21:01:36 [scrapy.core.engine] INFO: Spider opened
2025-02-11 21:01:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 21:01:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-11 21:01:40 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:03:13 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 21:03:37 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 21:04:52 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 21:05:39 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 21:06:41 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 21:07:30 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:08:21 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a478c613260>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/81fb1398e3ee9a60778dc45dcc3e6ea6/actions
2025-02-11 21:08:21 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a478c63c7a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/81fb1398e3ee9a60778dc45dcc3e6ea6/actions
2025-02-11 21:08:21 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a478c63c3b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/81fb1398e3ee9a60778dc45dcc3e6ea6/actions
2025-02-11 21:08:22 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a478c63dc40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/81fb1398e3ee9a60778dc45dcc3e6ea6/actions
2025-02-11 21:08:22 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a478c63c8f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/81fb1398e3ee9a60778dc45dcc3e6ea6/actions
2025-02-11 21:08:22 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a478c63dee0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/81fb1398e3ee9a60778dc45dcc3e6ea6/actions
2025-02-11 21:08:23 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a478c6131d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/81fb1398e3ee9a60778dc45dcc3e6ea6/actions
2025-02-11 21:08:23 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a478c613290>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/81fb1398e3ee9a60778dc45dcc3e6ea6/actions
2025-02-11 21:08:23 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a478c63c380>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/81fb1398e3ee9a60778dc45dcc3e6ea6/actions
2025-02-11 21:08:24 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a478c63c260>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/81fb1398e3ee9a60778dc45dcc3e6ea6/actions
2025-02-11 21:08:24 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a478c63e0c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/81fb1398e3ee9a60778dc45dcc3e6ea6/actions
2025-02-11 21:08:24 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a478c63ce60>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/81fb1398e3ee9a60778dc45dcc3e6ea6/actions
2025-02-11 21:08:25 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 1 items (at 1 items/min)
2025-02-11 21:08:30 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a478c63e8d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/81fb1398e3ee9a60778dc45dcc3e6ea6/execute/sync
2025-02-11 21:08:30 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a478c63ea80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/81fb1398e3ee9a60778dc45dcc3e6ea6/execute/sync
2025-02-11 21:08:30 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a478c63ec30>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/81fb1398e3ee9a60778dc45dcc3e6ea6/execute/sync
2025-02-11 21:08:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=software%20dengineer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7a478c5d6a50>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 147, in parse
    self.human_scroll(driver)
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 231, in human_scroll
    driver.execute_script("window.scrollBy(0, 500);")  # Scroll down
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=50389): Max retries exceeded with url: /session/81fb1398e3ee9a60778dc45dcc3e6ea6/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a478c5d6a50>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-11 21:08:32 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:09:20 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 1 pages/min), scraped 2 items (at 1 items/min)
2025-02-11 21:09:27 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:10:07 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 1 pages/min), scraped 3 items (at 1 items/min)
2025-02-11 21:10:10 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.linkedin.com/jobs/view/4077016444/?eBP=CwEAAAGU9b7dm5M2-ATJh8A_5Mo695pkeR4NSxBi7zhaFbYCDm2623wDpmN66EniHhw0KqvaRjbbCPfDwKWfh_ED7JcUmVBu-HOZjTE_5Ew02tJvX8PPZaB60LVlqAasxwwfjQf1HqvKqaR4TPCMsBtYyCoIc37d7Gm-fZJzWkJAYylx_kO7irDAsvwmdKqVkGAMXOKBn7Z0JMiojIJy-ybqZ5jgl1wtavanTeZ6HH0UwdP8ym-X1n0ZcmdB_fZqeOtPNMUy-PgVK_PIHCrAkdItHWrb6iBcQ17ovgXAUjRd9i3DK7V3n-JMP88xZZ1ANUuMR8Zg0ABZpF_2D5umjdRICUJgd_yuyjMPlmp2X_TlpajB5ZsVJ08bUi3YS9GBGDmgUygH95BlhBIcToSZxk3UqfdTbOn4nIAcIvHNYXEEx8X8_IfjGeMonurzbck5S_CbRtRyNooiNeg_b6mS9A&refId=gmWo1k88BxNIid%2FAp8iHEg%3D%3D&trackingId=mmN46UXdw6bPcosFwPYNwg%3D%3D&trk=flagship3_search_srp_jobs. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2025-02-11 21:10:14 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:10:57 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 1 pages/min), scraped 4 items (at 1 items/min)
2025-02-11 21:11:04 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:11:54 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 1 pages/min), scraped 5 items (at 1 items/min)
2025-02-11 21:12:00 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:12:57 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 1 pages/min), scraped 6 items (at 1 items/min)
2025-02-11 21:13:03 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:13:59 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 1 pages/min), scraped 7 items (at 1 items/min)
2025-02-11 21:14:05 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:14:52 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 1 pages/min), scraped 8 items (at 1 items/min)
2025-02-11 21:14:58 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:15:44 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 1 pages/min), scraped 9 items (at 1 items/min)
2025-02-11 21:15:51 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:16:40 [scrapy.extensions.logstats] INFO: Crawled 11 pages (at 1 pages/min), scraped 10 items (at 1 items/min)
2025-02-11 21:16:41 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:17:28 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:18:23 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 2 pages/min), scraped 12 items (at 2 items/min)
2025-02-11 21:18:28 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:20:15 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 1 pages/min), scraped 13 items (at 1 items/min)
2025-02-11 21:20:20 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:21:35 [scrapy.extensions.logstats] INFO: Crawled 15 pages (at 1 pages/min), scraped 14 items (at 1 items/min)
2025-02-11 21:21:40 [scrapy.extensions.logstats] INFO: Crawled 15 pages (at 0 pages/min), scraped 14 items (at 0 items/min)
2025-02-11 21:21:43 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:22:40 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 1 pages/min), scraped 15 items (at 1 items/min)
2025-02-11 21:22:41 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 21:23:35 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-11 21:23:35 [scrapy.extensions.feedexport] INFO: Stored json feed (16 items) in: output4.json
2025-02-11 21:23:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 1,
 'downloader/request_bytes': 22283,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 2542469,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'elapsed_time_seconds': 1319.04147,
 'feedexport/success_count/FileFeedStorage': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 11, 16, 23, 35, 757254, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 21998817,
 'httpcompression/response_count': 17,
 'item_scraped_count': 16,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 48,
 'log_count/WARNING': 16,
 'memusage/max': 144789504,
 'memusage/startup': 73547776,
 'request_depth_max': 1,
 'response_received_count': 17,
 'responses_per_minute': None,
 'retry/count': 1,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 1,
 'scheduler/dequeued': 18,
 'scheduler/dequeued/memory': 18,
 'scheduler/enqueued': 18,
 'scheduler/enqueued/memory': 18,
 'spider_exceptions/MaxRetryError': 1,
 'start_time': datetime.datetime(2025, 2, 11, 16, 1, 36, 715784, tzinfo=datetime.timezone.utc)}
2025-02-11 21:23:35 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-11 23:21:40 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-11 23:21:40 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-11 23:21:40 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-11 23:21:40 [scrapy.extensions.telnet] INFO: Telnet Password: 9a750b456ee9a4f4
2025-02-11 23:21:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-11 23:21:40 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-11 23:21:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-11 23:21:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-11 23:21:40 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-11 23:21:40 [scrapy.core.engine] INFO: Spider opened
2025-02-11 23:21:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 23:21:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-11 23:21:43 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:22:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-11 23:22:58 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 23:22:58 [linkedin_job] INFO: Sent 25 job URLs to be scraped from page 1
2025-02-11 23:23:01 [linkedin_job] INFO: Extracting job listings from page 2
2025-02-11 23:23:59 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 23:23:59 [linkedin_job] INFO: Sent 25 job URLs to be scraped from page 2
2025-02-11 23:24:03 [linkedin_job] INFO: Extracting job listings from page 3
2025-02-11 23:25:03 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 23:25:03 [linkedin_job] INFO: Sent 25 job URLs to be scraped from page 3
2025-02-11 23:25:09 [linkedin_job] INFO: Extracting job listings from page 4
2025-02-11 23:26:02 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 23:26:02 [linkedin_job] INFO: Sent 25 job URLs to be scraped from page 4
2025-02-11 23:26:06 [linkedin_job] INFO: Extracting job listings from page 5
2025-02-11 23:27:04 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 23:27:04 [linkedin_job] INFO: Sent 25 job URLs to be scraped from page 5
2025-02-11 23:27:09 [linkedin_job] INFO: Extracting job listings from page 6
2025-02-11 23:28:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 23:28:10 [linkedin_job] INFO: Sent 25 job URLs to be scraped from page 6
2025-02-11 23:28:16 [linkedin_job] INFO: Extracting job listings from page 7
2025-02-11 23:29:15 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 23:29:15 [linkedin_job] INFO: Sent 25 job URLs to be scraped from page 7
2025-02-11 23:29:20 [linkedin_job] INFO: Extracting job listings from page 8
2025-02-11 23:30:14 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 23:30:14 [linkedin_job] INFO: Sent 25 job URLs to be scraped from page 8
2025-02-11 23:30:20 [linkedin_job] INFO: Extracting job listings from page 9
2025-02-11 23:31:18 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 23:31:18 [linkedin_job] INFO: Sent 25 job URLs to be scraped from page 9
2025-02-11 23:31:22 [linkedin_job] INFO: Extracting job listings from page 10
2025-02-11 23:32:25 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-11 23:32:25 [linkedin_job] INFO: Sent 28 job URLs to be scraped from page 10
2025-02-11 23:32:27 [linkedin_job] INFO: Scraped 1 jobs out of 28 sent
2025-02-11 23:32:28 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:33:18 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 3 pages/min), scraped 1 items (at 1 items/min)
2025-02-11 23:33:23 [linkedin_job] INFO: Scraped 2 jobs out of 28 sent
2025-02-11 23:33:24 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:34:08 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 0 pages/min), scraped 2 items (at 1 items/min)
2025-02-11 23:34:12 [linkedin_job] INFO: Scraped 3 jobs out of 28 sent
2025-02-11 23:34:13 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:35:08 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 0 pages/min), scraped 3 items (at 1 items/min)
2025-02-11 23:35:13 [linkedin_job] INFO: Scraped 4 jobs out of 28 sent
2025-02-11 23:35:14 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:35:57 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 1 pages/min), scraped 4 items (at 1 items/min)
2025-02-11 23:36:00 [linkedin_job] INFO: Scraped 5 jobs out of 28 sent
2025-02-11 23:36:01 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:36:54 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 3 pages/min), scraped 5 items (at 1 items/min)
2025-02-11 23:36:58 [linkedin_job] INFO: Scraped 6 jobs out of 28 sent
2025-02-11 23:36:59 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:37:35 [linkedin_job] INFO: Scraped 7 jobs out of 28 sent
2025-02-11 23:37:36 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:38:21 [scrapy.extensions.logstats] INFO: Crawled 11 pages (at 3 pages/min), scraped 7 items (at 2 items/min)
2025-02-11 23:38:25 [linkedin_job] INFO: Scraped 8 jobs out of 28 sent
2025-02-11 23:38:26 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:39:07 [linkedin_job] INFO: Scraped 9 jobs out of 28 sent
2025-02-11 23:39:08 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:39:42 [linkedin_job] INFO: Scraped 10 jobs out of 28 sent
2025-02-11 23:39:43 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:40:17 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 1 pages/min), scraped 10 items (at 3 items/min)
2025-02-11 23:40:28 [linkedin_job] INFO: Scraped 11 jobs out of 28 sent
2025-02-11 23:40:30 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:41:25 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 0 pages/min), scraped 11 items (at 1 items/min)
2025-02-11 23:41:31 [linkedin_job] INFO: Scraped 12 jobs out of 28 sent
2025-02-11 23:41:32 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:42:15 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 4 pages/min), scraped 12 items (at 1 items/min)
2025-02-11 23:42:18 [linkedin_job] INFO: Scraped 13 jobs out of 28 sent
2025-02-11 23:42:20 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:43:13 [linkedin_job] INFO: Scraped 14 jobs out of 28 sent
2025-02-11 23:43:15 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:43:55 [linkedin_job] INFO: Scraped 15 jobs out of 28 sent
2025-02-11 23:43:56 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:44:48 [scrapy.extensions.logstats] INFO: Crawled 16 pages (at 0 pages/min), scraped 15 items (at 3 items/min)
2025-02-11 23:45:03 [linkedin_job] INFO: Scraped 16 jobs out of 28 sent
2025-02-11 23:45:04 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:45:47 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 4 pages/min), scraped 16 items (at 1 items/min)
2025-02-11 23:45:52 [linkedin_job] INFO: Scraped 17 jobs out of 28 sent
2025-02-11 23:45:53 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:46:37 [linkedin_job] INFO: Scraped 18 jobs out of 28 sent
2025-02-11 23:46:38 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:47:22 [linkedin_job] INFO: Scraped 19 jobs out of 28 sent
2025-02-11 23:47:24 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:48:14 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 0 pages/min), scraped 19 items (at 3 items/min)
2025-02-11 23:48:22 [linkedin_job] INFO: Scraped 20 jobs out of 28 sent
2025-02-11 23:48:25 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:49:18 [scrapy.extensions.logstats] INFO: Crawled 21 pages (at 1 pages/min), scraped 20 items (at 1 items/min)
2025-02-11 23:49:23 [linkedin_job] INFO: Scraped 21 jobs out of 28 sent
2025-02-11 23:49:24 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:50:16 [scrapy.extensions.logstats] INFO: Crawled 23 pages (at 2 pages/min), scraped 21 items (at 1 items/min)
2025-02-11 23:50:20 [linkedin_job] INFO: Scraped 22 jobs out of 28 sent
2025-02-11 23:50:22 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:51:11 [scrapy.extensions.logstats] INFO: Crawled 25 pages (at 2 pages/min), scraped 22 items (at 1 items/min)
2025-02-11 23:51:15 [linkedin_job] INFO: Scraped 23 jobs out of 28 sent
2025-02-11 23:51:16 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:52:08 [linkedin_job] INFO: Scraped 24 jobs out of 28 sent
2025-02-11 23:52:09 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:53:02 [scrapy.extensions.logstats] INFO: Crawled 26 pages (at 1 pages/min), scraped 24 items (at 2 items/min)
2025-02-11 23:53:11 [linkedin_job] INFO: Scraped 25 jobs out of 28 sent
2025-02-11 23:53:12 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:53:54 [scrapy.extensions.logstats] INFO: Crawled 27 pages (at 1 pages/min), scraped 25 items (at 1 items/min)
2025-02-11 23:53:58 [linkedin_job] INFO: Scraped 26 jobs out of 28 sent
2025-02-11 23:53:59 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:54:47 [scrapy.extensions.logstats] INFO: Crawled 27 pages (at 0 pages/min), scraped 26 items (at 1 items/min)
2025-02-11 23:54:52 [linkedin_job] INFO: Scraped 27 jobs out of 28 sent
2025-02-11 23:54:53 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:55:34 [linkedin_job] INFO: Scraped 28 jobs out of 28 sent
2025-02-11 23:55:35 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:56:17 [scrapy.extensions.logstats] INFO: Crawled 33 pages (at 6 pages/min), scraped 28 items (at 2 items/min)
2025-02-11 23:56:21 [linkedin_job] INFO: Scraped 29 jobs out of 28 sent
2025-02-11 23:56:22 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:57:03 [linkedin_job] INFO: Scraped 30 jobs out of 28 sent
2025-02-11 23:57:04 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:57:56 [linkedin_job] INFO: Scraped 31 jobs out of 28 sent
2025-02-11 23:57:57 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:58:40 [scrapy.extensions.logstats] INFO: Crawled 33 pages (at 0 pages/min), scraped 31 items (at 3 items/min)
2025-02-11 23:58:51 [scrapy.extensions.logstats] INFO: Crawled 33 pages (at 0 pages/min), scraped 31 items (at 0 items/min)
2025-02-11 23:58:51 [linkedin_job] INFO: Scraped 32 jobs out of 28 sent
2025-02-11 23:58:52 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-11 23:59:35 [linkedin_job] INFO: Scraped 33 jobs out of 28 sent
2025-02-11 23:59:37 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:00:15 [scrapy.extensions.logstats] INFO: Crawled 36 pages (at 3 pages/min), scraped 33 items (at 2 items/min)
2025-02-12 00:00:18 [linkedin_job] INFO: Scraped 34 jobs out of 28 sent
2025-02-12 00:00:19 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:01:08 [linkedin_job] INFO: Scraped 35 jobs out of 28 sent
2025-02-12 00:01:09 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:01:57 [scrapy.extensions.logstats] INFO: Crawled 37 pages (at 1 pages/min), scraped 35 items (at 2 items/min)
2025-02-12 00:02:05 [linkedin_job] INFO: Scraped 36 jobs out of 28 sent
2025-02-12 00:02:06 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:02:50 [scrapy.extensions.logstats] INFO: Crawled 38 pages (at 1 pages/min), scraped 36 items (at 1 items/min)
2025-02-12 00:02:54 [linkedin_job] INFO: Scraped 37 jobs out of 28 sent
2025-02-12 00:02:55 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:03:41 [scrapy.extensions.logstats] INFO: Crawled 38 pages (at 0 pages/min), scraped 37 items (at 1 items/min)
2025-02-12 00:03:46 [linkedin_job] INFO: Scraped 38 jobs out of 28 sent
2025-02-12 00:03:47 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:04:33 [linkedin_job] INFO: Scraped 39 jobs out of 28 sent
2025-02-12 00:04:34 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:05:19 [linkedin_job] INFO: Scraped 40 jobs out of 28 sent
2025-02-12 00:05:21 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:05:58 [linkedin_job] INFO: Scraped 41 jobs out of 28 sent
2025-02-12 00:06:00 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:06:46 [linkedin_job] INFO: Scraped 42 jobs out of 28 sent
2025-02-12 00:06:47 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:07:23 [scrapy.extensions.logstats] INFO: Crawled 44 pages (at 6 pages/min), scraped 42 items (at 5 items/min)
2025-02-12 00:07:39 [linkedin_job] INFO: Scraped 43 jobs out of 28 sent
2025-02-12 00:07:40 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:08:22 [scrapy.extensions.logstats] INFO: Crawled 44 pages (at 0 pages/min), scraped 43 items (at 1 items/min)
2025-02-12 00:08:28 [linkedin_job] INFO: Scraped 44 jobs out of 28 sent
2025-02-12 00:08:29 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:09:22 [scrapy.extensions.logstats] INFO: Crawled 47 pages (at 3 pages/min), scraped 44 items (at 1 items/min)
2025-02-12 00:09:27 [linkedin_job] INFO: Scraped 45 jobs out of 28 sent
2025-02-12 00:09:28 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:10:13 [scrapy.extensions.logstats] INFO: Crawled 47 pages (at 0 pages/min), scraped 45 items (at 1 items/min)
2025-02-12 00:10:18 [linkedin_job] INFO: Scraped 46 jobs out of 28 sent
2025-02-12 00:10:19 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:11:11 [scrapy.extensions.logstats] INFO: Crawled 47 pages (at 0 pages/min), scraped 46 items (at 1 items/min)
2025-02-12 00:11:15 [linkedin_job] INFO: Scraped 47 jobs out of 28 sent
2025-02-12 00:11:16 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:12:13 [scrapy.extensions.logstats] INFO: Crawled 51 pages (at 4 pages/min), scraped 47 items (at 1 items/min)
2025-02-12 00:12:17 [linkedin_job] INFO: Scraped 48 jobs out of 28 sent
2025-02-12 00:12:18 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:13:12 [linkedin_job] INFO: Scraped 49 jobs out of 28 sent
2025-02-12 00:13:13 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:14:06 [scrapy.extensions.logstats] INFO: Crawled 51 pages (at 0 pages/min), scraped 49 items (at 2 items/min)
2025-02-12 00:14:16 [linkedin_job] INFO: Scraped 50 jobs out of 28 sent
2025-02-12 00:14:17 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:14:59 [scrapy.extensions.logstats] INFO: Crawled 51 pages (at 0 pages/min), scraped 50 items (at 1 items/min)
2025-02-12 00:15:04 [linkedin_job] INFO: Scraped 51 jobs out of 28 sent
2025-02-12 00:15:05 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:15:50 [scrapy.extensions.logstats] INFO: Crawled 52 pages (at 1 pages/min), scraped 51 items (at 1 items/min)
2025-02-12 00:15:54 [linkedin_job] INFO: Scraped 52 jobs out of 28 sent
2025-02-12 00:16:00 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:16:42 [scrapy.extensions.logstats] INFO: Crawled 55 pages (at 3 pages/min), scraped 52 items (at 1 items/min)
2025-02-12 00:16:45 [linkedin_job] INFO: Scraped 53 jobs out of 28 sent
2025-02-12 00:16:46 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:17:42 [scrapy.extensions.logstats] INFO: Crawled 57 pages (at 2 pages/min), scraped 53 items (at 1 items/min)
2025-02-12 00:17:46 [linkedin_job] INFO: Scraped 54 jobs out of 28 sent
2025-02-12 00:17:47 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:19:33 [scrapy.extensions.logstats] INFO: Crawled 57 pages (at 0 pages/min), scraped 54 items (at 1 items/min)
2025-02-12 00:19:37 [linkedin_job] INFO: Scraped 55 jobs out of 28 sent
2025-02-12 00:19:38 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:20:43 [linkedin_job] INFO: Scraped 56 jobs out of 28 sent
2025-02-12 00:20:44 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:22:01 [scrapy.extensions.logstats] INFO: Crawled 58 pages (at 1 pages/min), scraped 56 items (at 2 items/min)
2025-02-12 00:22:10 [linkedin_job] INFO: Scraped 57 jobs out of 28 sent
2025-02-12 00:22:11 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:23:28 [scrapy.extensions.logstats] INFO: Crawled 58 pages (at 0 pages/min), scraped 57 items (at 1 items/min)
2025-02-12 00:23:34 [linkedin_job] INFO: Scraped 58 jobs out of 28 sent
2025-02-12 00:23:35 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:24:45 [scrapy.extensions.logstats] INFO: Crawled 62 pages (at 4 pages/min), scraped 58 items (at 1 items/min)
2025-02-12 00:24:49 [linkedin_job] INFO: Scraped 59 jobs out of 28 sent
2025-02-12 00:24:50 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:25:52 [scrapy.extensions.logstats] INFO: Crawled 62 pages (at 0 pages/min), scraped 59 items (at 1 items/min)
2025-02-12 00:25:55 [linkedin_job] INFO: Scraped 60 jobs out of 28 sent
2025-02-12 00:25:56 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:27:05 [linkedin_job] INFO: Scraped 61 jobs out of 28 sent
2025-02-12 00:27:06 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:27:59 [scrapy.extensions.logstats] INFO: Crawled 62 pages (at 0 pages/min), scraped 61 items (at 2 items/min)
2025-02-12 00:28:07 [linkedin_job] INFO: Scraped 62 jobs out of 28 sent
2025-02-12 00:28:08 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:29:11 [scrapy.extensions.logstats] INFO: Crawled 65 pages (at 3 pages/min), scraped 62 items (at 1 items/min)
2025-02-12 00:29:15 [linkedin_job] INFO: Scraped 63 jobs out of 28 sent
2025-02-12 00:29:16 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:30:25 [linkedin_job] INFO: Scraped 64 jobs out of 28 sent
2025-02-12 00:30:26 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:31:19 [scrapy.extensions.logstats] INFO: Crawled 65 pages (at 0 pages/min), scraped 64 items (at 2 items/min)
2025-02-12 00:31:27 [linkedin_job] INFO: Scraped 65 jobs out of 28 sent
2025-02-12 00:31:28 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:32:16 [scrapy.extensions.logstats] INFO: Crawled 66 pages (at 1 pages/min), scraped 65 items (at 1 items/min)
2025-02-12 00:32:20 [linkedin_job] INFO: Scraped 66 jobs out of 28 sent
2025-02-12 00:32:21 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:33:01 [scrapy.extensions.logstats] INFO: Crawled 68 pages (at 2 pages/min), scraped 66 items (at 1 items/min)
2025-02-12 00:33:05 [linkedin_job] INFO: Scraped 67 jobs out of 28 sent
2025-02-12 00:33:06 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:33:44 [scrapy.extensions.logstats] INFO: Crawled 70 pages (at 2 pages/min), scraped 67 items (at 1 items/min)
2025-02-12 00:33:47 [linkedin_job] INFO: Scraped 68 jobs out of 28 sent
2025-02-12 00:33:48 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:34:28 [linkedin_job] INFO: Scraped 69 jobs out of 28 sent
2025-02-12 00:34:29 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:35:12 [scrapy.extensions.logstats] INFO: Crawled 71 pages (at 1 pages/min), scraped 69 items (at 2 items/min)
2025-02-12 00:35:15 [linkedin_job] INFO: Scraped 70 jobs out of 28 sent
2025-02-12 00:35:16 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:36:01 [scrapy.extensions.logstats] INFO: Crawled 72 pages (at 1 pages/min), scraped 70 items (at 1 items/min)
2025-02-12 00:36:06 [linkedin_job] INFO: Scraped 71 jobs out of 28 sent
2025-02-12 00:36:07 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:36:50 [scrapy.extensions.logstats] INFO: Crawled 72 pages (at 0 pages/min), scraped 71 items (at 1 items/min)
2025-02-12 00:36:54 [linkedin_job] INFO: Scraped 72 jobs out of 28 sent
2025-02-12 00:36:55 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:38:37 [scrapy.extensions.logstats] INFO: Crawled 78 pages (at 6 pages/min), scraped 72 items (at 1 items/min)
2025-02-12 00:38:40 [scrapy.extensions.logstats] INFO: Crawled 78 pages (at 0 pages/min), scraped 72 items (at 0 items/min)
2025-02-12 00:38:40 [linkedin_job] INFO: Scraped 73 jobs out of 28 sent
2025-02-12 00:38:42 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:39:48 [linkedin_job] INFO: Scraped 74 jobs out of 28 sent
2025-02-12 00:39:49 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:40:29 [linkedin_job] INFO: Scraped 75 jobs out of 28 sent
2025-02-12 00:40:30 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:41:20 [linkedin_job] INFO: Scraped 76 jobs out of 28 sent
2025-02-12 00:41:21 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:42:07 [linkedin_job] INFO: Scraped 77 jobs out of 28 sent
2025-02-12 00:42:08 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:43:02 [scrapy.extensions.logstats] INFO: Crawled 78 pages (at 0 pages/min), scraped 77 items (at 5 items/min)
2025-02-12 00:43:24 [linkedin_job] INFO: Scraped 78 jobs out of 28 sent
2025-02-12 00:43:25 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:44:21 [scrapy.extensions.logstats] INFO: Crawled 80 pages (at 2 pages/min), scraped 78 items (at 1 items/min)
2025-02-12 00:44:26 [linkedin_job] INFO: Scraped 79 jobs out of 28 sent
2025-02-12 00:44:27 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:45:35 [scrapy.extensions.logstats] INFO: Crawled 80 pages (at 0 pages/min), scraped 79 items (at 1 items/min)
2025-02-12 00:45:40 [linkedin_job] INFO: Scraped 80 jobs out of 28 sent
2025-02-12 00:45:40 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:46:25 [scrapy.extensions.logstats] INFO: Crawled 81 pages (at 1 pages/min), scraped 80 items (at 1 items/min)
2025-02-12 00:46:28 [linkedin_job] INFO: Scraped 81 jobs out of 28 sent
2025-02-12 00:46:29 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:47:11 [scrapy.extensions.logstats] INFO: Crawled 84 pages (at 3 pages/min), scraped 81 items (at 1 items/min)
2025-02-12 00:47:14 [linkedin_job] INFO: Scraped 82 jobs out of 28 sent
2025-02-12 00:47:15 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:48:01 [linkedin_job] INFO: Scraped 83 jobs out of 28 sent
2025-02-12 00:48:02 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:48:48 [scrapy.extensions.logstats] INFO: Crawled 85 pages (at 1 pages/min), scraped 83 items (at 2 items/min)
2025-02-12 00:48:57 [linkedin_job] INFO: Scraped 84 jobs out of 28 sent
2025-02-12 00:48:59 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:49:45 [scrapy.extensions.logstats] INFO: Crawled 86 pages (at 1 pages/min), scraped 84 items (at 1 items/min)
2025-02-12 00:49:49 [linkedin_job] INFO: Scraped 85 jobs out of 28 sent
2025-02-12 00:49:50 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:50:42 [scrapy.extensions.logstats] INFO: Crawled 86 pages (at 0 pages/min), scraped 85 items (at 1 items/min)
2025-02-12 00:50:44 [linkedin_job] INFO: Scraped 86 jobs out of 28 sent
2025-02-12 00:50:45 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:51:31 [linkedin_job] INFO: Scraped 87 jobs out of 28 sent
2025-02-12 00:51:32 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:52:18 [linkedin_job] INFO: Scraped 88 jobs out of 28 sent
2025-02-12 00:52:19 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:53:04 [scrapy.extensions.logstats] INFO: Crawled 91 pages (at 5 pages/min), scraped 88 items (at 3 items/min)
2025-02-12 00:53:12 [linkedin_job] INFO: Scraped 89 jobs out of 28 sent
2025-02-12 00:53:13 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:54:15 [linkedin_job] INFO: Scraped 90 jobs out of 28 sent
2025-02-12 00:54:17 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:55:07 [scrapy.extensions.logstats] INFO: Crawled 91 pages (at 0 pages/min), scraped 90 items (at 2 items/min)
2025-02-12 00:55:17 [linkedin_job] INFO: Scraped 91 jobs out of 28 sent
2025-02-12 00:55:18 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:56:21 [scrapy.extensions.logstats] INFO: Crawled 94 pages (at 3 pages/min), scraped 91 items (at 1 items/min)
2025-02-12 00:56:25 [linkedin_job] INFO: Scraped 92 jobs out of 28 sent
2025-02-12 00:56:26 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:57:22 [linkedin_job] INFO: Scraped 93 jobs out of 28 sent
2025-02-12 00:57:23 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:58:05 [scrapy.extensions.logstats] INFO: Crawled 94 pages (at 0 pages/min), scraped 93 items (at 2 items/min)
2025-02-12 00:58:14 [linkedin_job] INFO: Scraped 94 jobs out of 28 sent
2025-02-12 00:58:16 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 00:58:57 [scrapy.extensions.logstats] INFO: Crawled 97 pages (at 3 pages/min), scraped 94 items (at 1 items/min)
2025-02-12 00:59:01 [linkedin_job] INFO: Scraped 95 jobs out of 28 sent
2025-02-12 00:59:02 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:00:09 [scrapy.extensions.logstats] INFO: Crawled 97 pages (at 0 pages/min), scraped 95 items (at 1 items/min)
2025-02-12 01:00:13 [linkedin_job] INFO: Scraped 96 jobs out of 28 sent
2025-02-12 01:00:14 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:01:11 [scrapy.extensions.logstats] INFO: Crawled 97 pages (at 0 pages/min), scraped 96 items (at 1 items/min)
2025-02-12 01:01:15 [linkedin_job] INFO: Scraped 97 jobs out of 28 sent
2025-02-12 01:01:16 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:02:02 [scrapy.extensions.logstats] INFO: Crawled 98 pages (at 1 pages/min), scraped 97 items (at 1 items/min)
2025-02-12 01:02:06 [linkedin_job] INFO: Scraped 98 jobs out of 28 sent
2025-02-12 01:02:08 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:03:24 [scrapy.extensions.logstats] INFO: Crawled 101 pages (at 3 pages/min), scraped 98 items (at 1 items/min)
2025-02-12 01:03:27 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.linkedin.com/jobs/view/4137839846/?eBP=CwEAAAGU9kLVSikAZ_TJ6_jswkJYp1QB8pTtN7sjqdYrQxR2W2fRFk-0ee0SqMTR8oODhRTDqAr7zO21tWZZnt59daDCLG_H_Hn0gvN4HbEqciV0RsBPywCJ2Fp4LVBrRtcjjcWeigqyEQt_A-fQxNDIaP1RON98hla3IkJXK41OIW_nezQgI2YitnIifjO8lqRDNrfyxcq-CBCV8lsIlTTP2MAQO750ZDL57_HARW73GgfS_WFN9wf8_rS6EFI9KzGP_rAvG9Xc9Qcl_jajFwL2KPKQGq08-DHATEfcLGTZXPX5uNJSnstXDNb23Qcvw6NlcqINy2I45IT5lQ56Ss4JZSkJB7sPOXBIFENQA0ryqA97KO4pnZwFAhfTAhcoiSIiVhE9zV6I1VZBx-sgQ421WPU0Bb3_V6eEYICA2iflBLx0DrbzONZuATN0shArbgdLUR-t2rwSvNoewJE&refId=SMrd4umDoUxXS1M8PrFG3Q%3D%3D&trackingId=03elGjO2UKv3prm%2FwUgjNQ%3D%3D&trk=flagship3_search_srp_jobs. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2025-02-12 01:03:27 [linkedin_job] INFO: Scraped 99 jobs out of 28 sent
2025-02-12 01:03:28 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:04:38 [linkedin_job] INFO: Scraped 100 jobs out of 28 sent
2025-02-12 01:04:39 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:05:32 [scrapy.extensions.logstats] INFO: Crawled 102 pages (at 1 pages/min), scraped 100 items (at 2 items/min)
2025-02-12 01:05:41 [scrapy.extensions.logstats] INFO: Crawled 102 pages (at 0 pages/min), scraped 100 items (at 0 items/min)
2025-02-12 01:05:41 [linkedin_job] INFO: Scraped 101 jobs out of 28 sent
2025-02-12 01:05:42 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:06:41 [scrapy.extensions.logstats] INFO: Crawled 102 pages (at 0 pages/min), scraped 101 items (at 1 items/min)
2025-02-12 01:06:46 [linkedin_job] INFO: Scraped 102 jobs out of 28 sent
2025-02-12 01:06:47 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:07:41 [scrapy.extensions.logstats] INFO: Crawled 105 pages (at 3 pages/min), scraped 102 items (at 1 items/min)
2025-02-12 01:07:45 [linkedin_job] INFO: Scraped 103 jobs out of 28 sent
2025-02-12 01:07:46 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:09:03 [linkedin_job] INFO: Scraped 104 jobs out of 28 sent
2025-02-12 01:09:04 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:09:53 [scrapy.extensions.logstats] INFO: Crawled 106 pages (at 1 pages/min), scraped 104 items (at 2 items/min)
2025-02-12 01:10:02 [linkedin_job] INFO: Scraped 105 jobs out of 28 sent
2025-02-12 01:10:03 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:11:05 [scrapy.extensions.logstats] INFO: Crawled 107 pages (at 1 pages/min), scraped 105 items (at 1 items/min)
2025-02-12 01:11:10 [linkedin_job] INFO: Scraped 106 jobs out of 28 sent
2025-02-12 01:11:11 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:12:15 [scrapy.extensions.logstats] INFO: Crawled 107 pages (at 0 pages/min), scraped 106 items (at 1 items/min)
2025-02-12 01:12:20 [linkedin_job] INFO: Scraped 107 jobs out of 28 sent
2025-02-12 01:12:21 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:13:12 [scrapy.extensions.logstats] INFO: Crawled 111 pages (at 4 pages/min), scraped 107 items (at 1 items/min)
2025-02-12 01:13:16 [linkedin_job] INFO: Scraped 108 jobs out of 28 sent
2025-02-12 01:13:17 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:14:17 [linkedin_job] INFO: Scraped 109 jobs out of 28 sent
2025-02-12 01:14:18 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:14:56 [linkedin_job] INFO: Scraped 110 jobs out of 28 sent
2025-02-12 01:14:57 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:16:06 [scrapy.extensions.logstats] INFO: Crawled 111 pages (at 0 pages/min), scraped 110 items (at 3 items/min)
2025-02-12 01:16:18 [linkedin_job] INFO: Scraped 111 jobs out of 28 sent
2025-02-12 01:16:19 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:17:11 [scrapy.extensions.logstats] INFO: Crawled 113 pages (at 2 pages/min), scraped 111 items (at 1 items/min)
2025-02-12 01:17:16 [linkedin_job] INFO: Scraped 112 jobs out of 28 sent
2025-02-12 01:17:17 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:18:15 [scrapy.extensions.logstats] INFO: Crawled 113 pages (at 0 pages/min), scraped 112 items (at 1 items/min)
2025-02-12 01:18:20 [linkedin_job] INFO: Scraped 113 jobs out of 28 sent
2025-02-12 01:18:21 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:19:18 [scrapy.extensions.logstats] INFO: Crawled 115 pages (at 2 pages/min), scraped 113 items (at 1 items/min)
2025-02-12 01:19:22 [linkedin_job] INFO: Scraped 114 jobs out of 28 sent
2025-02-12 01:19:23 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:19:57 [scrapy.extensions.logstats] INFO: Crawled 117 pages (at 2 pages/min), scraped 114 items (at 1 items/min)
2025-02-12 01:20:01 [linkedin_job] INFO: Scraped 115 jobs out of 28 sent
2025-02-12 01:20:02 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:20:53 [linkedin_job] INFO: Scraped 116 jobs out of 28 sent
2025-02-12 01:20:54 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:21:43 [scrapy.extensions.logstats] INFO: Crawled 117 pages (at 0 pages/min), scraped 116 items (at 2 items/min)
2025-02-12 01:21:52 [linkedin_job] INFO: Scraped 117 jobs out of 28 sent
2025-02-12 01:21:53 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:22:39 [linkedin_job] INFO: Scraped 118 jobs out of 28 sent
2025-02-12 01:22:41 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:23:25 [scrapy.extensions.logstats] INFO: Crawled 120 pages (at 3 pages/min), scraped 118 items (at 2 items/min)
2025-02-12 01:23:28 [linkedin_job] INFO: Scraped 119 jobs out of 28 sent
2025-02-12 01:23:29 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:24:09 [scrapy.extensions.logstats] INFO: Crawled 121 pages (at 1 pages/min), scraped 119 items (at 1 items/min)
2025-02-12 01:24:12 [linkedin_job] INFO: Scraped 120 jobs out of 28 sent
2025-02-12 01:24:13 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:25:08 [scrapy.extensions.logstats] INFO: Crawled 122 pages (at 1 pages/min), scraped 120 items (at 1 items/min)
2025-02-12 01:25:13 [linkedin_job] INFO: Scraped 121 jobs out of 28 sent
2025-02-12 01:25:14 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:26:01 [scrapy.extensions.logstats] INFO: Crawled 123 pages (at 1 pages/min), scraped 121 items (at 1 items/min)
2025-02-12 01:26:06 [linkedin_job] INFO: Scraped 122 jobs out of 28 sent
2025-02-12 01:26:07 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:27:03 [scrapy.extensions.logstats] INFO: Crawled 123 pages (at 0 pages/min), scraped 122 items (at 1 items/min)
2025-02-12 01:27:09 [linkedin_job] INFO: Scraped 123 jobs out of 28 sent
2025-02-12 01:27:10 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:27:58 [scrapy.extensions.logstats] INFO: Crawled 130 pages (at 7 pages/min), scraped 123 items (at 1 items/min)
2025-02-12 01:28:01 [linkedin_job] INFO: Scraped 124 jobs out of 28 sent
2025-02-12 01:28:02 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:28:52 [linkedin_job] INFO: Scraped 125 jobs out of 28 sent
2025-02-12 01:28:53 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:29:55 [linkedin_job] INFO: Scraped 126 jobs out of 28 sent
2025-02-12 01:29:56 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:30:52 [scrapy.extensions.logstats] INFO: Crawled 130 pages (at 0 pages/min), scraped 126 items (at 3 items/min)
2025-02-12 01:31:06 [linkedin_job] INFO: Scraped 127 jobs out of 28 sent
2025-02-12 01:31:07 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:31:49 [linkedin_job] INFO: Scraped 128 jobs out of 28 sent
2025-02-12 01:31:50 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:32:34 [linkedin_job] INFO: Scraped 129 jobs out of 28 sent
2025-02-12 01:32:35 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:33:19 [scrapy.extensions.logstats] INFO: Crawled 130 pages (at 0 pages/min), scraped 129 items (at 3 items/min)
2025-02-12 01:33:32 [linkedin_job] INFO: Scraped 130 jobs out of 28 sent
2025-02-12 01:33:33 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:34:12 [scrapy.extensions.logstats] INFO: Crawled 133 pages (at 3 pages/min), scraped 130 items (at 1 items/min)
2025-02-12 01:34:15 [linkedin_job] INFO: Scraped 131 jobs out of 28 sent
2025-02-12 01:34:16 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:35:11 [linkedin_job] INFO: Scraped 132 jobs out of 28 sent
2025-02-12 01:35:12 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:36:16 [scrapy.extensions.logstats] INFO: Crawled 133 pages (at 0 pages/min), scraped 132 items (at 2 items/min)
2025-02-12 01:36:24 [linkedin_job] INFO: Scraped 133 jobs out of 28 sent
2025-02-12 01:36:25 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:37:12 [scrapy.extensions.logstats] INFO: Crawled 134 pages (at 1 pages/min), scraped 133 items (at 1 items/min)
2025-02-12 01:37:16 [linkedin_job] INFO: Scraped 134 jobs out of 28 sent
2025-02-12 01:37:17 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:38:00 [scrapy.extensions.logstats] INFO: Crawled 136 pages (at 2 pages/min), scraped 134 items (at 1 items/min)
2025-02-12 01:38:04 [linkedin_job] INFO: Scraped 135 jobs out of 28 sent
2025-02-12 01:38:05 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:38:48 [scrapy.extensions.logstats] INFO: Crawled 138 pages (at 2 pages/min), scraped 135 items (at 1 items/min)
2025-02-12 01:38:52 [linkedin_job] INFO: Scraped 136 jobs out of 28 sent
2025-02-12 01:38:53 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:39:30 [linkedin_job] INFO: Scraped 137 jobs out of 28 sent
2025-02-12 01:39:31 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:40:21 [scrapy.extensions.logstats] INFO: Crawled 139 pages (at 1 pages/min), scraped 137 items (at 2 items/min)
2025-02-12 01:40:25 [linkedin_job] INFO: Scraped 138 jobs out of 28 sent
2025-02-12 01:40:26 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:41:06 [scrapy.extensions.logstats] INFO: Crawled 141 pages (at 2 pages/min), scraped 138 items (at 1 items/min)
2025-02-12 01:41:09 [linkedin_job] INFO: Scraped 139 jobs out of 28 sent
2025-02-12 01:41:10 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:41:56 [linkedin_job] INFO: Scraped 140 jobs out of 28 sent
2025-02-12 01:41:57 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:42:41 [scrapy.extensions.logstats] INFO: Crawled 141 pages (at 0 pages/min), scraped 140 items (at 2 items/min)
2025-02-12 01:42:49 [linkedin_job] INFO: Scraped 141 jobs out of 28 sent
2025-02-12 01:42:50 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:43:32 [linkedin_job] INFO: Scraped 142 jobs out of 28 sent
2025-02-12 01:43:33 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:44:34 [linkedin_job] INFO: Timeout: No job title found.
2025-02-12 01:44:44 [linkedin_job] INFO: Scraped 143 jobs out of 28 sent
2025-02-12 01:44:45 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 01:45:05 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-12 01:45:05 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-12 01:45:08 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x70b884c83da0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/7e077dafd7d9a804cf66c6332f63be59/execute/sync
2025-02-12 01:45:08 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x70b8857c2c00>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/7e077dafd7d9a804cf66c6332f63be59/execute/sync
2025-02-12 01:45:08 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x70b8857c3590>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/7e077dafd7d9a804cf66c6332f63be59/execute/sync
2025-02-12 01:45:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/3771699267/?eBP=BUDGET_EXHAUSTED_JOB&refId=bF2PbnZI3k7K1GIydUF2cg%3D%3D&trackingId=O5Id2OxtZegQ2XGrb9GLVQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=software%20dengineer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x70b8857c28d0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 224, in parse_job_page
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 270, in human_scroll
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=53785): Max retries exceeded with url: /session/7e077dafd7d9a804cf66c6332f63be59/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x70b8857c28d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-12 01:45:08 [scrapy.extensions.logstats] INFO: Crawled 144 pages (at 3 pages/min), scraped 142 items (at 2 items/min)
2025-02-12 01:45:13 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-12 20:17:35 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 20:17:35 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 20:17:35 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 20:17:35 [scrapy.extensions.telnet] INFO: Telnet Password: d76c9398bb81384d
2025-02-12 20:17:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 20:17:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 20:17:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 20:17:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 20:17:35 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-12 20:17:35 [scrapy.core.engine] INFO: Spider opened
2025-02-12 20:17:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 20:17:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 20:17:39 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:17:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:17:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:17:59 [linkedin_job] INFO: Extracting job listings from page 2
2025-02-12 20:17:59 [linkedin_job] INFO: Total unique job URLs collected so far: 14
2025-02-12 20:18:05 [linkedin_job] INFO: Extracting job listings from page 3
2025-02-12 20:18:05 [linkedin_job] INFO: Total unique job URLs collected so far: 21
2025-02-12 20:18:09 [linkedin_job] INFO: Extracting job listings from page 4
2025-02-12 20:18:10 [linkedin_job] INFO: Total unique job URLs collected so far: 28
2025-02-12 20:18:15 [linkedin_job] INFO: Extracting job listings from page 5
2025-02-12 20:18:15 [linkedin_job] INFO: Total unique job URLs collected so far: 35
2025-02-12 20:18:19 [linkedin_job] INFO: Extracting job listings from page 6
2025-02-12 20:18:19 [linkedin_job] INFO: Total unique job URLs collected so far: 42
2025-02-12 20:18:24 [linkedin_job] INFO: Extracting job listings from page 7
2025-02-12 20:18:24 [linkedin_job] INFO: Total unique job URLs collected so far: 49
2025-02-12 20:18:30 [linkedin_job] INFO: Extracting job listings from page 8
2025-02-12 20:18:30 [linkedin_job] INFO: Total unique job URLs collected so far: 56
2025-02-12 20:18:33 [linkedin_job] INFO: Extracting job listings from page 9
2025-02-12 20:18:33 [linkedin_job] INFO: Total unique job URLs collected so far: 63
2025-02-12 20:18:38 [linkedin_job] INFO: Extracting job listings from page 10
2025-02-12 20:18:38 [linkedin_job] INFO: Total unique job URLs collected so far: 70
2025-02-12 20:18:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4148214777/?eBP=CwEAAAGU-rzm6tOLflMRHZCkBZME8ZUVpmsdzUdH-bg5q1zwTZyIhE31RRzsaOreuFZv-SQ8lZUBXhB3f8hM8rRie9apRQp0Ckh9sGDK7CcJ1Vi6mxSTS4mbrj0uT_j7LQZj4f8LsITmnTeNQTeCJ36rAwhplNGS9GTgiRE7tqRqLgqnyopxFtR-5vnDGRuIhr9AhyVtzq1Rm_KMohp6a6D3TYK8oZbWRwSiBB0r-pxLC3x2DJA_Y4pJg5Z7v_cm1ACdKV31Gq_d0PAFiknPWLDSXXtCqnA_Y1hesRo0r1LJpq0dZcrudfDPVgPzwIlBbGfN0YN1qmC4ObElgSLUPS7YdaoiNY-8FJcAAb8czQcqcnxFKF3PgtQIGVcJxvHtvMvagLa4ukqfPKbvlDIC7i1gpipX4AxPxRo-RvfqiLSc41BlkNQMqeBCeQDP7WAHK0xjY2VwWPTErM_IMU61lyymEhxFZzyJCAMtZgwPNSuwJ5zDriQeojpKrU6XzvuaHrmd13anZWwN3gMcAKf1IUoQCPsV&refId=tXGOXC1%2F%2BZkkPbk4oaXeaw%3D%3D&trackingId=jJ3Q1EY%2F2jnuTFj3ghl5xA%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=software%20engineer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:18:38 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 20:18:44 [linkedin_job] INFO: Extracting job listings from page 11
2025-02-12 20:18:48 [linkedin_job] INFO: Total unique job URLs collected so far: 77
2025-02-12 20:18:54 [linkedin_job] INFO: Extracting job listings from page 12
2025-02-12 20:18:54 [linkedin_job] INFO: Total unique job URLs collected so far: 84
2025-02-12 20:18:58 [linkedin_job] INFO: Extracting job listings from page 13
2025-02-12 20:18:59 [linkedin_job] INFO: Total unique job URLs collected so far: 91
2025-02-12 20:19:04 [linkedin_job] INFO: Extracting job listings from page 14
2025-02-12 20:19:05 [linkedin_job] INFO: Total unique job URLs collected so far: 98
2025-02-12 20:19:06 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-12 20:19:06 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-12 20:19:08 [linkedin_job] INFO: Extracting job listings from page 15
2025-02-12 20:19:08 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72dfb62b7fe0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/bb54d3dfc9ec793d7597cb51773c356f/element
2025-02-12 20:19:08 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72dfb5938080>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/bb54d3dfc9ec793d7597cb51773c356f/element
2025-02-12 20:19:08 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72dfb5938260>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/bb54d3dfc9ec793d7597cb51773c356f/element
2025-02-12 20:19:08 [linkedin_job] INFO: Timeout: No job links found within 15 seconds.
2025-02-12 20:19:08 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72dfb59387a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/bb54d3dfc9ec793d7597cb51773c356f/source
2025-02-12 20:19:08 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72dfb5938950>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/bb54d3dfc9ec793d7597cb51773c356f/source
2025-02-12 20:19:08 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72dfb5938b00>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/bb54d3dfc9ec793d7597cb51773c356f/source
2025-02-12 20:19:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=software%20engineer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x72dfb5938cb0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 71, in parse
    search_page_html = driver.page_source
                       ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=42473): Max retries exceeded with url: /session/bb54d3dfc9ec793d7597cb51773c356f/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72dfb5938cb0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-12 20:19:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4148615075/?eBP=CwEAAAGU-rzm6qErOahJ0kbrVOTRs8h9EuNCofR2bhJgu4xR6I-0NyHvTvafoR8xPyX9EUGiCgkRjQy4ozPoJ6WztPBJAyIImVY8va1IYJmA0ZtxFySAu3Hf3Xsuhv83Je7RcyDYGGl0OLiM0O1nZx2EYPXOlCfIHrg0GvQVyJE23uZR-iClIvcAslKgAWsgT6X94tYXTcl4Bb-gSPwI4aIQvcjOgAAyYlHhCLW02jcskM-1zBmJBefkL2VDTkSTa5lavV6m70AyrsFqr-v0E4WOTA7hMzxi-p2mQ4Xayd_e-rMqgJzS0Uc0Keyh0Ah9owNXYMB4KW4aOXVu0ZlcibDF3tHUcph_L3rDORtrqhbWOmnw47hgdniyoRCaBwdBoxgm4ObnB5veYPQlhUdbkZdYYRFhNJ0y-rPvMSZHKIMFqKfWn3KDBVxG-smUpk1_zyXioUWqaQTfTJroEdqWsoqtyWI&refId=tXGOXC1%2F%2BZkkPbk4oaXeaw%3D%3D&trackingId=ADE1IhW9VuJ%2Bf0YPOCZi4Q%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=software%20engineer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:19:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4087718317/?eBP=BUDGET_EXHAUSTED_JOB&refId=w%2BNOPVIKXgs9pzkMsl%2FAkA%3D%3D&trackingId=2KMNNoDtjlqBZn2XHYugzg%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4117028113&keywords=software%20engineer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:19:08 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-12 20:19:08 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.linkedin.com/jobs/view/4147174263/?eBP=CwEAAAGU-rzsdc7GylFichlBgucCnoRTRFc48wP_Ctyy_iuY0OVJVaPHF2Tck6Jj9rlXU-I_82upPP4LmNatvwIrFAISF3-XMve3cOcE_mB7jRp7N7Fg2T6P6N-yHjYY7OZFrhE_LEitWa-aIPia1X9ZFR9z1_UgHYos-7OWu7ebQNQOFB3zKdwRKxdlS9MdJw4UPN5PktwSqCXLyd4ADf1tU-lcc8o0V8ifeBSMS2fYz3MDBQzZdNT4LOSsO2i63h7AjWW0rhO4gSJDnJMhiYjxcgoYpyLQb7HIYM7WR5OPRHxLqcWsBZfX6BryB3n5A22zrhiFSXqulei7eN9JHf5CkuUSwE4n0liHzmhk52nFktntdhQ-hVkOXy4AjOxhqXuolnd6VWWiu7vtUa1ndy_QSTeOPqySRQcrUibP3bATosBKeJEPFb7w1AINYLUXjuXn5rDM17GZbl-1_3eEV4y807XaA_VliaIwyLkU3OCczBYXp-HKxxr5O7W1pbb7pTBZqOtwYO959xXvrilH4bKh86vNIIkqcGpkYZM&refId=w%2BNOPVIKXgs9pzkMsl%2FAkA%3D%3D&trackingId=yeKHV7zaIvmXSQtFLg5bTg%3D%3D&trk=flagship3_search_srp_jobs. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2025-02-12 20:20:53 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 20:20:53 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 20:20:53 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 20:20:53 [scrapy.extensions.telnet] INFO: Telnet Password: ec39d20286e4a9a7
2025-02-12 20:20:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 20:20:53 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 20:20:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 20:20:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 20:20:53 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-12 20:20:53 [scrapy.core.engine] INFO: Spider opened
2025-02-12 20:20:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 20:20:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 20:20:56 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:21:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4150080475/?eBP=CwEAAAGU-r_o2sZiaEdM_6xM-n5r6ypSS36Fb-VQKWabFLKRtYSwzNfnzSXDEEDP4fqfqm4TuaidC-RvPlPav7cXVu59E1xYoEGrqb5sWgOaMjrDgem8Jz-7sTTR3z9Jra5idpW1tQ8dZf7E4wju_AehuP8SkzdQ1Qc7TPEZXEXj_l-_hnyUHXPbVvTSdillkPlj8qQEi_VhodAmca7KRNW4lJs6gsPk4hwsxWydcivSSWe-dYbffVRGVitEF86s_DLN3FPcN5-oD-gEBX2k_XX-JUCL3qX-zslE44nVT2Qo_f-6qQ_appNYnFHWWGIbxMYk97SBOftoiT6jM4vd8WQPI3vO49HxOwDvLZnmJ0STdFYViIi4Hnee4EX7gpTTkE0OkcLT7njuEhSO2I_FcEobFX6DWBMpOV4QhpMtgMN91ffXNlwA_mg40zNkAjIQOWa_ZMkD8UxPYA9BqyPOvKIhhO43eHjXhEbIiDFhBw&refId=s81DjXzlmRazB91%2F7amWGg%3D%3D&trackingId=6eeCk1El0%2BfitpALKWQ80g%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4141918018&keywords=react%20developer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:21:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4122063827/?eBP=CwEAAAGU-r_o2hP4cRILTRwDoDEi8h622q6PBHVBTF1f0shcl5oHcZiC8xut3KjVSjLFuvE69-Uv3wQg_aE-j1IuU_4Du0YBVwhk8O8nCAqjEzA4Ea37rhrlwej7kTJw-tAEycvfkHs8y9pFPeARrRNfBr990qsAACazQWOPeeeIFNhdkD9ZMLWdmQVDHOVMokwAl5zUmfkhwGkcBVapZQpuJ-C6oLUycqCOSMNFS9kzuRLLB_0UZuF9Xtz8y7zP2rw6VYbnZFOT_KD4Dv9U0ZhLUWF97SbujX6YrkCBUiHDwftvII84hiV6hc9pb-3JnlgASpHBnxP3yyR7JofyR_MNqfF-KEl1Brxf4HvwNMvfANy7UgTcxvblNySt3MHeIpaBMw0UPAgorGR3aW3M0IYZaYojhLL1VibcfPkb34iWl5MQUD2IhnWE4LDY78GkMHMcxPnsJd9-1QajyR8z-UnIUaVhUZj_leg3PsnTkBa27MJyQtFpIWApzNVmrvHT2BBzkAFrBxM&refId=s81DjXzlmRazB91%2F7amWGg%3D%3D&trackingId=NKKIu2I95lH2MLG5p0WTEw%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4141918018&keywords=react%20developer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:21:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4099744031/?eBP=BUDGET_EXHAUSTED_JOB&refId=s81DjXzlmRazB91%2F7amWGg%3D%3D&trackingId=casuDuI9iwZ%2Br9OJtprvjQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4141918018&keywords=react%20developer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:21:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4141918018/?eBP=BUDGET_EXHAUSTED_JOB&refId=s81DjXzlmRazB91%2F7amWGg%3D%3D&trackingId=RA7Ao9jBNe6I84XZoxPGbA%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4141918018&keywords=react%20developer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:21:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4145794602/?eBP=CwEAAAGU-r_o2vxJAyxksvWNnD2JCWwN7JvI2w9UQKGo23KUDudiw1GiTdpeYhYJQPea5yTxoccYsZKBeHAYRkwmOaSHE2M-vLPQ8QocwWnK8FJbuvLXbiZPzAZcE2fZ4pB9BvlAwU0VWvt7zjg_nBA09Y-bkLL6qa82cvNEHX2aYLhS40yMsL6ytdXlltnh-ThmTaSs1OZvSHLglTKhSJwARtNsIGs66e-VBuSLvKquAit6JZJxlkx9bR2ayhJcb8ujLx8UvQqUeOC-jO1SKAnQ4d_MJtgm1mGhvvVR44jkzoG5VHtl91ljj7TI7xL45AzOz8yf3eVttszPUR5L05X29hfPgZV0wDgHe3RpnKCExO_QZCjxAPh0HuJYtAX-B9WSxptucgMJslyoRJWW3uu0fBiSeF77BHUgRNxHMbsBtohWWVtIU0FAdSINGR0mBKXVFKIW2Txr-ii2BN8XEgdZyjRrCstc8aSUY5ypNVIv8Aa2Fokc9ex1tAAae9JizHY&refId=s81DjXzlmRazB91%2F7amWGg%3D%3D&trackingId=Piy%2FblE63%2Fj5AURZp1VJcQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4141918018&keywords=react%20developer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:21:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4131239990/?eBP=CwEAAAGU-r_o2jmaOF63XZxyJeH7bxSuyYaERqUir0wbpXKz3iZQkZch10HT6UiftHcV99gX6O68XS3e9eroA-fhM8ljtqGogdWmEgXpAn6w4HTlnjALVmdz_FeHSVL5d0TL2YWGA853JFxLnn2OHVhVX6Xc6o6bydQqz78LAcHg_qqILtJ1CAM2ytErLHIPRUe2BuGm_ljaq4b6A450VlIsL-n1dheEspqTiCzzcPi0aSSGEUs_L7kFNiCxqZJYjlhfXJGvoyb-Neidw9XyCO-0IjKc3yP9VK1Arv75UfxwmE0ZZQ7U0yaHPL3NdaU7v-kCHfolUN7u7AjjMl7ytfTJ-KjudIyn_5Nfl958cMqitOcaq6cGmJltcOlkWgFTDbhBxbk0yQgP9N7nToRQT1bGkCeQ5M6_XP2dbQuTVALouskxEWijfcU2LOYyuGoasL4rgI9bWeJKE9qVTp7d6rKr5-7guixhXEOpKoxUPL2qb9uY6JUkv5d7MoQzFyQfwI_f7gkXwW65WjzAyRtC&refId=s81DjXzlmRazB91%2F7amWGg%3D%3D&trackingId=5Sq3sOfNr3Snls3%2F1paXYA%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4141918018&keywords=react%20developer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:21:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4087718317/?eBP=BUDGET_EXHAUSTED_JOB&refId=s81DjXzlmRazB91%2F7amWGg%3D%3D&trackingId=LBg2KneRXt7khF5E8Cj9oQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4141918018&keywords=react%20developer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:21:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:21:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:21:49 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-12 20:21:49 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /session/fc103372fd92e93521a57bc1be37149a/source
2025-02-12 20:21:49 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c76f2ae6540>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fc103372fd92e93521a57bc1be37149a/source
2025-02-12 20:21:49 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c76f2ae51f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fc103372fd92e93521a57bc1be37149a/source
2025-02-12 20:21:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4141918018&keywords=react%20developer&origin=JOBS_HOME_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7c76f2ae7170>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 71, in parse
    search_page_html = driver.page_source
                       ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=57513): Max retries exceeded with url: /session/fc103372fd92e93521a57bc1be37149a/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c76f2ae7170>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-12 20:21:49 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-12 20:21:49 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: linkedin_scraper/data/linkedin_jobs_20250212_202053.json
2025-02-12 20:21:49 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/linkedin_jobs_20250212_202053.csv
2025-02-12 20:21:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 9021,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 1189464,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 8,
 'dupefilter/filtered': 1792,
 'elapsed_time_seconds': 55.25654,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2025, 2, 12, 15, 21, 49, 30288, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 10319517,
 'httpcompression/response_count': 8,
 'items_per_minute': None,
 'log_count/ERROR': 8,
 'log_count/INFO': 529,
 'log_count/WARNING': 3,
 'memusage/max': 73572352,
 'memusage/startup': 73572352,
 'request_depth_max': 1,
 'response_received_count': 8,
 'responses_per_minute': None,
 'scheduler/dequeued': 8,
 'scheduler/dequeued/memory': 8,
 'scheduler/enqueued': 8,
 'scheduler/enqueued/memory': 8,
 'spider_exceptions/AttributeError': 7,
 'spider_exceptions/MaxRetryError': 1,
 'start_time': datetime.datetime(2025, 2, 12, 15, 20, 53, 773748, tzinfo=datetime.timezone.utc)}
2025-02-12 20:21:49 [scrapy.core.engine] INFO: Spider closed (shutdown)
2025-02-12 20:21:49 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-12 20:22:18 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 20:22:18 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 20:22:18 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 20:22:18 [scrapy.extensions.telnet] INFO: Telnet Password: c4cd5c44cd6bb699
2025-02-12 20:22:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 20:22:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 20:22:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 20:22:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 20:22:18 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-12 20:22:18 [scrapy.core.engine] INFO: Spider opened
2025-02-12 20:22:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 20:22:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 20:22:21 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:22:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:22:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:22:42 [linkedin_job] INFO: ✅ Final total job URLs: 7
2025-02-12 20:22:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4150080475/?eBP=CwEAAAGU-sFCG2vMg4LZmlaEJuVbWAIBi2hYSepHRrGRlcOrkgAWsLFq7wDujfRj_rXYdCb-AbS8WjjOjvGCd3cHQjCSbsog8zx5aNGMXfXq3yKoaSBay-6gIGym6hKSQfyvc4gaS1AYinFrHlW8pbIuLyWbJSDniBUolSifMGwUsnbuWoOclYZEz44h_wPSfXkFRUzKe4K5J2_vhoKCrZoaeVIMMTTSLcWCE84DdBWjYbG2CW0DM0OimpCN8orfH_Bmj1AznO_eGnh83XZgndvHhGQGUwubAqlRsOLZUi0fSSiuVqyIyOchPYPp0DeSUhHA1CwY30pow8QzoJyXxKH2ZjnMOdZYAlLnX7HwQFagxShHvpMfjI_ZSWQSxNPpodXiCD7jO8_AKxvZtO1MxQ0pnl6ZyQzhU2uJtTgR5Daa5JcbUeUbdP8g8Xb9AKjwB7PdUe9sD3BIIJ_-OIBIxQdbKpptqgErrquKTKJw5Q&refId=v2ZWazixD1e%2FQnolYsT1EQ%3D%3D&trackingId=%2BRmdU9cd2ar6BCaDU3jY3Q%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4141918018&keywords=react%20developer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:22:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4087718317/?eBP=BUDGET_EXHAUSTED_JOB&refId=v2ZWazixD1e%2FQnolYsT1EQ%3D%3D&trackingId=2p7mNZFGTQxCwUj%2BsH1SfQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4141918018&keywords=react%20developer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:22:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4099744031/?eBP=BUDGET_EXHAUSTED_JOB&refId=v2ZWazixD1e%2FQnolYsT1EQ%3D%3D&trackingId=CY0tX3wX8aTzIw2CdM%2FOcw%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4141918018&keywords=react%20developer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:22:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4122063827/?eBP=CwEAAAGU-sFCG2SohGcWiDVRQfHY9ch6AI84_nGdKxLEzEBUU5ttKDy1a5fsGgIZNu-VpxHLcwfC1PJqgPTSn6FBvEB2bI7A8L8Zm6BFUeqBFZl8V1aV6TJq-_kadw9XVGOOL4EpI-7goUIfrVvccqTocAvVuWJUWnUZizEDCIxsYU7KEMC4c-cs7f8w9lzz9yq7Tnamk9hIr_7VULcgIdHaGMXyOm6sscWzc-2dpRWIaXPgMygS9JndTFgnRQtD2_UCDGmklFXr5CCawX8pzQ_Z1sZjvBTmSydMzsTDIIUpqqi6MQVtrYa5QdUKfN0_87qy80MurhO-6UM0RsOIwEV6JfjEh3M8iXnTHfyobqjLqNrITXvbQ26-2Eaj8JruGrLNddVybmgwbFPda-_WyFsOyLByC-4mPRQePwSjMO3eg7V4_Dcuq_70EQbODiVP_iNaBmazXlFRVRnBdrfDGZLfPn1M4BqFcXkZtGrtViNejmeFEmTjhdCHPwujYMhSEdk9LG7tfmo&refId=v2ZWazixD1e%2FQnolYsT1EQ%3D%3D&trackingId=hMfjHT%2FyjvsovszeSC09hA%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4141918018&keywords=react%20developer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:22:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4131239990/?eBP=CwEAAAGU-sFCG5HIsMKDcws8FxqZj4YiI0C9WYFajeK9LRF75jem_-6gB2hJG_5CaBbNP-Xhqqxobi3UCAiOFX4zYK4KSrIlWDJLFgste8u7n3cjbDdy2-aQ6HU4mmJu5B_9bg4dBZ8xtzKVBcpnxwBDvG369eY0ZmJOuxyQt_tpH2Fe28fNZzGwHjToE0X6H_rpoUyKVKQttZdq0M1JUQ-0jGiO-Y_z0_ghfACJwBoR3JAXgQZb_M2Xa9NugtUx6K7aZ9XYUxjbsC-QMtODJsY3YqyDHGqGkHFSFfU7S5qlnwc72YQBN8vI0Ef9qeIBLUbHY-O9TD0IeFDyGNyms_YqdEpH7lmMltSXxF3fiJnfKQ4rt5_mzOOohXMfw67od26c6hNopUqctVVUR5Brenjn29ULX-_8KALw6Qb73-RhZiGVPvQwqc_DR9EPMGA3qM8CJt_eUBdHTWC5dSultYDxWqdKSuacnTuOmhzwXmAvHxVoBe3BDEiJkrsR5WlKecGh0qEa7zsld0k&refId=v2ZWazixD1e%2FQnolYsT1EQ%3D%3D&trackingId=Sk4rto%2BAT5PZO6l%2BF79%2F7Q%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4141918018&keywords=react%20developer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:22:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4141918018/?eBP=BUDGET_EXHAUSTED_JOB&refId=v2ZWazixD1e%2FQnolYsT1EQ%3D%3D&trackingId=sAePSNIcpWfOmqH%2BtNdcNA%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4141918018&keywords=react%20developer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:23:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4145794602/?eBP=CwEAAAGU-sFCG9qo_MCDGpDSr4wIw8nUPfE-0Q4tIgz3e4dI1gZ8FkTkpR1t7QAPM1UMdoKAERYM6I95S42-eKRWG5PB0XhgWIxSF6t3_AQ5AEnEBFtZrvVbEKAo5Nm8_5DlQnPXtJ1AoFz0QV4CIWUenqFfhnMtQaFsuC2pofkLgo9hVE5om_0ERUzwEyrbql-K2h7K7qYjCo4tRmr6Rn_HfE7WKHLT3Uq7ZHENQT4KDR4N9-k0rKXLV_1NBFPETPGkGtkWL7UzudejiGbmfLC3CZHJngpBOzDij3_YWHzWfD1UFprczkvEsvgMjDk1L0E2sgDj0DIGYESnzguT_N8mIM60hK4ko_6eRgbJCN7y5IZiuVpWVYw1tUQ7vBkgP6z-M30yJvDcZTP2lDTe7vZ8W_JwS4lHnI0UrXbXkTXeLUiSH0f-JN_QptF5ilOKUvXzyGg2Ca--80Kr1xm7FXpMT1QUEPq2xZ81Oto2sghCdtYT0cFSaOyWVfPk-CU0O8En6g&refId=v2ZWazixD1e%2FQnolYsT1EQ%3D%3D&trackingId=bwgSRk78u29n4mFGgR6TjQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4141918018&keywords=react%20developer&origin=JOBS_HOME_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:23:03 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-12 20:23:03 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: linkedin_scraper/data/linkedin_jobs_20250212_202217.json
2025-02-12 20:23:03 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/linkedin_jobs_20250212_202217.csv
2025-02-12 20:23:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 9025,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 1190496,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 8,
 'elapsed_time_seconds': 45.408949,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 12, 15, 23, 3, 501775, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 10323917,
 'httpcompression/response_count': 8,
 'items_per_minute': None,
 'log_count/ERROR': 7,
 'log_count/INFO': 16,
 'memusage/max': 73940992,
 'memusage/startup': 73940992,
 'request_depth_max': 1,
 'response_received_count': 8,
 'responses_per_minute': None,
 'scheduler/dequeued': 8,
 'scheduler/dequeued/memory': 8,
 'scheduler/enqueued': 8,
 'scheduler/enqueued/memory': 8,
 'spider_exceptions/AttributeError': 7,
 'start_time': datetime.datetime(2025, 2, 12, 15, 22, 18, 92826, tzinfo=datetime.timezone.utc)}
2025-02-12 20:23:03 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-12 20:26:01 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 20:26:01 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 20:26:01 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 20:26:01 [scrapy.extensions.telnet] INFO: Telnet Password: e77bedc6b8d2d40b
2025-02-12 20:26:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 20:26:01 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 20:26:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 20:26:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 20:26:01 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-12 20:26:01 [scrapy.core.engine] INFO: Spider opened
2025-02-12 20:26:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 20:26:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 20:26:07 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:26:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:27:28 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-12 20:27:28 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 20:27:33 [linkedin_job] INFO: Extracting job listings from page 2
2025-02-12 20:28:29 [linkedin_job] INFO: Total unique job URLs collected so far: 50
2025-02-12 20:28:29 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 20:28:34 [linkedin_job] INFO: Extracting job listings from page 3
2025-02-12 20:29:32 [linkedin_job] INFO: Total unique job URLs collected so far: 75
2025-02-12 20:29:32 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 20:29:37 [linkedin_job] INFO: Extracting job listings from page 4
2025-02-12 20:30:37 [linkedin_job] INFO: Total unique job URLs collected so far: 100
2025-02-12 20:30:37 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 20:30:40 [linkedin_job] INFO: Extracting job listings from page 5
2025-02-12 20:31:38 [linkedin_job] INFO: Total unique job URLs collected so far: 125
2025-02-12 20:31:38 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 20:31:44 [linkedin_job] INFO: Extracting job listings from page 6
2025-02-12 20:31:53 [linkedin_job] INFO: Total unique job URLs collected so far: 129
2025-02-12 20:31:53 [linkedin_job] INFO: ✅ Final total job URLs: 129
2025-02-12 20:31:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4148767884/?eBP=CwEAAAGU-sS7iO2slGhUeKFqMogk-kGjdUX1TOEa2TxxbPfJQ-ODffkbJP1yrsBIKdmcsXVYvhlAbIgF_cEO-7M1EMvkFiPJoC1qsFJT0abT7TjVEGY833HfVGuig8FI7yN7sxY8tDu18hz8hmhkH0Jb-QeSf7zH33ItseeHNHhx4DsmMyJtQ4DyFItboAmAKwAVY6jhGYR_kpyFYPVT_sQsGcthKg0UnQEWALEH4roK9GGSTSsSrFOanNhyvYQsLVKx87d5l2qoU70DIZsJnlbtt4_Rnn7bXG6KGFidXzjnYu4Rq6Ha4PuujXoETw7Hfl7x_N7-yYQ3syznO37FU9CkKxDoUMKzy0OfMWJEoiX17hEEnqKU1qEbmeyXYTe4PeLgpfg4ERc6qtpPy7sRzyUCqY9bALRDEq_3YwW5odjDFTpzBUz5mzyQF-WehuSLMl--8qykRI0-jdHiyH35DBCcjeSDvOE1XNQT3Hdm1Dq2iz0X&refId=j1AxBnDL%2BOLXUK8J2qoHcw%3D%3D&trackingId=ixP1XmAPZ9bsv00rPI1Whw%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:31:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4148734135/?eBP=CwEAAAGU-sS7iTBVHdlh3S5fRPntK3Qsgx4OFEEpsZDJXbnWW4FL3oqEAEUknGjsPa8gCUc8BpJLtVuT-amRwXh0WRLxSbV9UVkoy5DXTR-1de-cpFKuNDCiamFwlSK2SWJ7-dlpvxyn0PzeLq4cSgdfZf4MPJ7WIotYwwQXunyLlZBvVlP-TIUzU7-qcgq05YiY-ltPGZN6AMK7y3aEptVCBxhRH_jLjiSg2LI-DsJPKSv_y4kq27KElFUjnL4pvcUSHi6u_nbwbQ9QPRloJ8hOP-VN0ZtDrZTJ_cnBTkDOVzz5Csfm7kc7z-Zz8_vklyW0TfYaXF7Ru-4gODX88GJIcqtqdBzmH-mA8JXkxY_OEh0U2efZhSigNsXWRjXdhjXmWpRiduEiT6sZu7HrmEEtHk6R8YO4Fx-4a-D-pYBwTYzUlecTqBmmJDoTp8Ombgn7LyVEJuWzzFL4joDEePmc-oVE5GYssSG2lJcVxfEUuk1AsdsMeiSWSiowYZJY4b9FzA0emLakaN53SwXu8kU7TOsX9u9zNrFDVA&refId=j1AxBnDL%2BOLXUK8J2qoHcw%3D%3D&trackingId=fZCB7SUzdpJ7QrXGjaK8IA%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:31:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4135872606/?eBP=CwEAAAGU-sS7iKz6zQDdMx0fjOFJvsO42B9VM1typ2x8dfDnJVQ5OguTQgALw-2YwlkrRkwXbWEfWG12uiuNJ9pwoDuUfxtlHRCjW3HroQsTLDs2v4O9xAqXmASJlm4-9vNmvTfk6OjCXHA0QqBAJvH_CmSkni8qfe1n9mPPRbUPav_QpQ-_LJuwpISggIxTy1D3g7HlQ1bc0-NIewETsQ9vBA856nSrWQ7w2P6Wi9_1xTbs3GtV5Eioo-H8nHUnKH__BgbnUmCTburiQCx2wJ9baKS-b7fo6HAdx3dteE4WjKAu9DtvSBBn3nF8YZVa_cyYglz7lEUgL7uDRvGlOwlRjaPMatRcpGwsmZZLaYewkifXj31hYOti3SvkO-gjqRZG0rllLyIQU7ATmi0vpKRbLo755Mbe65PDuMpO1aciD-lAeCDxqflB8ZSHwPPCF1Bb7FmKmWAYjYmr-5CkRXZ0WkYJC1jD_Z8BPZLl1IpCDTf00KHMQAujR8dwN2aGAByV3JKhQAEePJypt2qv3Mu1LY42&refId=j1AxBnDL%2BOLXUK8J2qoHcw%3D%3D&trackingId=u6tLkWHhNSm3H4lsNS0Hcg%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:31:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4148779100/?eBP=CwEAAAGU-sS7iMHYUoqd_9XS_7TaWQ7SwgO12jeGn9b3m_2Cp3uRlxnZCAN1Xty2xY1euNxqmkjJgmqeC7VaZYsw5B1QiLB20f9y-z4eBS7dN-WX6sLqVSfsEAi3n1L10X9f6JfYClgBBAobvmQl-1cA1yifd59o9wWMgO4lxXaYQvrvsKMt3Hr_pnYvjljwwAhV19qMxUjyXuaz187S23-ME47ONK21VZ_Qw0SclDHQKMwEcKfPYaBXekEe1YmfSIPWBB1H7FmrgU0CEWP1qY4KuYtXcd5whKAb97KiRohGq-pIIVj0FNl29q4ucbOZFFKLXfIdQGJxbNHySb01nPeU8YHS-4igoKgNLn0sgo_kfXo-EauvP4Ti9NEfd76tqKVNxPsPZPxhYpwg080Msq-dojWmg6iNbaV4RyGCCZWPmYSE9dKSpLulu-u3odfkeU-ps7SIFV0kgyjXrhS-tkrAr18CeAzg-tUzqAF5FX3MiQIS&refId=j1AxBnDL%2BOLXUK8J2qoHcw%3D%3D&trackingId=xIk0yIE5BfWb70wxBDin5g%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:31:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4148727527/?eBP=CwEAAAGU-sS7iESXkr8aCsnhDt_kE58CcLiCcX--8_3IC17qPlLNlHefFhmwAJwSCwa3IcqQCN_2iZAjfwywPrKUPuu--J3qkdm2E3ZhTgApKFdrktgGfHCfQGLwd85KiETqOP46JmG48xLIor70x32-bH0ZH42-9n-NG5A8GrFrVM5taToy1uFNWqG2QigaphspWLhCOF7-qlzMeas_5wA9N_CHtPTF184XUPSzGyHVXD6BGOH8HSEfnKilk-CzLr6vKr1CFb7mQUY2HzGFJ8s5v7Sp60DZDKBuPjgmrsd2VuQj-JtW97Ly9Rh8oI33LXGXGMqriGL0qvYpaI6TjWVwOXv5-YEVeS-Dc-f3dmTtkdCgGUwHTfs_50RctSfRSZ8P3v9tYRUVbn7zDU3paZ5OXibgS-AZZWaOax5vh3dCkBwM0OabYsPSbCIfWLPN8EUpgp-CdtGEA_IYXQEyqGVtFkJFIt7uQs6ZMh4EmAQ3YwuB&refId=j1AxBnDL%2BOLXUK8J2qoHcw%3D%3D&trackingId=X%2FrJVYS0%2F1%2FykvBDIVNo2Q%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4138893441/?eBP=CwEAAAGU-sS7iQdu8Fh1gWX7Iy8rEt4Vm6335-6W6nWjz9SAdblq3mC9n-fZ0jDzhrf351Rfq4odKLKgnC6CJqcnrfLZbFMvNql3fXzaNqKcErtp7n9MgfInwXnxldf7mPwFmV_IYX9QyYp6YLiPcWsAJwM7sYA2cazb-2nfXAKU-iHRVEEMFgOICT_EhGsWgua86iQxyNi7fcyiy7ZD_u7iYugSCJPBWPTNdcFVqmThtH3WrzeXG9nbey9DWLl3mUCOuuN_foR3cZP35QQkOdAE36FIytQ19ZnH1PMxbkVNLo2V0tQyEv7G7AhZwiSuAkpi6Qg-smQjgLZFjxf5JhyTg3Ez49sWAr9bR4P7igXkJTwg4pXEu7jctY8JiQ0JsTZsqOVyMTFd_NGO-LF4pMm_rVPl_s4K8N3L5BTsFjn_qPiGfRUqVyGt6KcTHfTNnHtpuETX3lflRxfuIn5XTjlm&refId=j1AxBnDL%2BOLXUK8J2qoHcw%3D%3D&trackingId=Pdfd7v%2FxH1%2B5iwfZhZ%2FSwg%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:01 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 20:32:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4148740902/?eBP=CwEAAAGU-sS7iVjfHDnKFlfCQKqSoDURkEeY2J1HSE7USnP3iIXZu1lNaW2Vds4gV5ZUocko6yWSEHPrrs0PgisPB7-ImtM5vZDyqlAsI2hSvNfw2c6KA_hvfbIpTQrkyJjwLvMm7lrtuscP-y49jVaeUa0U2byeroltV0h3lzySOaFIphYoczn90rh6KoVmF_wyjxrUhr6hl16eAimUnOUPpvxqxqXe_INCXkdjmuvbsXmGYBmc4xdMxlButaU-LTDBNc6793qrYtJ0M6H7fUq46UbCxCPq1Eot-EmMIF224iwKAaLt87hbR8qtcDzV0W3PhWmEOfxPB4VUCq78gz_X7WB67YhVLQGVzn0fwVQA34vHgdNuXY1QFMFtAgCmO2Rb0Y_IdILsp8i1_Y5CoMCa84kkDOj37uAEvWerxrcGeUAwkKbB-4avNP_vTS2p0aqVsk0ATIj8WaM3heiG4-zMialwDqOqGxebYZtCoP6CiWbb&refId=j1AxBnDL%2BOLXUK8J2qoHcw%3D%3D&trackingId=4tm5HhmQeNH3Yvmxqby3GQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4148751999/?eBP=CwEAAAGU-sS7iEFE4pNqLD4zoPkrikS4Ytyw3LjoxJ-kt4mi9fjHdB0omrMdOqJMd2nPAA1adeMrgueOO6LQYoL8kSTu6rbGnlvsMwy6tVmGKhnlcnTf0aVEUHgDjVZhCWaHZ0Kb2V0hwHxYpK2Fye91a8677yivs5E11YtLQyCORoMsTfw7QzNuN6wkwvjToS-eatWwWfedhi2JMx8mKjeZVplVbTEMnbkQB9iZgvC0k9n5XPluBgNmArefEysFB6PEu8_IeGWx_l-5TWOnUnfq61FiBFH1yShxsiyiq3nJADzpcerYsCXdTwAhQyvmD9Yep59A35zCb2x7jTwypOAkFBD2JTdMCNKnNzpDmPaZhy7E7ksntsScMfVubq3W00hxIPeqOdhhsKIFGqiMzZMAGTwAqX8YlBYJIujUBIuk0ax-9f-m_UiBXEMN_TRtJYu_Kj5QBDH3rL9dPHYvFMGXCae6jfyp-U1ULsSQ5AS38grg&refId=j1AxBnDL%2BOLXUK8J2qoHcw%3D%3D&trackingId=4T02QJ8%2FvHkJKCjX%2FzUkAw%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/3739543677/?eBP=CwEAAAGU-sW4aqnxW8I3EuXEWArCd0Wmxqr5DlACRtcWDe4nZH97dWNKOfiiMlBfJRbYhmSkH8Eh81T5kUsSOWylKRDzVlRP16FOpoINYHir60ODO9JzV1ldRwiog567HlT5bfwDLU1N9oFmlh2nuSXFsMRyr2vhH8skk4EHHf341lNR9W1inhnInbRWYMZRS_C5WuX8Ah29uGHLXgOoaiuBJ0VicTEE_5EBbVQhhnk19KqvNGFUOLTJQuVU1YfTyuIrhpHnarsSfNlWrEQdF8c4DMXWpl-Plma2kEDnYYyIsnyFvtcbUBWDeCBAa0vUOTBEw-NiQ1ug9phKy8deD23zuSk79xGzSdoI9M8_Px8KWJDyuECu-nyAD99JNiYP73jE1A2_1EWikUVMJm3q2UneVwrWLStU9bn5dxPucLBA8iFgIupQ7vzE3aYCCSl2Xrnhu8U5VUVwwSjV8rY&refId=NTfP47wyl3dcgldvpfWqrQ%3D%3D&trackingId=0AIBvLhQ%2BfO5znlVfGJyuQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4073323489/?eBP=CwEAAAGU-siYjlAZBmc9cT9zApg35SsPl27CQH8ZIdFv9jloJQNjv4qKK7gypuo0fXty1N2HjefaNuWNI994gqu9uGK1xyRWHkhINEiiyxou3bTbaVeIhhXql-VytccUVjN1GI9CUmcKmkkUf0VjmWe9eSdZfc-ym_VjK_oRNWlbbjpE8ZlpnTLJbp9UabMMZQx2m_QxqCW7fpfj2uLzLdBMOYKo_9Yuj4mOIZihwYhJsf6e8tPootZM-1pFlsw46unq03Y3QdXaez7inioX9M583ksyU7qf9PvHPoozHMbQPD0F3zWL1ELeT7Rt-_GjFoVkwuG3tUP3MkgotrqBrSpF6pE889NhCWli5k8eKfCpbzliV156lzhbjJ4tkMM2MFI29iBBUIAH6glmyu_QZiBsqNeYPwyNrUZQwexJAu9aKfDqZ8LzhiDk8RfDoyXZj-hqn-1Tp2NTAYLlqWMf4BdU&refId=03Y5t1fqFoUyFxby%2BnLZOA%3D%3D&trackingId=fzEvjlU1ritywG6Vw%2Bh3Cw%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4138924341/?eBP=CwEAAAGU-siYjl61rk_Ie0E3IpfQt8IeHvm2MwVi-eHI-2C9dfxGITQ8Y2gM7Yr8W8zCZp0-HrnmMjwPk5wPUWyriAqUYGKmDVqMlw2R0dCscr1hd0qGLyTj_yut2F3WD4t1VzfT4yAK6d3eKM76P4wWcNS0cTAvHkdvlRipWXlR-k94b2eM3LGOdOeG3Lu7Xx8GJKgSmMeTaS2htOrUIcz_98ZVwQ26Wtt66J_wTx07eoR9fn3mzuCEF21_keN_vVORqw6Yot7BvGZArUyaYHqcwZOHAIEur3oXoJY0kU0bLVCuqCTfqGhB-aeg7BFgxnhtB3e9bQ8Fi60ALcdHbO95VcRNC6ihf6B5yaWAimIxjAmMzPi5-kFaPdMdpoQUM9UYeK9-pwnis9raOantB0pSKywbZOQk9ZRUo29edG2PYFcHdGeJ3eLFe2d6OLIPugv7BgycABm7sUxLvfMKn5DaEOnqF9e-jriaSf0HMMQPMuSlR0KUYpjv_BMMGA&refId=03Y5t1fqFoUyFxby%2BnLZOA%3D%3D&trackingId=EJyIad%2FgYKH67gTfahif4A%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4075489487/?eBP=CwEAAAGU-siYjk6pzmqDnZHpv4QkIsT6jAcyeSdx49iCRsvCUsVi97bI6ByaXJ6c67-hZ0z6m6u_oYiJ6Ekss1aEkNBBNzZ3y5F7Yrj8SAmdmgeL6rHktYQHMFkSjTvQ9ZU_swOJ24YrCk6NYz1m5digr0p6mKkriajx0wI-knQcQIVN06H6Quu-fMeneas8ODe8ySBCjYBnAWJz4vD9yD6jp12Yli_PInTiwvlCmwAHlvhKle8crQBzechxAYNFr39y2TtMmZvVhyUywMK0XpQDSRTIpdDSDDeEWX2kvrMK-9sRkl35D4mTp09L18dVFUGBMsihADdPX3AjNCvtAau8D4rFR2KbLQHq0pi2JRROhvsamT9QCnxQogux_heTG0-RwvD7VKFFs1JBuFHTaGL2Sb8KMfD8kKvgDsl5V6QtIc_Vw_Wh7j7r4FDjVP_I7YlZ7uShaBNRsa2MrvCk-ekt&refId=03Y5t1fqFoUyFxby%2BnLZOA%3D%3D&trackingId=b2tGAnzjFOslUA6RhyZcVQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4148458348/?eBP=CwEAAAGU-siYjtsZ4zLRy0E6T7aAVfZpbP7dKnMImpIQNxF1a8wED-fL_vOCrpf9W-Kc6DuRhv9Wx_wUzDN5pR6GQsItAbF4dfcers96awYPkzk9tw8c92oYuVHvxX-hex4rfk9iUbuv0RrS38ya3joZQtXtSUB7Ess9pGBp0pNMVFpXIU8hA6RmqKf332XwmCgCsa55-D3ypUEEz3-1wmIdHur-guAajPTMy3yI4J_CN2Y48XbYLy6gufWBxV_thx0XmK-t6XGkTkrQMBEJvSt3TgjAPYsMBrASOTa8oy49SQxqu3TlGN2Pfpmb_gEG5vviyRCTI_J3zXDq-ndnwmZ5hvUkg9zpEY86RhbBrEeIM3UG4sf1HvY5gPH1hdv2OlunGQsppG0JFSLybPxJQNWpkzpJLDhPRVGYIzNRY2W8JbhbJCdKzZ4i-7gf5wDcFHvHpQGzVS_wHU2X6gOXDDQc9yc&refId=03Y5t1fqFoUyFxby%2BnLZOA%3D%3D&trackingId=w6yAOWVq66mlOsIhUluybQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4151285838/?eBP=BUDGET_EXHAUSTED_JOB&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=sa3rnyfnGtceziqJikR%2BzQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4150470802/?eBP=CwEAAAGU-seb0KtoRR_Ixxpy_VohurTQ5sMDZEbMk3h__-zZ123OWr4ClVHUJFPXAwIUZNJFQ1BZF55sTyy5nEeZWgJiKac0f35VhlcI6NEOxGDTv1aCPLWfWUUhZqiYaXtvkrkyaVRFwFWsaWrjcdq-l1h6uW1lM6g4TwdeHWWQ5gHk_me-lIVRlezVbWpirOfIv9CpJV2slwQbtXMNTHlMH7xl05RaAN02NrraKh2SJW-2z2gjGLozAtgDVguQpKuYrJRq6WNLULP6pSZvSdb1RxKknPIh2sykNZaKzmEY7wFq_EcmJTPRwNSFXlH793j_urKr-U45LSJKLlDDbz38AT_gC3eCFHFHwB21xTZArKgXec6fLuz6NrB9SGqmqSHprAD6ztPkWaN6BiOOZ6Sl5OCN_GoGLFqvZf0slyrIVdxfOSfk7N6K5jEp5soMFCxyENADKS8i_h1030glxyWHT8gP7QmnNWMkYN-7sOxxh-cjJtT0AqoDHnWlkg&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=8WE7kbqF4%2B07ChjLWsidew%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4087720210/?eBP=CwEAAAGU-seb0N0XANnIFYA-4psD0uC4NoZYhW7BVSomnruFXlJu_5fSmxh-ioFoeRyX-dNdT5CjEN8ydaHCRa-P0WIgxnaEy8sDNNUCCNZmezh4cnfj96cxgnw1vMZme09T_kh8lgxqEEeZouTuou6nrd15ed-2rvZRjS_aH7NtRNIMONVRhBaVBBTavZCBfc2j_DF-_SnjJzjrp899hwP2JhIq61UHyvDNPWko172wWO_tmevSmlEav8UV5sU2_SbcQxNX0zmt_c5fE9ObYnhBdNV_MNSnFtsZf7bg2oijAvqRUgpvUa6L4aFVQRjgN5_3vAS8Sl9nIl4q4RJMrSaRjwgddjHqiA4bz0bq6fFf6Xc6s55PG6I44J8zYDOHitJeKYTsr_0fKOhkYaB9vusGUR5xyvf_WRM9LtbYMMFndF6-YuNwDloSCMF9gn9WMI6LLLLvy6-uvpXLaQrScqK5kAzn1VY3JTtUGgAgL_GRKC_0X4ZRTHFQk0qhPrO-8is&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=McHSccKHpunHqSBEyQnvJA%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4076494626/?eBP=CwEAAAGU-sebz0SyvDMLt0b2xeNFmpY-0yog4mxGeyDqJ2t-utrrKxEL3hADmDtiv7X67ZDMkCJKIEhIE_tjxKA3lP32nOwvIEq-_4PAIOpTs2gK4azJH7tBGFQ1PSlfofh2VkrprLDTot56BQEt33a3KO0vIpT1d6mfzJrHxdYdwfNzmEu--qznDt4EwFEtkUgY3MaCdnX1htw1HzRcMuk_sPX8Kp1oyLgB21ZG8jiQl9KEM9Z-pxuQiGXDaYBL3-gxYrS7XpPQjF8hGs5FN864wLPpvsnbI2Vkw0VFeaF_DlTb0jvfWgLIHUq9WC2ABNd9Z1UkuzhEPpZZi3opxyTJKQQb7jf6w7X5ltWDzs2p9NdX6ioSVgHQlbPT2lxa3vWFHgZpqSl7CX_C4zxQQ8Z2I6vsFfjd8RNBmo6AoT4ciK1KIpSRAc0Lt_ypSUeo9qB8-Gn-xaF9OyB2AsknaJKMF4cL6tcfkPjSgcJB4oGI0h6hDLfZAW_PqgvF1Irv&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=eh8R%2FJ%2B4n%2BjNZWV0muS0sw%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4074837971/?eBP=CwEAAAGU-seb0BJ7eI7rg2iOJLLy-V-Znj3bA22eelePwfTRTG1OiUqVJtb5iM1flodtOidDm9Dik-09BhdqiESVJYKqWm7brF_x7RjGBon2GiHBilj-wStcBwGgPxXh1p7w9n4hZN2uyx6BfgocQ4tHxacvYgYCeo_e9P_qOOTdvs_vrLZh5-A_I1ZgQbjV68OnDutvZJamLUNaZObLXZyrdLgYj5yuyUTzSF6yhtUpt5hPxBTolfK9FQejDhYjgWbJM5Wh1rgBvv1jMCHEZ91ScWcX_DjVPZKAYQLvVPhTaySSOq8NI8SS0MtV7iIsK4lYNRNuULiaElSY_kOtgpYFxoVbIMPVgHXACruDFDWQDZ2ZDM6QuS8VSH-k5EDSGtqngUZpREXuwyCENJpjiY7BXofQ0TrePj_P-59QD8EWR5byjGgoCugggcRCeBcmUrPw2Xbvf6Dsm8E2jGcg5jBkw961hd3mSybZSgfWS0C2OvCpfKm3LtdplcSsTP16&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=XyLMxSqkpzlIqK4YHed4FQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4076497124/?eBP=CwEAAAGU-sebz0iHhRjbv94xcRHirGJRJdIoCyy5iM_-desupygxKLzR0-LyosvHNekL-NLhsCgUkWiIPiarYWvv41H3yh57vW8FjrC8zs6Q68prDxgdj1ubnK9WBhQSeH6gAI5w4pBLxk00AbEDcH4eMhHT9VCLuO_uajcC693cDwSQxvv0Vgf5hqpZj237nIKRL5MLb_KuUEtFNkngZFwRR2Lisaso9mSrj5iMiUqvEtz0dKcrOeED-2oTUgDcxY_A03BCMVhBbZ1nMmGeeFnxG4f5SXEztISZ1WzIvlc2rNIVipqfQmdOPxcwidyPHEwhqlEjV7mIqEBwO6IAO5sSk2oBoqNLmt-kCl_PPUpdWBVAGfoIBZMGGDbSDgJuNpJ__kC-MVrglkUS-65VwM7ICYlClPpWHKFDrmoj6o7MyfUVD8onzjsXs74_KKgYO_ZVh5yeqOJKgP-5a81DBw&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=kPeImCOMzu%2BCH5eroFi4Pw%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4131718052/?eBP=CwEAAAGU-sebzxEEdowwPw0f7ETHegdwX2a5vjT76x3tBxiiabAUVaY0cTynOZ-CjQ-hDRnLCxMWKOo2aLKtHHcqSBk1eeVl9Pn_rRDR8AZqkkNzfbwQDwHlzo3SdWR5eo4gyhS4iZQglOGMZCgNE9hqG8bRcHiA2PyOCwOogNmBOZLCvXfWATZK7NELiqxrENXRrIp_1nqsVaZnAIBbXYQFm5ZVV8oYqzoyYnRu_L5_0dfsyXHLany-6otf7uCmfo3J_kJi04iQe5GMKdRFaaHsMxmUWTCgYmDMk8zOZK0WyZ9-SQJjlNCcq4sAs0lsraIh1eMwoZBjjO7pcUGBiZAyEMlos3fFpGydTDcougbSYcmiQv-3NOK6oyAOTuAUihLzL7M1WSchZDgSqe4pvCKGTuofUu-bGrKxm43Hs3JdaV29BXKjbtlS6Dw1sTSARdGZNNIZ4zfAuldCUayuwDp0Tz6s6pUj81exs_pABvpJJWn0psVI-uNDnpBMh0HFjQBxOvvOkMcK-Hw&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=y1GBIw%2FoSEa1GCXjEmbWew%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4148460707/?eBP=CwEAAAGU-seb0PKORa06jzfH7R0JgJKsbii6gc5o4wOukPoHas3_RILe0Ubc6_Gc6qYagDHLC-nwvqC1txid8OPSTz12relYCEnEswYvaqp66wZ84e6UGCJ-R2ZVe8pa61-ExppWq5iCE71qQllABOUpdIz0uv5Rk0KFL_fMWoSlQsu8115jRd-Kn48bTYeZgOaz2bAlxZXI2PUM2cmGeLddybyDS4JC3l9xjjqfEC_ygpv4fyCp2KU7RRIlw-GatV0ONPu1XbLgJxFOvkFwsuoF6Nyb7YfiW7xXlZ9fvvrL1sU6wh3mx6DJWIOPGA-IOc-7Au0n36hqOhWwuJliZDywETI81v9qAbkVsEefsx0UyFREE4jVZhM7mHMD9N2SlXD62v3KYN5ybeCYI7NgW_V4vlkAJ62kKI9vhFeehn6dlznWzTDZSFSN2r2rhn6_wRCQ8_6EDFIaMuXizCWQA8hlhc0QTyzv6Uh9_3QkDFLLoRcQyw0kAs3Z7SsuzQ&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=CglTOducoKw3TMu1aGVudQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:32:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4148460721/?eBP=CwEAAAGU-seb0CV7RrvQ0wmTjH1nc1TUKtIOfMjgb8JMkQtre-yDAOOifCJi9NGf5rXMGuT8zNkmExjK5uNQQpzjTWmGnaJ0t4JQbc42mhHkEsdZjFQoQQiRftu7_4k7xBZMrTZ7GnFSwo1jfHSRTjqtBcF4uBuW5YBjBtcaRejeSnI_vQRAitS6oleCf5NqMvz7TRxe_UX0YhvoHYLakbE0R8QgQXGD8XrullcQp-k_9703Fyhym4FPgGmkj6xi0NtcVrwnvVghKlXX6m5qjvrDlF_zZjycif4_FIZIV7htarK27fd43SvXHFFSOapyGfJ9VlpLX577LCSzEtrFIlGMW7IOkz3dIk3HgRpPDKLD6i77mNitzxOISX07yPTbVV5DEUTIhmXN1353kcE227ACNnlz32cTThzZnbryGrTSwyG6cTLz1L5jK3lAMpkNCQdKtJ0KxyGtTrRZ76Jri63QPAkGCcTwLWXZwHu1A-QANJrVkd-JHS-XWRmzCMOz&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=Cc29W0f%2BA7AKTvkEXnL0zw%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:33:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/3711352584/?eBP=CwEAAAGU-seb0Lxwnjab9JXDTI94-Asaof5x7mXgYKz3EcxDwkAl14Qqp5xIaDVtzpRa63qXPH0Tdg4YBXScVhqkhOZbDhAZMmnlm9zk1W6dhygqz3_gkPTPW0DYz_PSi30JKs9KuuelhuyPwEojh3hv-2pGvYTtuqBOtM4ZT3qIZVUREUaRxYWm3x76kcaQReEL5aaWKS5u4wDjSIDVmVcGlvHT5l-OISdC57U06mQfBJO-uSi8QRKUq6a4KXZbBW3kFDbCr4ntyyA2tPNE3_q9V0C9K7DrvNXgY2isbn1yxlrVeiRXrNab_COM_whsybzvSjEseC1jsjBSPNzExiOLXPePy_VwLHuiHQepv9RZkj843rs7oOMr3_NHhcNK5gRxiXXyToU_hFkH7Ctvfg8jM_Gtshh6nR8FoKFld2tYcekqXknUbAhAMXiBZyer8gP1jYv7YfcACa8WeM7W6Jz8jU608qPOMHRKalbymY5RQP8czZNTB107-hIpZ1i9&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=YibNw1ID%2FD3m8gxl3fkJwQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:33:01 [scrapy.extensions.logstats] INFO: Crawled 24 pages (at 17 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 20:33:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4131717085/?eBP=CwEAAAGU-sebz38XYaruXOzDVrfUjhvV4XtjlK8imXI64pjPteuvlCEm8FlfQwwScMsI6JAk5483MWziGUJU2VPmIfw653EYNCnQRNMM5RiPxvq10STeb-YVbtlH5uZhFyQHBLeWgx5Nz4OEZM0nejKspZj3ucB7vB169MDKAYCOWmS3e1eALxRcZRpJi_vVIuRmQuSp3FI96yKFGYt1DoJcABjD_D-kydC9_8fTyeIEMeLXLQHCeBcEWqiB7165bqyVFK5G1S7qtMDtugFdz84V0MvqzqAscWN9trMg6PVFpLfZ4047sgH1-21OQFK3xut7KLEIZ9U0LSiGoPHjV9Y1rL9AOS0raajLHaW1WqF7FZQXJXNzO6Fx_glQmwk8KOHFYXscjWY3gPWVbQljJxN6F5z4zG7wUYuvbD-Id0z9QaFXQuQbUNvvyJXzPQgt6urNZg858ioezBdZNPC9BLMa6nqz4ehhZ80n-bDgbyNiVUWK6PdFrjSJAbxlDSmWmMBGClB1Ye4IDek&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=S4717ADaFfnQ5PNEjHDpHg%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:33:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4148458916/?eBP=CwEAAAGU-sebz7ooUTP4fjq3WsSfgNB1Tl_uQ7CcuoNh8sSO5yvLIyc9STRN2huUE6IE2lqfMRcynrsEJC23dpF2thwSar5a44Tx8GBoJAq_GiRFRmyaA3_QCU3mpdOavauGtoDR6544ooglXyHyha4tqNBO_--7J8AQiJeYZ_0bOocfFHJ6Do21GXcVPvghpjTWST3XS73_R39VhnzsNviVRW9_w_z_-2ZUl3aZ_WGXhIUp9U_J7T3E3OJgZjmWkYOu4yLGH8NJcWsmStMJdUntfhLz5Ct4rds_7xaOCevkQ7X_rcRDrC9uT1DRu3VEyObpjypgZxY150zNyeSZFfLB_fVmJ2-WdoP76dO0BMzWFANlVV9JfE-iLaQpcbK-n0tpHLQFTyvOWYiYWjWxFY8x6r8U0AaSQJ1F5U0oTF7sRFCI71pA74O2TTqT0umV-YXC89uUgT2PyuwoEpqmgQ&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=0LRo9nxhEVB54WkIdhePBA%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:33:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4042556466/?eBP=CwEAAAGU-seb0NLNB4VCLb7EJu3JU3bMQp3sWlr-XBx20N8HCjmw2PFjEcG7bu2Z_WQYHUV178sKHa6D5ZTu7D9hMVTMSoquz0SvhkHqLgu10OJS_6iSTjOkjsRG65Hjbze672qrg1ja7EnitWZYUaOIYk0vX1SKhxZEivos_UqllXf3S5Zx7FumRwb3a56Sez5R7NVgziqWyLtNIycPo3lfZ9I5o5a-Mqej1_W6gnU6os2EzNlPusGntLxMrCOUB9BBaU_87HU2ogYViWfSY27VytP1--W9WvoNFCNu0p4EikD9FFoI9Q7lFdKsS73Iu2rn4DwFCKOb8kTjeHCUBtBeQVtoayeTZcYEEPuXOgdNdPa4lMLFRUG9hi0pfoFlVlEeXICsjPP0BBlKYhYvKyEuvsMqcrmk7_unGVlh6ASUkevuCAr51IxuO4RxQQDVOvN-rnUzz1aayV3nj_UTOpTKDoLswm2kJJdfCA1CtFJOXs9kyzUGvQJyIF34mflfph8&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=IUoT8V7eA4fxEnUPbVF2ag%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:33:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4076478763/?eBP=CwEAAAGU-seb0LVlvneDNsxe-U1VsmFzVx0fVCkkgqqpcKxdEIpoTco3cGA3Vp4WUffQNhKaq-taXzGLL8qPZK6niz0-Ye7E1-2FGRzn28OmTip2K-aJ3xQGgvT25pxRED9XHUpGlqUsuRcTtysrxluqkDYouERRJxLSAeeHuq12HaHKNY_Y7Fd_iPVHu8KBmSR8Kr0PKlTMD0rh247TTWF4JhQEV-yBlirrEoarFYyg9QI7qvl3Z2bpOo_kkiWAzognMLrP9YsVHqUTawj3GqDjyOQFOB366EpEROm7-2yohhNSLdPMxmF78VzShiK1IU9Y2OneTJcH8CHFSCC0x6kiLduAvc6EtoGXKmeE9HAS7Wr-BE9psoqBECHJJEmxQHTezLrL9KtoD9P_y01UE-O5t-Cn2DBtrIie-r3P4NZv57gh4aOMwXc2vHyGtdFf58hjx8KGEMx2GvBa9A1cEpfes87QAgC761DjjJ08oBecFLYD8kO1WRhcmBNjOA&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=vm8DhX2C8e%2FlN04TSabMaQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:33:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4076480459/?eBP=CwEAAAGU-seb0EPjLrxEMrQOimTGo9vIsZmg_SEPvAI_4x9OFufEZMpFmKeQSS_6CLdW2GL9k4wlJCDqD3AQSdMHZpi1f_QvdaVRP43h2KoE0WXWgiG9q5A8SQ5vuXpoHZByBBDU-bh26rS_35YrznYPvmseuj07sUEsXyLCJ_6LKM_6rASqBAYGro_8vYpiWoqRPTHloU4EeGsmoQtcwxFZrjLhjLBe7It5tZsmxP35n3TumrgRMOw2RS-SijyL5jK2oO1Wk5SZir2gsIa9mnDpB58DbuDYt3UxTSeqOG-Dku4xXtOavQ-4hZNpkfCRkWuFOCfo5D2ePl42T4cLIat1dmxuLbVP8Ig9GEJgwXoFCefIFkbrmkUZ6wl9uGwOYlwNeGMkdaEeh2aC-9wbUf_TQQ7C9QGkb_c7OoCPNRNdVGdzkPLP9RiunC0limmp-JIObq5HofsLWj7XReuAHRvrYuoHLAGMuKVa8mw7lzBZ7dFNOiIuX2cX8wUFww&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=zA5Bj9R3DVBD8NPe%2FdCPwA%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:33:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/3824082761/?eBP=CwEAAAGU-seb0MQAQSumtzCfat95vc072L-5RVvivCQyBKXkgZtvJfQoWZKeP1aFvXDW_RFwFneQFfyecL09ouZ8CmhX6_QG4p1s3vNIiB0IZMETBtAnvD0KPQddqTmq_C6nTaAru2dmqCOv4K1Q5Pu5xPHipKg2FjKG82zM9Z6diV04PajYx3dB5AoIY3vU1EKICRjvEYhL6onY5F6A0e-RpFNlIBhg-WYger_yFsauERIGKBi3yslNhvqq4DA2axBKXjDFtpbvEYo_u-56Rn6Tb9aY7Q2ry5UYH6TZI---nQdP84F8hQsIDdsZsLJMRTAdayo56TUlsPBhMXZMes7aNVakfxe3oFB3cQN2V8Yr_XgsoA_u7U1obRmvS8lzrEnu-0_csuIsWksrPXlm6ODALQAJZZX_enhNhNmUeKhS5Uyqov1eJP4CNdy6W3pOPpN-DdbPia_hQwu4O_hTI_aMAEGQCoO1RhvAeQprx3X_vWhzISy8PQcPYrXVjw&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=O%2FFH%2BD%2F4rYrbAL0YuxIXIg%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:33:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4077863964/?eBP=CwEAAAGU-seb0MCW61JgKCQVsrEV7VCAsI2F094k-mKtUjUoueKn6i0iVoIUt9ouzEoaHA8pgmqJjbKBzJzge4W1Qm4Slb5O319DsjpAYC-Izp-pYJlYItfjLDkiVyzfw3CAZEJfkyAx46WyQGpWO3pY1FoFflbPBALnuvK4jtNpzcG_ToJnHjEGRSV-6mrIr3OMF5Wgs2OURUPdlhl73s3pmGEVSya2Cw-u5sXWDeDEnvgxn01Xn14yTGmodcRMezZoO4STeRHXgXwRr0LM_munArEAFfdxmbr3PxswC_zAlrrP_VIGZmIlFN9B5pbJ8AFDYmlAilNg-1aGb1f7miokp7_drGtOS52oddq_yORZtc0kFuIENTGlbjn-NTENroPvBRDvV4PZNyWhyJMhmwaS2GZsyULift9gfHm5MlZsuSwkTqSqLjxw8F5aa4wRgnWxQHst6USAy-_5FGS3Pq-obaxZ2TsO6eT6kqv-O4vtUXVlkbqTyKSv1cs15oWl3fY&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=6I1bKoBJy7fMTcBJShk%2B3g%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:33:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4150440715/?eBP=CwEAAAGU-seb0HjZdaCnsncKz8EVkZBIQWVTITRtYtcSEy0z1b6-Ox04bluFbMcOY8OkIUlUVt9EeOgdQc_xN3JNQPaRw8lgMpDzD_EFCI2k7K5kvU1SFJPlGEMjqIwVLH53gwc6iInC1lS2xkMsl_gC5f9cxlmQBjdU9ZIl3CdHmly5u8wIIJGxFWFtW0otRlkD32lxjxSL3azZFg9LCQ8Gc4D9hk8zym1czQ9Mg73V4V5NRZYldTbxVr4zYdrCla7CzVeTsKST_mIOgJwwoJ4oSPfl4DFjKABo0KNNhshRYI5Bl8dL5dgjUbAMp913Z-jwLxZXDku5eEO5L5zKOtTvlsNGGqHgnWRgrPDCeqKO2WEAPyHCqxONJ_cuJkl-J7KLKdLIOWzkQvaaqQ-I0kMgvUUInt42OPznCQ3kT0XxpVR_cjHHiDvbU3-NRweiWJcQ_M7vg8rVz4TmDrhM7Ki-XhHqBHdS-jIeKelZyrT2Ii8Ia4yQAP_ccr8KZcz7nW60KEvVHn3w41kBba_KIhK_&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=ODdtLZW21Qjy0gREr5PR0g%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:33:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4073323482/?eBP=CwEAAAGU-seb0FYjokQXbyEFbYZTGZyDKFgfBjoZpk9HiIZUJQ8WGComu0n4ddW-gq3pDDvEqfdcpD2uiGWpjJgtrN5dHlJvQOstqOiBL2myip9yOiB7QoXA4uzh4ts6z1EhLYl4Gh3bKvDWHRek5u-khGJzHYAZO0mcOL4eB9Jq4q6t2mnysOM0MOD_MNJ6HUXrDImFBGO08hA6e28lrcTIhhJ9w3GISC-0Uqs86APCE_UaKvWwqrg8zgU01nST7jpgfOAM-6OwuOdme46Hazul_tpJxoWQlRIs7ABWGWL-qayFMrXdMBJb5DmUlwMqvMgU4m3V98OHhnrOo3bML0mCUQ879UGOK6iRZpIvRpD3pgGZoyz2sVMcTVz2EJ5HzeKLDx3M5rKyiS1SMWm1W4_JNjopztsplotMcOLR_78RiBdBvblI6K5im6NxT3OuZMT5NJx_Mb9Eseyrcc802XZDe7Xn-waBIxEn8-TBtYxbP4YrmhsfbSr8qO5laA&refId=KPm8c61tCwN4xsgcvq1llw%3D%3D&trackingId=SYSLihUyXNRzj1dFOCMmdA%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 130, in parse_job_page
    self.scraped_urls.add(response.url)
    ^^^^^^^^^^^^^^^^^
AttributeError: 'LinkedInJobSpider' object has no attribute 'scraped_urls'
2025-02-12 20:33:34 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-12 20:33:34 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-12 20:33:35 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-12 20:42:15 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 20:42:15 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 20:42:15 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 20:42:15 [scrapy.extensions.telnet] INFO: Telnet Password: 98db3206510b6651
2025-02-12 20:42:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 20:42:15 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 20:42:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 20:42:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 20:42:15 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-12 20:42:15 [scrapy.core.engine] INFO: Spider opened
2025-02-12 20:42:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 20:42:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 20:42:18 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:42:28 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-12 20:42:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 57, in parse
    driver.get(response.url)
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 665, in get
    return super().get(url)
           ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 454, in get
    self.execute(Command.GET, {"url": url})
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/util.py", line 38, in reraise
    raise value.with_traceback(tb)
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2025-02-12 20:42:28 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-12 20:42:28 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: linkedin_scraper/data/linkedin_jobs_20250212_204215.json
2025-02-12 20:42:28 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/linkedin_jobs_20250212_204215.csv
2025-02-12 20:42:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 450,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 149863,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 12.620898,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2025, 2, 12, 15, 42, 28, 397543, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1298841,
 'httpcompression/response_count': 1,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 14,
 'memusage/max': 73932800,
 'memusage/startup': 73932800,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ProtocolError': 1,
 'start_time': datetime.datetime(2025, 2, 12, 15, 42, 15, 776645, tzinfo=datetime.timezone.utc)}
2025-02-12 20:42:28 [scrapy.core.engine] INFO: Spider closed (shutdown)
2025-02-12 20:42:28 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-12 20:43:06 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 20:43:06 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 20:43:06 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 20:43:06 [scrapy.extensions.telnet] INFO: Telnet Password: 3574fd8dcc272e2b
2025-02-12 20:43:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 20:43:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 20:43:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 20:43:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 20:43:06 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-12 20:43:06 [scrapy.core.engine] INFO: Spider opened
2025-02-12 20:43:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 20:43:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 20:43:09 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:43:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:43:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:43:36 [linkedin_job] INFO: ✅ Final total job URLs: 7
2025-02-12 20:43:38 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:44:30 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2025-02-12 20:44:36 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:44:40 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-12 20:44:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4138927056/?eBP=CwEAAAGU-tRj807O-8gFpL2aligNCCtIRH6fuBQgiSAYA5soyHpPpbHL-KRbI25bKG-81BUDxaSxlR-Ct_BH7LH8kJ1ZUji9nX9x_lPw2bX5sUl-8YEFBmo-JIfC8wAiHZOHuPZKgzDTGOJJx0Y24PYhKsrZMm5L6HTwOk8YM7xMPprLOjGZiHM6MBGvWx91ECPhBMZ-Jx2fma90S57xky-vf1Tv4ASycfV-qMhG5lq5g4LS4lRK1tdYtaua6ScJ2PXjBoVR3JXXHak26Bmbs3bBdVQmtDb0akxJqLBdTyyAUcBBnjFyBrxODVlpdowJHXFrckhKCOm76lkTDOcQWNHCp5TkCCD-dLxuja299QJy3SjBkKToDwV3JRriFWGDy9rXYu1XjJAe2MDVj6aPk9wUeDhCTc3LKv82n9XO3N0MZh0rkjrb7zLhSDJTeIxDTpIuSltOsItWJECauUo&refId=hp9ySwqDfzTNtSljjTFs1Q%3D%3D&trackingId=oC9NBMZkb3ZJ61Q1ByYTLQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 146, in parse_job_page
    driver.get("https://www.linkedin.com")
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 665, in get
    return super().get(url)
           ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 454, in get
    self.execute(Command.GET, {"url": url})
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/util.py", line 38, in reraise
    raise value.with_traceback(tb)
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2025-02-12 20:44:40 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-12 20:44:41 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-12 20:44:42 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:44:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4078147490/?eBP=CwEAAAGU-tRj8wz2mGIV7dqispBC4wQMlzKjut-_rSXEELSLR0Q-hoxrFb9pXWUhr-36bJqMoCL4gqngeyb2uVAMTLStLWWM2ZshheRqQx2I5DBgeOwpxHFEhJd9kLoFXasYcOVQ3iCea66v4TdTz8zDxPga95FO61eITZJ7R6qAjbovXetVOGJOdBvPhbou5BgbKDLISGzjQlClv8SZwknyPmgePMmtXaRPFlDrp7ZvWTzrOAz-4QkU9W-TFqPIJ-YJtlp9pJPgHorm6URYLHmU6iFvfXC7mkFBk31bBnnBDQIUdMMM7ugAJIJQZjZGa2XDR3PTp9rUfOT6hcHEVx90ldOVhIBXPwxY--9CDkK8RbYJwpNTZ-ZgFoutEDimYPHa35U-jlrBxcOEIvdz0380h4wN4sXki_ZAQaHT4mv-RlFtnEhqE3Mq-Sg1qJ2W0pIz4h2IFUPPuWG7Wb5dJ6hK15mlsjL0px3jL4y7pXLeJFbfJsx-E8Xo5bgDbxHM&refId=hp9ySwqDfzTNtSljjTFs1Q%3D%3D&trackingId=l6iPx1Lqt%2BpKTBwXfTOC%2Bw%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 150, in parse_job_page
    driver.add_cookie({"name": name, "value": value})
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 767, in add_cookie
    self.execute(Command.ADD_COOKIE, {"cookie": cookie_dict})
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 429, in execute
    self.error_handler.check_response(response)
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py", line 232, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: invalid session id: session deleted as the browser has closed the connection
from disconnected: not connected to DevTools
  (Session info: chrome=133.0.6943.53)
Stacktrace:
#0 0x63965e20b09a <unknown>
#1 0x63965dcd28b0 <unknown>
#2 0x63965dcb843e <unknown>
#3 0x63965dce0e01 <unknown>
#4 0x63965dd522b9 <unknown>
#5 0x63965dd6f505 <unknown>
#6 0x63965dd49753 <unknown>
#7 0x63965dd1538e <unknown>
#8 0x63965dd16b51 <unknown>
#9 0x63965e1d476b <unknown>
#10 0x63965e1d86f2 <unknown>
#11 0x63965e1c08fc <unknown>
#12 0x63965e1d92e4 <unknown>
#13 0x63965e1a49cf <unknown>
#14 0x63965e1f9cd8 <unknown>
#15 0x63965e1f9eb6 <unknown>
#16 0x63965e209f16 <unknown>
#17 0x7fc9f029caa4 <unknown>
#18 0x7fc9f0329c3c <unknown>

2025-02-12 20:44:47 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.linkedin.com/jobs/view/4148711597/?eBP=CwEAAAGU-tRj85ZzNpPEW1zCb2gkg6NbPtNN8Lkp4z4ixU-Y9OUOP5gbHiKn3n-r4HcTijpn-X-XiqTJfz7HhaOTsBSMnAwWV9nfL7hVks-HMOt8e94e-s88fbVSassuVpPolgt-xGZfNDK7KUIINPZFRWY4xy5psyAbeZt8XofsQjK7flBHz2EaFNEwNT2nhV79BjDU3Xnkq7BBqrxxgxr6WsN6bAsXTlpJtvZnDZ1dWODKPVP4sPvh21FVv8z15bAuuJtoce89Y1-eiK0dHpUgCPAj3rPneFgyEMtkvfZbtyLBH7fablVVb_t9Oc5x2zd-mXN1NY-ymqYyssM6rURr_SKImPHJ60IaNdj0QjZqP9xjktUuFmtH7yTub8wrIBM0kFxZNlwTnPA32A9ifHoVt1-d0pNk6_OCiTR9v5BgntjuaubEJ5IdNw2rt85pGZH-CA8F-Kmyzwb-GHbAwZwey5QuYa_lp4YAGBGZ9gcmXQ&refId=hp9ySwqDfzTNtSljjTFs1Q%3D%3D&trackingId=qc80evpNVzw5MhI3V2yt2A%3D%3D&trk=flagship3_search_srp_jobs. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2025-02-12 20:49:06 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 20:49:06 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 20:49:06 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 20:49:06 [scrapy.extensions.telnet] INFO: Telnet Password: caafb19481da466a
2025-02-12 20:49:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter']
2025-02-12 20:49:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOGSTATS_INTERVAL': 0,
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 20:49:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 20:49:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 20:49:06 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-12 20:49:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 20:51:14 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 20:51:14 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 20:51:14 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 20:51:14 [scrapy.extensions.telnet] INFO: Telnet Password: d4af68f744dcc134
2025-02-12 20:51:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 20:51:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 20:51:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 20:51:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 20:51:14 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-12 20:51:14 [scrapy.core.engine] INFO: Spider opened
2025-02-12 20:51:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 20:51:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 20:51:17 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:51:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 20:51:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 20:51:40 [linkedin_job] INFO: ✅ Final total job URLs: 7
2025-02-12 20:51:43 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:52:24 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2025-02-12 20:52:31 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:53:28 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 1 pages/min), scraped 2 items (at 1 items/min)
2025-02-12 20:53:34 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:54:17 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 1 pages/min), scraped 3 items (at 1 items/min)
2025-02-12 20:54:18 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:55:01 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:55:46 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 2 pages/min), scraped 5 items (at 2 items/min)
2025-02-12 20:55:52 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:56:35 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 1 pages/min), scraped 6 items (at 1 items/min)
2025-02-12 20:56:43 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 20:56:56 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-12 20:56:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4148711597/?eBP=CwEAAAGU-tvI25oQhx9zqA3kSVjOveh0aY77d000SlA3fGzdjL5ZvXkC1cEb1nezkyVHe209AZrn5QBaWd1r58EcBVujxINSjoQbbN6unv1pRer4vdlwAqNYQVWqUJbadwZI40IERqX9dHY2lxJ5OWBWjs8i1GmKisgBFF52Na3emPdI4uMlxB-z2sAG9PEMSUzBLAzqHcNhihT4lP6rGhecm2VZ1DSP_AfOabn5-q5wC9z5Y3TRquZozPYZWh1XJ9xns7Gq_12--OfRSs9pBKE8hSvny8XOj9eycgxi2I3dtpa5TrT-IZWtrSX1ge97OE3fCpdf66tehEDM0IrT9EHk0GILiB3bXnfKdciLfcnrjev_QbbktiaXgkLH4Vi2wG9Ce_75ERDVuknFK0xfJRvURh-YjVArxSgphsftY8Vpv443X-lXVVlj9ZKdctc3L6s0TSJpIIjZH8qGttzCCZ-Hw28RyieAIKLSnndflPCr4g&refId=h4gVC6pyh6anAPPAPfFjqg%3D%3D&trackingId=JUeo4Ry5SHRR0D0%2BPCeJvQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 154, in parse_job_page
    driver.get(response.url)
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 665, in get
    return super().get(url)
           ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 454, in get
    self.execute(Command.GET, {"url": url})
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/util.py", line 38, in reraise
    raise value.with_traceback(tb)
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2025-02-12 20:56:56 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-12 20:56:56 [scrapy.extensions.feedexport] INFO: Stored json feed (6 items) in: output6.json
2025-02-12 20:56:56 [scrapy.extensions.feedexport] INFO: Stored json feed (6 items) in: linkedin_scraper/data/linkedin_jobs_20250212_205114.json
2025-02-12 20:56:56 [scrapy.extensions.feedexport] INFO: Stored csv feed (6 items) in: linkedin_scraper/data/linkedin_jobs_20250212_205114.csv
2025-02-12 20:56:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 10464,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 1146559,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 8,
 'elapsed_time_seconds': 341.748671,
 'feedexport/success_count/FileFeedStorage': 3,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2025, 2, 12, 15, 56, 56, 266759, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 9980022,
 'httpcompression/response_count': 8,
 'item_scraped_count': 6,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 30,
 'memusage/max': 144793600,
 'memusage/startup': 73932800,
 'request_depth_max': 1,
 'response_received_count': 8,
 'responses_per_minute': None,
 'scheduler/dequeued': 8,
 'scheduler/dequeued/memory': 8,
 'scheduler/enqueued': 8,
 'scheduler/enqueued/memory': 8,
 'spider_exceptions/ProtocolError': 1,
 'start_time': datetime.datetime(2025, 2, 12, 15, 51, 14, 518088, tzinfo=datetime.timezone.utc)}
2025-02-12 20:56:56 [scrapy.core.engine] INFO: Spider closed (shutdown)
2025-02-12 20:56:56 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-12 21:07:44 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 21:07:44 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 21:07:44 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 21:07:44 [scrapy.extensions.telnet] INFO: Telnet Password: 5beb23dc9efae482
2025-02-12 21:07:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 21:07:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 21:07:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 21:07:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 21:07:44 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-12 21:07:44 [scrapy.core.engine] INFO: Spider opened
2025-02-12 21:07:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 21:07:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 21:07:47 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:08:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 21:08:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 21:08:09 [linkedin_job] INFO: ✅ Final total job URLs: 7
2025-02-12 21:08:12 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:08:56 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2025-02-12 21:09:02 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:09:53 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 2 pages/min), scraped 2 items (at 1 items/min)
2025-02-12 21:09:58 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:10:46 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 0 pages/min), scraped 3 items (at 1 items/min)
2025-02-12 21:10:52 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:11:44 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:12:31 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 4 pages/min), scraped 5 items (at 2 items/min)
2025-02-12 21:12:36 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:13:35 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-12 21:13:35 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-12 21:13:40 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:13:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4138535120/?eBP=CwEAAAGU-urY-mcmm6_tW6tZmVf7hOdHtYVl7Z2-EMkV5fo8yzD-4wPeMkfowU-6H-O2Cwdo2bcDOxx5_mAMjP2g3YaeJ7hjZoxUhC-6UfqmhFXh6vVkwRMLHoOLZhb6qZBeHQSIrOqSGaKAGL0IbjE7g-5ilGyeccctoUd08B4h8XdXBXDA0NID7-JAmc3G3JneEFHoUWnhyOHWMMW273jIEG2V0kIzAUohj29jLzrfB2cYIgRA-PBIHONVtN0Li0omxN5-EVQP9Xx21rfN-b7LlcOhAYNNNGEfTkVfHRREMZCCT8-X3YN-j9SXCZBdD7nlpx772FL2zWlULJm8NNNUW4Qd5ugCyjNnANP9hy82Rpxt1JFMuJ8K3v0A06Y3A3So7dQwVX5xR2Du0bWLVL1iei2xzWvHVNvqVrHVB8e0Zi_CAk_wJs66znyyRWn247d92nYM29dJSJ62UN3LlynUX4EKvH8-fv1BGxNX8jU&refId=F7froDMMKGCPZHgGdjQ53A%3D%3D&trackingId=%2FspKSYV3GjsKTgyykOssCQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 150, in parse_job_page
    driver.add_cookie({"name": name, "value": value})
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 767, in add_cookie
    self.execute(Command.ADD_COOKIE, {"cookie": cookie_dict})
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 429, in execute
    self.error_handler.check_response(response)
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py", line 232, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: invalid session id: session deleted as the browser has closed the connection
from disconnected: not connected to DevTools
  (Session info: chrome=133.0.6943.53)
Stacktrace:
#0 0x6385e3d2409a <unknown>
#1 0x6385e37eb8b0 <unknown>
#2 0x6385e37d143e <unknown>
#3 0x6385e37f9e01 <unknown>
#4 0x6385e386b2b9 <unknown>
#5 0x6385e3888505 <unknown>
#6 0x6385e3862753 <unknown>
#7 0x6385e382e38e <unknown>
#8 0x6385e382fb51 <unknown>
#9 0x6385e3ced76b <unknown>
#10 0x6385e3cf16f2 <unknown>
#11 0x6385e3cd98fc <unknown>
#12 0x6385e3cf22e4 <unknown>
#13 0x6385e3cbd9cf <unknown>
#14 0x6385e3d12cd8 <unknown>
#15 0x6385e3d12eb6 <unknown>
#16 0x6385e3d22f16 <unknown>
#17 0x7e533529caa4 <unknown>
#18 0x7e5335329c3c <unknown>

2025-02-12 21:13:47 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 6 items (at 1 items/min)
2025-02-12 21:13:51 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-12 21:14:51 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 21:14:51 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 21:14:51 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 21:14:51 [scrapy.extensions.telnet] INFO: Telnet Password: cc809b6b99b1c9fc
2025-02-12 21:14:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter']
2025-02-12 21:14:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOGSTATS_INTERVAL': 0,
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 21:14:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 21:14:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 21:14:51 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-12 21:14:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 21:28:25 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 21:28:25 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 21:28:25 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 21:28:25 [scrapy.extensions.telnet] INFO: Telnet Password: 4e4b21a7a9530984
2025-02-12 21:28:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 21:28:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 21:28:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 21:28:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 21:28:26 [scrapy.middleware] INFO: Enabled item pipelines:
['linkedin_scraper.pipelines.LinkedinScraperPipeline']
2025-02-12 21:28:26 [scrapy.core.engine] INFO: Spider opened
2025-02-12 21:28:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 21:28:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 21:28:30 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:28:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 21:28:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-12 21:28:55 [linkedin_job] INFO: ✅ Final total job URLs: 7
2025-02-12 21:28:58 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:29:43 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2025-02-12 21:29:50 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:30:45 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 1 pages/min), scraped 2 items (at 1 items/min)
2025-02-12 21:30:51 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:31:36 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 1 pages/min), scraped 3 items (at 1 items/min)
2025-02-12 21:31:42 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:32:29 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 2 pages/min), scraped 4 items (at 1 items/min)
2025-02-12 21:32:34 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:33:36 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 0 pages/min), scraped 5 items (at 1 items/min)
2025-02-12 21:33:44 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:34:42 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 1 pages/min), scraped 6 items (at 1 items/min)
2025-02-12 21:34:48 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:35:27 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 1 pages/min), scraped 7 items (at 1 items/min)
2025-02-12 21:35:31 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-12 21:35:31 [scrapy.extensions.feedexport] INFO: Stored json feed (7 items) in: linkedin_scraper/data/linkedin_jobs_20250212_212825.json
2025-02-12 21:35:31 [scrapy.extensions.feedexport] INFO: Stored csv feed (7 items) in: linkedin_scraper/data/linkedin_jobs_20250212_212825.csv
2025-02-12 21:35:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 10541,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 1192789,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 8,
 'elapsed_time_seconds': 425.359012,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 12, 16, 35, 31, 369373, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 10349682,
 'httpcompression/response_count': 8,
 'item_scraped_count': 7,
 'items_per_minute': None,
 'log_count/INFO': 30,
 'memusage/max': 145735680,
 'memusage/startup': 73584640,
 'request_depth_max': 1,
 'response_received_count': 8,
 'responses_per_minute': None,
 'scheduler/dequeued': 8,
 'scheduler/dequeued/memory': 8,
 'scheduler/enqueued': 8,
 'scheduler/enqueued/memory': 8,
 'start_time': datetime.datetime(2025, 2, 12, 16, 28, 26, 10361, tzinfo=datetime.timezone.utc)}
2025-02-12 21:35:31 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-12 21:41:58 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 21:41:58 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 21:41:58 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 21:41:58 [scrapy.extensions.telnet] INFO: Telnet Password: ba3f64352a487504
2025-02-12 21:41:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 21:41:58 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 21:41:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 21:41:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 21:41:58 [twisted] CRITICAL: Unhandled error in Deferred:
2025-02-12 21:41:58 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/misc.py", line 74, in load_object
    obj = getattr(mod, name)
          ^^^^^^^^^^^^^^^^^^
AttributeError: module 'linkedin_scraper.pipelines' has no attribute 'LinkedinScraperPipeline'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/twisted/internet/defer.py", line 2017, in _inlineCallbacks
    result = context.run(gen.send, result)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/crawler.py", line 152, in crawl
    self.engine = self._create_engine()
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/crawler.py", line 166, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 102, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/scraper.py", line 101, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
            ^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'linkedin_scraper.pipelines' doesn't define any object named 'LinkedinScraperPipeline'
2025-02-12 21:42:25 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 21:42:25 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 21:42:25 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 21:42:25 [scrapy.extensions.telnet] INFO: Telnet Password: 10c5b3dfdf54d97b
2025-02-12 21:42:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 21:42:25 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 21:42:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 21:42:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 21:42:25 [twisted] CRITICAL: Unhandled error in Deferred:
2025-02-12 21:42:25 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/misc.py", line 74, in load_object
    obj = getattr(mod, name)
          ^^^^^^^^^^^^^^^^^^
AttributeError: module 'linkedin_scraper.pipelines' has no attribute 'LinkedinScraperPipeline'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/twisted/internet/defer.py", line 2017, in _inlineCallbacks
    result = context.run(gen.send, result)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/crawler.py", line 152, in crawl
    self.engine = self._create_engine()
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/crawler.py", line 166, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 102, in __init__
    self.scraper: Scraper = Scraper(crawler)
                            ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/scraper.py", line 101, in __init__
    self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/middleware.py", line 77, in from_crawler
    return cls._from_settings(crawler.settings, crawler)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/middleware.py", line 86, in _from_settings
    mwcls = load_object(clspath)
            ^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/misc.py", line 76, in load_object
    raise NameError(f"Module '{module}' doesn't define any object named '{name}'")
NameError: Module 'linkedin_scraper.pipelines' doesn't define any object named 'LinkedinScraperPipeline'
2025-02-12 21:42:48 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 21:42:48 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 21:42:48 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 21:42:48 [scrapy.extensions.telnet] INFO: Telnet Password: 5bd6a3e1b644cd5f
2025-02-12 21:42:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 21:42:48 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 21:42:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 21:42:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 21:42:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-12 21:42:48 [scrapy.core.engine] INFO: Spider opened
2025-02-12 21:42:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 21:42:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 21:42:51 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:43:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-12 21:44:15 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-12 21:44:15 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 21:44:21 [linkedin_job] INFO: Extracting job listings from page 2
2025-02-12 21:45:17 [linkedin_job] INFO: Total unique job URLs collected so far: 50
2025-02-12 21:45:17 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 21:45:21 [linkedin_job] INFO: Extracting job listings from page 3
2025-02-12 21:46:20 [linkedin_job] INFO: Total unique job URLs collected so far: 75
2025-02-12 21:46:20 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 21:46:26 [linkedin_job] INFO: Extracting job listings from page 4
2025-02-12 21:47:24 [linkedin_job] INFO: Total unique job URLs collected so far: 100
2025-02-12 21:47:24 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 21:47:28 [linkedin_job] INFO: Extracting job listings from page 5
2025-02-12 21:48:30 [linkedin_job] INFO: Total unique job URLs collected so far: 125
2025-02-12 21:48:30 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 21:48:35 [linkedin_job] INFO: Extracting job listings from page 6
2025-02-12 21:48:45 [linkedin_job] INFO: Total unique job URLs collected so far: 129
2025-02-12 21:48:45 [linkedin_job] INFO: ✅ Final total job URLs: 129
2025-02-12 21:48:48 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:49:41 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 4 pages/min), scraped 1 items (at 1 items/min)
2025-02-12 21:49:47 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:50:42 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:51:35 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:52:20 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 0 pages/min), scraped 4 items (at 3 items/min)
2025-02-12 21:52:34 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:53:29 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 1 pages/min), scraped 5 items (at 1 items/min)
2025-02-12 21:53:34 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:54:33 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 2 pages/min), scraped 6 items (at 1 items/min)
2025-02-12 21:54:38 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 21:54:40 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-12 21:54:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4122797356/?eBP=CwEAAAGU-wr9S8NsuVZ9XT0UvUgCcAvdr7aJXM6c_j4dUajxg3hGeSSETHmKBgfztLIC3CUiqtcGVdsLD1DxBkmVycNlBPHXYBqrxYkaNQkYeiwgImXz8pjgQL_Om77BdJOWgT3BtLL9jNgzSwpeeTe4uJByIOco2qvVVg4IIIknbmS6oji6p7QNuTR1sA9asnbwcWw4Z9pCwoZikuohaMLdAurZI5IVdU9192PkeClHT8inE3l1ROp_12uipLouMZMqDTjOYQaNn7yX904HJZCOvsIFhkGA_J9_3LVipz9RnONGdflhXXDn1SZO1dbpeeyfFXTor7FIjUXcPsQLnnTiP3IgOp64TUrpnKCpsOq7wtrdkWfoRgpcKF6O4nU1xA7i69AvRqw6K1spOGp61TeuDB3eCPjAxMJnP1zQa8z5UwJbZ_tyA7TrXiLrAfCXs1JmPPx28HVPxoWZtWGJp9KWFAbJASFOjQCePqgbErUv9L-dQHbW5dMPIM5-Xm--fvI&refId=pWmhFBq2Kl0z4Jaj7xJnbw%3D%3D&trackingId=7B1ETHvHka%2FYcANvcQExXA%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linked_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 146, in parse_job_page
    driver.get("https://www.linkedin.com")
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 665, in get
    return super().get(url)
           ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 454, in get
    self.execute(Command.GET, {"url": url})
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/util/util.py", line 38, in reraise
    raise value.with_traceback(tb)
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linked_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2025-02-12 21:54:40 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-12 21:54:40 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-12 23:02:32 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 23:02:32 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 23:03:43 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 23:03:43 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 23:13:04 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 23:13:04 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 23:13:04 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 23:13:04 [scrapy.extensions.telnet] INFO: Telnet Password: e05036967178d49b
2025-02-12 23:13:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 23:13:04 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 23:13:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 23:13:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 23:13:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-12 23:13:04 [scrapy.core.engine] INFO: Spider opened
2025-02-12 23:13:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 23:13:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 23:13:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/jobs?q=software+developer&l=remote> (failed 6 times): 403 Forbidden
2025-02-12 23:13:21 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/jobs?q=software+developer&l=remote>: HTTP status code is not handled or not allowed
2025-02-12 23:13:21 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-12 23:13:21 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: linkedin_scraper/data/indeed_jobs_20250212_231303.json
2025-02-12 23:13:21 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/indeed_jobs_20250212_231303.csv
2025-02-12 23:13:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2035,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 14941,
 'downloader/response_count': 6,
 'downloader/response_status_count/403': 6,
 'elapsed_time_seconds': 17.530757,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 12, 18, 13, 21, 695007, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 32700,
 'httpcompression/response_count': 6,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 13,
 'memusage/max': 73547776,
 'memusage/startup': 73547776,
 'response_received_count': 1,
 'responses_per_minute': None,
 'retry/count': 5,
 'retry/max_reached': 1,
 'retry/reason_count/403 Forbidden': 5,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2025, 2, 12, 18, 13, 4, 164250, tzinfo=datetime.timezone.utc)}
2025-02-12 23:13:21 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-12 23:18:35 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 23:18:35 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 23:18:35 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 23:18:35 [scrapy.extensions.telnet] INFO: Telnet Password: 89882df526319dc4
2025-02-12 23:18:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 23:18:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 23:18:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 23:18:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 23:18:35 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-12 23:18:35 [scrapy.core.engine] INFO: Spider opened
2025-02-12 23:18:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 23:18:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 23:18:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/jobs?q=software+developer&l=remote> (failed 6 times): 403 Forbidden
2025-02-12 23:18:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/jobs?q=software+developer&l=remote>: HTTP status code is not handled or not allowed
2025-02-12 23:18:54 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-12 23:18:54 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: linkedin_scraper/data/indeed_jobs_20250212_231835.json
2025-02-12 23:18:54 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/indeed_jobs_20250212_231835.csv
2025-02-12 23:18:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2035,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 14948,
 'downloader/response_count': 6,
 'downloader/response_status_count/403': 6,
 'elapsed_time_seconds': 18.914651,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 12, 18, 18, 54, 817477, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 32700,
 'httpcompression/response_count': 6,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 13,
 'memusage/max': 73547776,
 'memusage/startup': 73547776,
 'response_received_count': 1,
 'responses_per_minute': None,
 'retry/count': 5,
 'retry/max_reached': 1,
 'retry/reason_count/403 Forbidden': 5,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2025, 2, 12, 18, 18, 35, 902826, tzinfo=datetime.timezone.utc)}
2025-02-12 23:18:54 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-12 23:20:01 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 23:20:01 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 23:20:01 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 23:20:01 [scrapy.extensions.telnet] INFO: Telnet Password: 4e4afefb2e57bd89
2025-02-12 23:20:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 23:20:01 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 23:20:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 23:20:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 23:20:01 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-12 23:20:01 [scrapy.core.engine] INFO: Spider opened
2025-02-12 23:20:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 23:20:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 23:20:05 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 23:20:13 [linkedin_job] INFO: Scraping job listings from page 1
2025-02-12 23:20:14 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-12 23:20:14 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-12 23:20:14 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79bb9c28ba70>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e2d41e71d1b722e3be167f758c3244/element
2025-02-12 23:20:14 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79bb9c28b800>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e2d41e71d1b722e3be167f758c3244/element
2025-02-12 23:20:14 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79bb9c28b200>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e2d41e71d1b722e3be167f758c3244/element
2025-02-12 23:20:14 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79bb9c28bf80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e2d41e71d1b722e3be167f758c3244/source
2025-02-12 23:20:14 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79bb9c2c01a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e2d41e71d1b722e3be167f758c3244/source
2025-02-12 23:20:14 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79bb9c2c02c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c4e2d41e71d1b722e3be167f758c3244/source
2025-02-12 23:20:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x79bb9c2c04d0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 153, in parse
    search_page_html = driver.page_source
                       ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=55789): Max retries exceeded with url: /session/c4e2d41e71d1b722e3be167f758c3244/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79bb9c2c04d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-12 23:20:14 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-12 23:23:11 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 23:23:11 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 23:23:11 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 23:23:11 [scrapy.extensions.telnet] INFO: Telnet Password: 57094fd2ef0ad5c1
2025-02-12 23:23:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 23:23:11 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 23:23:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 23:23:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 23:23:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-12 23:23:11 [scrapy.core.engine] INFO: Spider opened
2025-02-12 23:23:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 23:23:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 23:23:31 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/jobs?q=software+developer&l=remote> (failed 6 times): 403 Forbidden
2025-02-12 23:23:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/jobs?q=software+developer&l=remote>: HTTP status code is not handled or not allowed
2025-02-12 23:23:32 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-12 23:23:32 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: linkedin_scraper/data/indeed_jobs_20250212_232311.json
2025-02-12 23:23:32 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/indeed_jobs_20250212_232311.csv
2025-02-12 23:23:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2035,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 14942,
 'downloader/response_count': 6,
 'downloader/response_status_count/403': 6,
 'elapsed_time_seconds': 20.427621,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 12, 18, 23, 32, 43651, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 32700,
 'httpcompression/response_count': 6,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 13,
 'memusage/max': 73420800,
 'memusage/startup': 73420800,
 'response_received_count': 1,
 'responses_per_minute': None,
 'retry/count': 5,
 'retry/max_reached': 1,
 'retry/reason_count/403 Forbidden': 5,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2025, 2, 12, 18, 23, 11, 616030, tzinfo=datetime.timezone.utc)}
2025-02-12 23:23:32 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-12 23:26:54 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 23:26:54 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 23:26:54 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 23:26:54 [scrapy.extensions.telnet] INFO: Telnet Password: a530a23a0fce52c5
2025-02-12 23:26:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 23:26:54 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 23:26:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 23:26:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 23:26:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-12 23:26:54 [scrapy.core.engine] INFO: Spider opened
2025-02-12 23:26:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 23:26:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 23:26:55 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 23:27:03 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 23:27:16 [indeed_job] INFO: Scraping job listings from page 1
2025-02-12 23:27:49 [indeed_job] INFO: Extracted 15 job URLs from page 1
2025-02-12 23:27:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 23:28:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 23:29:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 23:30:13 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-12 23:30:13 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-12 23:30:22 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-12 23:31:15 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 23:31:15 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 23:31:15 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 23:31:15 [scrapy.extensions.telnet] INFO: Telnet Password: 1560bc6b05cb31b7
2025-02-12 23:31:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter']
2025-02-12 23:31:15 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOGSTATS_INTERVAL': 0,
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 23:31:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 23:31:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 23:31:15 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-12 23:31:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-12 23:42:46 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 23:42:46 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 23:42:46 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 23:42:46 [scrapy.extensions.telnet] INFO: Telnet Password: bb56fe8cd5dfd9f4
2025-02-12 23:42:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter']
2025-02-12 23:42:46 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOGSTATS_INTERVAL': 0,
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 23:42:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 23:42:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 23:42:46 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-12 23:42:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-12 23:43:47 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 23:43:47 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 23:43:47 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 23:43:47 [scrapy.extensions.telnet] INFO: Telnet Password: 58d2599f6ad5e626
2025-02-12 23:43:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter']
2025-02-12 23:43:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOGSTATS_INTERVAL': 0,
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 23:43:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 23:43:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 23:43:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-12 23:43:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2025-02-12 23:45:41 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 23:45:41 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 23:45:41 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 23:45:41 [scrapy.extensions.telnet] INFO: Telnet Password: 76d4a76421bc77b6
2025-02-12 23:45:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 23:45:41 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 23:45:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 23:45:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 23:45:41 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-12 23:45:41 [scrapy.core.engine] INFO: Spider opened
2025-02-12 23:45:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 23:45:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2025-02-12 23:45:42 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 23:45:49 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 23:46:02 [indeed_job] INFO: Scraping job listings from page 1
2025-02-12 23:46:31 [indeed_job] INFO: Extracted 15 job URLs from page 1
2025-02-12 23:46:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 23:47:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 23:47:44 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-12 23:47:44 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-12 23:47:45 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-12 23:48:22 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 23:48:22 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 23:48:22 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 23:48:22 [scrapy.extensions.telnet] INFO: Telnet Password: f79ab682a29fc61f
2025-02-12 23:48:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 23:48:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 23:48:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 23:48:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 23:48:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-12 23:48:22 [scrapy.core.engine] INFO: Spider opened
2025-02-12 23:48:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 23:48:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2025-02-12 23:48:24 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 23:48:34 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 23:48:40 [indeed_job] INFO: Scraping job listings from page 1
2025-02-12 23:48:43 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-12 23:48:43 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b0028f5fb60>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e34397de93c298945eb09b3a1f724745/element
2025-02-12 23:48:43 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b0028f5f7a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e34397de93c298945eb09b3a1f724745/element
2025-02-12 23:48:43 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b0028f5e7b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e34397de93c298945eb09b3a1f724745/element
2025-02-12 23:48:43 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b0028f5fda0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e34397de93c298945eb09b3a1f724745/element
2025-02-12 23:48:43 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b0028f5fb90>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e34397de93c298945eb09b3a1f724745/element
2025-02-12 23:48:43 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b0028f94260>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e34397de93c298945eb09b3a1f724745/element
2025-02-12 23:48:43 [indeed_job] INFO: No job links found. Exiting pagination.
2025-02-12 23:48:43 [indeed_job] INFO: ✅ Final total job URLs: 0
2025-02-12 23:48:43 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-12 23:48:43 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: linkedin_scraper/data/indeed_jobs_20250212_234822.json
2025-02-12 23:48:43 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/indeed_jobs_20250212_234822.csv
2025-02-12 23:48:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 20.654237,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 12, 18, 48, 43, 619175, tzinfo=datetime.timezone.utc),
 'items_per_minute': None,
 'log_count/INFO': 18,
 'log_count/WARNING': 6,
 'memusage/max': 73547776,
 'memusage/startup': 73547776,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 12, 18, 48, 22, 964938, tzinfo=datetime.timezone.utc)}
2025-02-12 23:48:43 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-12 23:49:47 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-12 23:49:47 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-12 23:49:47 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-12 23:49:47 [scrapy.extensions.telnet] INFO: Telnet Password: edf86c2af2e51f9b
2025-02-12 23:49:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-12 23:49:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-12 23:49:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-12 23:49:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-12 23:49:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-12 23:49:47 [scrapy.core.engine] INFO: Spider opened
2025-02-12 23:49:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-12 23:49:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2025-02-12 23:49:48 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 23:49:55 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-12 23:50:04 [indeed_job] INFO: Scraping job listings from page 1
2025-02-12 23:50:47 [indeed_job] INFO: No job links found. Exiting pagination.
2025-02-12 23:50:47 [indeed_job] INFO: ✅ Final total job URLs: 0
2025-02-12 23:50:47 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-12 23:50:47 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: linkedin_scraper/data/indeed_jobs_20250212_234947.json
2025-02-12 23:50:47 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/indeed_jobs_20250212_234947.csv
2025-02-12 23:50:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 59.661804,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 12, 18, 50, 47, 543644, tzinfo=datetime.timezone.utc),
 'items_per_minute': None,
 'log_count/INFO': 17,
 'memusage/max': 73416704,
 'memusage/startup': 73416704,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 12, 18, 49, 47, 881840, tzinfo=datetime.timezone.utc)}
2025-02-12 23:50:47 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 00:02:31 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 00:02:31 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-13 00:02:31 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 00:02:31 [scrapy.extensions.telnet] INFO: Telnet Password: 0599d5cb76a81acc
2025-02-13 00:02:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 00:02:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 00:02:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 00:02:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 00:02:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 00:02:31 [scrapy.core.engine] INFO: Spider opened
2025-02-13 00:02:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 00:02:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2025-02-13 00:02:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/jobs?q=software+developer&l=remote> (failed 6 times): 403 Forbidden
2025-02-13 00:02:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/jobs?q=software+developer&l=remote>: HTTP status code is not handled or not allowed
2025-02-13 00:02:50 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 00:02:50 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: linkedin_scraper/data/indeed_jobs_20250213_000231.json
2025-02-13 00:02:50 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/indeed_jobs_20250213_000231.csv
2025-02-13 00:02:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2035,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 14940,
 'downloader/response_count': 6,
 'downloader/response_status_count/403': 6,
 'elapsed_time_seconds': 18.62207,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 12, 19, 2, 50, 21080, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 32700,
 'httpcompression/response_count': 6,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 13,
 'memusage/max': 73408512,
 'memusage/startup': 73408512,
 'response_received_count': 1,
 'responses_per_minute': None,
 'retry/count': 5,
 'retry/max_reached': 1,
 'retry/reason_count/403 Forbidden': 5,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2025, 2, 12, 19, 2, 31, 399010, tzinfo=datetime.timezone.utc)}
2025-02-13 00:02:50 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 00:04:17 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 00:04:17 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-13 00:04:17 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 00:04:17 [scrapy.extensions.telnet] INFO: Telnet Password: a202b6c446a5a468
2025-02-13 00:04:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 00:04:17 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 00:04:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 00:04:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 00:04:17 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 00:04:17 [scrapy.core.engine] INFO: Spider opened
2025-02-13 00:04:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 00:04:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2025-02-13 00:04:18 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 00:04:26 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 00:04:36 [indeed_job] INFO: Scraping job listings from page 1
2025-02-13 00:04:41 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-13 00:04:41 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x778401aa7c20>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/7d450677819fe90736b67ead9f772468/element
2025-02-13 00:04:41 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x778401aa7f80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/7d450677819fe90736b67ead9f772468/element
2025-02-13 00:04:41 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x778401aa6780>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/7d450677819fe90736b67ead9f772468/element
2025-02-13 00:04:41 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x778401aa7f20>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/7d450677819fe90736b67ead9f772468/source
2025-02-13 00:04:41 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x778401adc140>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/7d450677819fe90736b67ead9f772468/source
2025-02-13 00:04:41 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x778401adc2f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/7d450677819fe90736b67ead9f772468/source
2025-02-13 00:04:41 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x778401adc4a0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/indeed_job.py", line 48, in start_requests
    yield from self.parse(response)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/indeed_job.py", line 74, in parse
    search_page_html = driver.page_source
                       ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=48647): Max retries exceeded with url: /session/7d450677819fe90736b67ead9f772468/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x778401adc4a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-13 00:04:41 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 00:04:41 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: linkedin_scraper/data/indeed_jobs_20250213_000417.json
2025-02-13 00:04:41 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/indeed_jobs_20250213_000417.csv
2025-02-13 00:04:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 23.923062,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 12, 19, 4, 41, 610436, tzinfo=datetime.timezone.utc),
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 16,
 'log_count/WARNING': 6,
 'memusage/max': 73416704,
 'memusage/startup': 73416704,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 12, 19, 4, 17, 687374, tzinfo=datetime.timezone.utc)}
2025-02-13 00:04:41 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 00:08:17 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 00:08:17 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-13 00:08:17 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 00:08:17 [scrapy.extensions.telnet] INFO: Telnet Password: 182bf460c316eef7
2025-02-13 00:08:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 00:08:17 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 00:08:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 00:08:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 00:08:17 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 00:08:17 [scrapy.core.engine] INFO: Spider opened
2025-02-13 00:08:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 00:08:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2025-02-13 00:08:18 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 00:08:26 [indeed_job] INFO: Scraping job listings from page 1
2025-02-13 00:09:00 [indeed_job] INFO: Extracted 15 job URLs from page 1
2025-02-13 00:09:00 [indeed_job] INFO: Total unique job URLs collected so far: 15
2025-02-13 00:09:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 00:10:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 00:11:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 00:12:02 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=adda9767e603e059&bb=uCYbSPZezJyBn_zPSQk2zrGdU1QnxPbYshjPxMX-hD9t6B9Lvf-YQau8f3or3oVSrPVdGrTc8GRr-8Hjt-wkstuUwyBrDy8fWzT3UfY0Uq4GieUQCrdpC4OH6aVieNy8&xkcb=SoBr67M3295w45QVrp0KbzkdCdPP&fccid=2d45ee15a8919257&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:12:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=adda9767e603e059&bb=uCYbSPZezJyBn_zPSQk2zrGdU1QnxPbYshjPxMX-hD9t6B9Lvf-YQau8f3or3oVSrPVdGrTc8GRr-8Hjt-wkstuUwyBrDy8fWzT3UfY0Uq4GieUQCrdpC4OH6aVieNy8&xkcb=SoBr67M3295w45QVrp0KbzkdCdPP&fccid=2d45ee15a8919257&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:12:07 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=54aa6147d6a945f8&bb=uCYbSPZezJyBn_zPSQk2ztys-wZazK2YyPWG5_OjkRRVRs3H4PsHszp6KTkWRqW5H6d_p1vhcMskuZx88vApyU0imeA58l3coiVU5A4dds3aH3bVIbK99gZnMVu7AoM5&xkcb=SoD267M3295w45QVrp0JbzkdCdPP&fccid=ad890d082b5315c9&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:12:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=54aa6147d6a945f8&bb=uCYbSPZezJyBn_zPSQk2ztys-wZazK2YyPWG5_OjkRRVRs3H4PsHszp6KTkWRqW5H6d_p1vhcMskuZx88vApyU0imeA58l3coiVU5A4dds3aH3bVIbK99gZnMVu7AoM5&xkcb=SoD267M3295w45QVrp0JbzkdCdPP&fccid=ad890d082b5315c9&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:12:10 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=a89335d20fb327ee&bb=uCYbSPZezJyBn_zPSQk2zrj0kopgS4EcZuvv-bMudKnH6-NTTZUa4fKIPLb2IxwArrEUublpLo-sGjkaglkAZM6I0waqNiDwkJR6R3YBsRFO7GTyWZ1jxAKlylA1R1pN&xkcb=SoBC67M3295w45QVrp0IbzkdCdPP&fccid=9eea8873d4a8d793&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:12:10 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=a89335d20fb327ee&bb=uCYbSPZezJyBn_zPSQk2zrj0kopgS4EcZuvv-bMudKnH6-NTTZUa4fKIPLb2IxwArrEUublpLo-sGjkaglkAZM6I0waqNiDwkJR6R3YBsRFO7GTyWZ1jxAKlylA1R1pN&xkcb=SoBC67M3295w45QVrp0IbzkdCdPP&fccid=9eea8873d4a8d793&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:12:13 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=f9f6fddbb284a4a6&bb=uCYbSPZezJyBn_zPSQk2zkZuml7r0-Lui91UoU7c_wgRlxlekcx7rSUpfQ3WP_mI3-PBCSrDJRCwFT7-A5-5rfTn6P2xVCovttCKMNggRWY30g14g3sD7cPDSwMTB8IB&xkcb=SoDM67M3295w45QVrp0PbzkdCdPP&fccid=4494b8cfea28ee1e&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:12:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=f9f6fddbb284a4a6&bb=uCYbSPZezJyBn_zPSQk2zkZuml7r0-Lui91UoU7c_wgRlxlekcx7rSUpfQ3WP_mI3-PBCSrDJRCwFT7-A5-5rfTn6P2xVCovttCKMNggRWY30g14g3sD7cPDSwMTB8IB&xkcb=SoDM67M3295w45QVrp0PbzkdCdPP&fccid=4494b8cfea28ee1e&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:12:17 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 00:12:17 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=d71edd82069d59bf&bb=uCYbSPZezJyBn_zPSQk2zqMD1ZqD5L0VKPBk57XgG3wc3moD9chJMRmlC_nfj73rcf5l1McV_enG_JN54Scpv3GPupdtwdsTEtkz-OcdrnOtegIhHFIgmUK9GXRaIWCn&xkcb=SoB467M3295w45QVrp0ObzkdCdPP&fccid=c594c442f5397e7b&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:12:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=d71edd82069d59bf&bb=uCYbSPZezJyBn_zPSQk2zqMD1ZqD5L0VKPBk57XgG3wc3moD9chJMRmlC_nfj73rcf5l1McV_enG_JN54Scpv3GPupdtwdsTEtkz-OcdrnOtegIhHFIgmUK9GXRaIWCn&xkcb=SoB467M3295w45QVrp0ObzkdCdPP&fccid=c594c442f5397e7b&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:12:21 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=e586fdc113ec366a&bb=uCYbSPZezJyBn_zPSQk2zq5f6bCHiAtvHi-L_l1EWlGl0-doKTmB7oEsuC4flmtVpH6AA16MzX_qbEZnOYGTkvpdd43bV0cL8i4i37Sk3UcvhSUdlnyxPc_RkC-_eMUN&xkcb=SoDl67M3295w45QVrp0NbzkdCdPP&fccid=43014b1412e0a7b6&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:12:31 [indeed_job] INFO: Scraping job listings from page 2
2025-02-13 00:13:08 [indeed_job] INFO: Extracted 15 job URLs from page 2
2025-02-13 00:13:08 [indeed_job] INFO: Total unique job URLs collected so far: 30
2025-02-13 00:13:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=e586fdc113ec366a&bb=uCYbSPZezJyBn_zPSQk2zq5f6bCHiAtvHi-L_l1EWlGl0-doKTmB7oEsuC4flmtVpH6AA16MzX_qbEZnOYGTkvpdd43bV0cL8i4i37Sk3UcvhSUdlnyxPc_RkC-_eMUN&xkcb=SoDl67M3295w45QVrp0NbzkdCdPP&fccid=43014b1412e0a7b6&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:13:08 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=debb794e317f6bc9&bb=uCYbSPZezJyBn_zPSQk2zme4oC4Emb1iyzY0bZJwmYrO1vrgpOBXyFheUEjpXSXxpNDRLz_v6UwxUQH_i4aX0sGtK2wHuLL8bnzsw4cn3bKhDd3-A_cO1KShksd-PSZj&xkcb=SoBR67M3295w45QVrp0MbzkdCdPP&fccid=5abbd6e84bf247cc&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:13:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=debb794e317f6bc9&bb=uCYbSPZezJyBn_zPSQk2zme4oC4Emb1iyzY0bZJwmYrO1vrgpOBXyFheUEjpXSXxpNDRLz_v6UwxUQH_i4aX0sGtK2wHuLL8bnzsw4cn3bKhDd3-A_cO1KShksd-PSZj&xkcb=SoBR67M3295w45QVrp0MbzkdCdPP&fccid=5abbd6e84bf247cc&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:13:12 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=7fc63896e6bf877b&bb=uCYbSPZezJyBn_zPSQk2zlR79glClXfHRdBC41wvWo4OIbfxqGtFViK2ogmUwrqvUYCBO-0ejS-cevbKgVvicAiN0k2oP94gditDKg0uIe6mcy_xb7ajIeLsGo_5DObu&xkcb=SoC467M3295w45QVrp0DbzkdCdPP&fccid=dd616958bd9ddc12&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:13:12 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=7fc63896e6bf877b&bb=uCYbSPZezJyBn_zPSQk2zlR79glClXfHRdBC41wvWo4OIbfxqGtFViK2ogmUwrqvUYCBO-0ejS-cevbKgVvicAiN0k2oP94gditDKg0uIe6mcy_xb7ajIeLsGo_5DObu&xkcb=SoC467M3295w45QVrp0DbzkdCdPP&fccid=dd616958bd9ddc12&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:13:15 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=5146d61fb228626c&bb=uCYbSPZezJyBn_zPSQk2znmlSK_FuW_f1KkL7B99XBqVixSFdUiHsQKsdhCEF-l0_cQXOyuPRpDhHY3WystNXh1b2n5mqMUIAccYOaor14TJScMSD9IU2zkRDncUM2TX&xkcb=SoAM67M3295w45QVrp0CbzkdCdPP&fccid=55ba08eea2f38362&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:13:15 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=5146d61fb228626c&bb=uCYbSPZezJyBn_zPSQk2znmlSK_FuW_f1KkL7B99XBqVixSFdUiHsQKsdhCEF-l0_cQXOyuPRpDhHY3WystNXh1b2n5mqMUIAccYOaor14TJScMSD9IU2zkRDncUM2TX&xkcb=SoAM67M3295w45QVrp0CbzkdCdPP&fccid=55ba08eea2f38362&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:13:17 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 00:13:19 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=9c13da9047590666&bb=uCYbSPZezJyBn_zPSQk2zme4oC4Emb1idqFpsVQgWq6KyMXn6pC9VGyz0_N2kKUXkpcLUJ6XJ6X8mTNUG1zQA8ms7yq3JT9UnVm0N8agrsbBE-J80kFwEA%3D%3D&xkcb=SoCR67M3295w45QVrp0BbzkdCdPP&fccid=2533ecec90dc66a3&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:13:19 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=9c13da9047590666&bb=uCYbSPZezJyBn_zPSQk2zme4oC4Emb1idqFpsVQgWq6KyMXn6pC9VGyz0_N2kKUXkpcLUJ6XJ6X8mTNUG1zQA8ms7yq3JT9UnVm0N8agrsbBE-J80kFwEA%3D%3D&xkcb=SoCR67M3295w45QVrp0BbzkdCdPP&fccid=2533ecec90dc66a3&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:14:17 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 00:15:17 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 00:16:17 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 00:16:20 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=8cbec710dbeceb54&bb=uCYbSPZezJyBn_zPSQk2zgWsgxcyTBugICoLQaoH5WaiVZxrB6xyUPWJzBx0sthJ9RGjeRCcU2VAaL--C3VVLYKunPTYTl1qxBY1Kcb1JekWDnvxvdysK9MNqGWm9KeG&xkcb=SoAl67M3295w45QVrp0AbzkdCdPP&fccid=9a6ebcf8d90d7793&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:16:20 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=8cbec710dbeceb54&bb=uCYbSPZezJyBn_zPSQk2zgWsgxcyTBugICoLQaoH5WaiVZxrB6xyUPWJzBx0sthJ9RGjeRCcU2VAaL--C3VVLYKunPTYTl1qxBY1Kcb1JekWDnvxvdysK9MNqGWm9KeG&xkcb=SoAl67M3295w45QVrp0AbzkdCdPP&fccid=9a6ebcf8d90d7793&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:16:25 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=c7d15890ba19f3a8&bb=uCYbSPZezJyBn_zPSQk2znqLnZCLqcDhEPUFz2hjJlny8TcUNADXyUcmu9CKmQwwcehstiNIqFQtw8jAinlwghSHOC917l59rgvHZkaV6qx4Fuv2_3CI2w%3D%3D&xkcb=SoCr67M3295w45QVrp0HbzkdCdPP&fccid=2d3238ace27344f8&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:16:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=c7d15890ba19f3a8&bb=uCYbSPZezJyBn_zPSQk2znqLnZCLqcDhEPUFz2hjJlny8TcUNADXyUcmu9CKmQwwcehstiNIqFQtw8jAinlwghSHOC917l59rgvHZkaV6qx4Fuv2_3CI2w%3D%3D&xkcb=SoCr67M3295w45QVrp0HbzkdCdPP&fccid=2d3238ace27344f8&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:16:27 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=b013b2c65dff5b3c&bb=uCYbSPZezJyBn_zPSQk2zl0OsIyy014hceIP_L0vN-f8-YEJSC-vdeU5rKXexicsQXCF4y7eBJ16C8Jth70-CJM4_MDQJUPdM2tFNDDHu6ZEqqkoyGFpmVMqx4ENamOB&xkcb=SoAf67M3295w45QVrp0GbzkdCdPP&fccid=353eb997fc901045&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:16:27 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=b013b2c65dff5b3c&bb=uCYbSPZezJyBn_zPSQk2zl0OsIyy014hceIP_L0vN-f8-YEJSC-vdeU5rKXexicsQXCF4y7eBJ16C8Jth70-CJM4_MDQJUPdM2tFNDDHu6ZEqqkoyGFpmVMqx4ENamOB&xkcb=SoAf67M3295w45QVrp0GbzkdCdPP&fccid=353eb997fc901045&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:16:30 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=ca168595bf9f61b5&bb=uCYbSPZezJyBn_zPSQk2zqMD1ZqD5L0VmlTyt8qRRIxNjfwZwsPIGaLfspbKYxglvGcwjZqctPsQ7HiAsBKj2NftActBW8WWJM_vyacSKpNHUi7auR5oJZMfcXS8L6-s&xkcb=SoCC67M3295w45QVrp0FbzkdCdPP&fccid=74e36d81c894b4b7&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:16:30 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=ca168595bf9f61b5&bb=uCYbSPZezJyBn_zPSQk2zqMD1ZqD5L0VmlTyt8qRRIxNjfwZwsPIGaLfspbKYxglvGcwjZqctPsQ7HiAsBKj2NftActBW8WWJM_vyacSKpNHUi7auR5oJZMfcXS8L6-s&xkcb=SoCC67M3295w45QVrp0FbzkdCdPP&fccid=74e36d81c894b4b7&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:16:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=14b92dfe52197c18&bb=uCYbSPZezJyBn_zPSQk2zrgkqSGEGoDTT1B7IHgG6UI3tPce0PI4xjUKn03j0bm8dm_ZoLq8GP-i0mojVwcQklIZlUrQBaElyYjquSN8UUpuZoYRqKLp3Q%3D%3D&xkcb=SoA267M3295w45QVrp0EbzkdCdPP&fccid=9abc205b1f92e947&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:16:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=14b92dfe52197c18&bb=uCYbSPZezJyBn_zPSQk2zrgkqSGEGoDTT1B7IHgG6UI3tPce0PI4xjUKn03j0bm8dm_ZoLq8GP-i0mojVwcQklIZlUrQBaElyYjquSN8UUpuZoYRqKLp3Q%3D%3D&xkcb=SoA267M3295w45QVrp0EbzkdCdPP&fccid=9abc205b1f92e947&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:16:37 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=ae2abac0eba2f200&bb=RKXcNQnOTf_yfJodutR4NAziRduStDtJgivIsaHBSarb27IDJQBDvA79Y2eIaL6csXkHNvk0swXDkjz1KT0TDYHjWuDrB31aj6dKwnDcGzhpFYv55U0QIm_vGw8q_sqC&xkcb=SoAX67M3296TFUQVrp0LbzkdCdPP&fccid=b27f874d9071f3c4&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:16:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=ae2abac0eba2f200&bb=RKXcNQnOTf_yfJodutR4NAziRduStDtJgivIsaHBSarb27IDJQBDvA79Y2eIaL6csXkHNvk0swXDkjz1KT0TDYHjWuDrB31aj6dKwnDcGzhpFYv55U0QIm_vGw8q_sqC&xkcb=SoAX67M3296TFUQVrp0LbzkdCdPP&fccid=b27f874d9071f3c4&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:16:42 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=51e93d432af482f8&bb=RKXcNQnOTf_yfJodutR4NMkIiua4hhMWgASRUv3jDRrXNe1UYAkUR0mzgS8R0uncBQ1qMdoFJG1JZfOmqLeBuN6fTH6x9VN7lxsaegitiZplbxqiMeG8M5DwVAEicv0e&xkcb=SoCj67M3296TFUQVrp0KbzkdCdPP&fccid=4494b8cfea28ee1e&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:16:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=51e93d432af482f8&bb=RKXcNQnOTf_yfJodutR4NMkIiua4hhMWgASRUv3jDRrXNe1UYAkUR0mzgS8R0uncBQ1qMdoFJG1JZfOmqLeBuN6fTH6x9VN7lxsaegitiZplbxqiMeG8M5DwVAEicv0e&xkcb=SoCj67M3296TFUQVrp0KbzkdCdPP&fccid=4494b8cfea28ee1e&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:16:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=06ca0cd709786552&bb=RKXcNQnOTf_yfJodutR4NGnWoGm8-epCMWG606iNB0oLUCMId5aK4fqTBGBLAEq19-28uM3cI13i552H-p3eXBDeKyIXrqUCiGzfvntidOctwNnjWJXvf-rA-4S3pqms&xkcb=SoA-67M3296TFUQVrp0JbzkdCdPP&fccid=d55ba0bc1050147f&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:16:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=06ca0cd709786552&bb=RKXcNQnOTf_yfJodutR4NGnWoGm8-epCMWG606iNB0oLUCMId5aK4fqTBGBLAEq19-28uM3cI13i552H-p3eXBDeKyIXrqUCiGzfvntidOctwNnjWJXvf-rA-4S3pqms&xkcb=SoA-67M3296TFUQVrp0JbzkdCdPP&fccid=d55ba0bc1050147f&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:16:49 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=1787c7491b6d0408&bb=RKXcNQnOTf_yfJodutR4NC1ajKjCoezGi2LEdx0iOXmVHO5sN24WiF1uf4VR-97jB4noKfT7-oNN7hf5NpCPUnL1Q25-Mqy4XjQ_1IAHT9NU2UQtb-jVgBZNKej8JME5&xkcb=SoCK67M3296TFUQVrp0IbzkdCdPP&fccid=c594c442f5397e7b&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:16:49 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=1787c7491b6d0408&bb=RKXcNQnOTf_yfJodutR4NC1ajKjCoezGi2LEdx0iOXmVHO5sN24WiF1uf4VR-97jB4noKfT7-oNN7hf5NpCPUnL1Q25-Mqy4XjQ_1IAHT9NU2UQtb-jVgBZNKej8JME5&xkcb=SoCK67M3296TFUQVrp0IbzkdCdPP&fccid=c594c442f5397e7b&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:16:54 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=d1a7c7752d6a00b4&bb=RKXcNQnOTf_yfJodutR4NOh8V26Ne8o2uo2CwFoq1Pq2aIh6Xu6qDxm9__YNGk_BrinOxdYXLRDdZGKJPDLuXWY9M6bz4n97tQcwna60c5s9LnRxo4M9mWGSjTaRMou5&xkcb=SoAE67M3296TFUQVrp0PbzkdCdPP&fccid=181ae1a6390e21b5&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:16:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=d1a7c7752d6a00b4&bb=RKXcNQnOTf_yfJodutR4NOh8V26Ne8o2uo2CwFoq1Pq2aIh6Xu6qDxm9__YNGk_BrinOxdYXLRDdZGKJPDLuXWY9M6bz4n97tQcwna60c5s9LnRxo4M9mWGSjTaRMou5&xkcb=SoAE67M3296TFUQVrp0PbzkdCdPP&fccid=181ae1a6390e21b5&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:17:17 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 00:18:17 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 00:19:17 [scrapy.extensions.logstats] INFO: Crawled 20 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 00:19:55 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.indeed.com/rc/clk?jk=9733b68c80a789f6&bb=RKXcNQnOTf_yfJodutR4NJ9qmGPKmCvyIW8Rfxd9MBNrjF1TGIfnxaBN9XUAWR0_vDbwJQmjvMPfawUW9HD2WziOHHHvVYTyJhT-2vH4SX1SijQ8BODoJw%3D%3D&xkcb=SoCw67M3296TFUQVrp0ObzkdCdPP&fccid=3a71a4d2f7990a25&vjs=3> (failed 6 times): 403 Forbidden
2025-02-13 00:20:05 [indeed_job] INFO: Scraping job listings from page 3
2025-02-13 00:20:23 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-13 00:20:23 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-13 00:20:24 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x784749c696a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/314f20597387f8d274dd7ff6028587cf/element
2025-02-13 00:20:24 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x784749662d80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/314f20597387f8d274dd7ff6028587cf/element
2025-02-13 00:20:24 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x784749663110>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/314f20597387f8d274dd7ff6028587cf/element
2025-02-13 00:20:24 [indeed_job] ERROR: Error scrolling job list: HTTPConnectionPool(host='localhost', port=46675): Max retries exceeded with url: /session/314f20597387f8d274dd7ff6028587cf/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x784749663320>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-13 00:20:24 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x784748b0d400>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/314f20597387f8d274dd7ff6028587cf/source
2025-02-13 00:20:24 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x784748b0c680>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/314f20597387f8d274dd7ff6028587cf/source
2025-02-13 00:20:24 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x784748b0c050>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/314f20597387f8d274dd7ff6028587cf/source
2025-02-13 00:20:24 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x784748738b30>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/indeed_job.py", line 46, in start_requests
    yield from self.parse(response)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/indeed_job.py", line 58, in parse
    search_page_html = self.driver.page_source
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=46675): Max retries exceeded with url: /session/314f20597387f8d274dd7ff6028587cf/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x784748738b30>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-13 00:20:24 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://www.indeed.com/rc/clk?jk=9733b68c80a789f6&bb=RKXcNQnOTf_yfJodutR4NJ9qmGPKmCvyIW8Rfxd9MBNrjF1TGIfnxaBN9XUAWR0_vDbwJQmjvMPfawUW9HD2WziOHHHvVYTyJhT-2vH4SX1SijQ8BODoJw%3D%3D&xkcb=SoCw67M3296TFUQVrp0ObzkdCdPP&fccid=3a71a4d2f7990a25&vjs=3>: HTTP status code is not handled or not allowed
2025-02-13 00:20:24 [scrapy.extensions.logstats] INFO: Crawled 21 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 00:20:24 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-13 00:56:16 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 00:56:16 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-13 00:57:04 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 00:57:04 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-13 00:59:15 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 00:59:15 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-13 00:59:15 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 00:59:15 [scrapy.extensions.telnet] INFO: Telnet Password: 4ebe75d32960a691
2025-02-13 00:59:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 00:59:15 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 00:59:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 00:59:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 00:59:15 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 00:59:15 [scrapy.core.engine] INFO: Spider opened
2025-02-13 00:59:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 00:59:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2025-02-13 00:59:17 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 00:59:24 [linkedin_job] INFO: Scraping job listings from page 1
2025-02-13 00:59:28 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-13 00:59:28 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x71d75e7e0bc0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/96e58ffce8d155a3ff87b1d4f6b6f95e/element
2025-02-13 00:59:28 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x71d75ab6f0e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/96e58ffce8d155a3ff87b1d4f6b6f95e/element
2025-02-13 00:59:28 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x71d75ab6f920>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/96e58ffce8d155a3ff87b1d4f6b6f95e/element
2025-02-13 00:59:28 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x71d75ab88470>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/96e58ffce8d155a3ff87b1d4f6b6f95e/source
2025-02-13 00:59:28 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x71d75ab886b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/96e58ffce8d155a3ff87b1d4f6b6f95e/source
2025-02-13 00:59:28 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x71d75ab88860>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/96e58ffce8d155a3ff87b1d4f6b6f95e/source
2025-02-13 00:59:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x71d75ab88a10>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 153, in parse
    search_page_html = driver.page_source
                       ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=52865): Max retries exceeded with url: /session/96e58ffce8d155a3ff87b1d4f6b6f95e/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x71d75ab88a10>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-13 00:59:28 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-13 00:59:28 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: linkedin_scraper/data/indeed_jobs_20250213_005915.json
2025-02-13 00:59:28 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/indeed_jobs_20250213_005915.csv
2025-02-13 00:59:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 450,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 149803,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 12.826765,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2025, 2, 12, 19, 59, 28, 150125, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1297639,
 'httpcompression/response_count': 1,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 15,
 'log_count/WARNING': 6,
 'memusage/max': 73547776,
 'memusage/startup': 73547776,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/MaxRetryError': 1,
 'start_time': datetime.datetime(2025, 2, 12, 19, 59, 15, 323360, tzinfo=datetime.timezone.utc)}
2025-02-13 00:59:28 [scrapy.core.engine] INFO: Spider closed (shutdown)
2025-02-13 01:01:08 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 01:01:08 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-13 01:01:08 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 01:01:08 [scrapy.extensions.telnet] INFO: Telnet Password: f39cf8ed1528a786
2025-02-13 01:01:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 01:01:08 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 01:01:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 01:01:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 01:01:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 01:01:08 [scrapy.core.engine] INFO: Spider opened
2025-02-13 01:01:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 01:01:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2025-02-13 01:01:11 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 01:01:28 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-13 01:01:29 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-13 01:01:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-13 01:01:29 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76b5f6cb3bf0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/846837791a4809c0381c859cd3e48a0c/element
2025-02-13 01:01:29 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76b5f6cb3590>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/846837791a4809c0381c859cd3e48a0c/element
2025-02-13 01:01:29 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76b5f6cb3770>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/846837791a4809c0381c859cd3e48a0c/element
2025-02-13 01:01:29 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76b5f6cd0470>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/846837791a4809c0381c859cd3e48a0c/element
2025-02-13 01:01:29 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76b5f6cd0650>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/846837791a4809c0381c859cd3e48a0c/element
2025-02-13 01:01:29 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76b5f6cd08c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/846837791a4809c0381c859cd3e48a0c/element
2025-02-13 01:01:29 [linkedin_job] INFO: Timeout: No job links found within 15 seconds.
2025-02-13 01:01:29 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76b5f6cb3740>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/846837791a4809c0381c859cd3e48a0c/source
2025-02-13 01:01:29 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76b5f6cb3560>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/846837791a4809c0381c859cd3e48a0c/source
2025-02-13 01:01:29 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76b5f6cb33b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/846837791a4809c0381c859cd3e48a0c/source
2025-02-13 01:01:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x76b5f6cd01a0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 75, in parse
    search_page_html = driver.page_source
                       ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=43769): Max retries exceeded with url: /session/846837791a4809c0381c859cd3e48a0c/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76b5f6cd01a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-13 01:01:29 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-13 01:25:39 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 01:25:39 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-13 01:25:39 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 01:25:39 [scrapy.extensions.telnet] INFO: Telnet Password: 65a12c271fd563bc
2025-02-13 01:25:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 01:25:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 01:25:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 01:25:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 01:25:39 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 01:25:39 [scrapy.core.engine] INFO: Spider opened
2025-02-13 01:25:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 01:25:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2025-02-13 01:25:43 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 01:26:08 [linkedin_post] INFO: Extracting posts from page 1
2025-02-13 01:26:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/search/results/all/?keywords=%22software%20engineer%22%2C%22hiring%22&origin=GLOBAL_SEARCH_HEADER&sid=1qU> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_post.py", line 81, in parse
    name = post.xpath('//span[contains(@class, "update-components-actor__single-line-truncate")]//span[@aria-hidden="true"]/text()').get().strip()
           ^^^^^^^^^^
AttributeError: 'str' object has no attribute 'xpath'
2025-02-13 01:26:39 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 01:26:39 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: linkedin_scraper/data/linkedin_posts_20250213_012539.json
2025-02-13 01:26:39 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/linkedin_posts_20250213_012539.csv
2025-02-13 01:26:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 441,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 174706,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 59.773624,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 12, 20, 26, 39, 284469, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1672491,
 'httpcompression/response_count': 1,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 14,
 'memusage/max': 73551872,
 'memusage/startup': 73551872,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2025, 2, 12, 20, 25, 39, 510845, tzinfo=datetime.timezone.utc)}
2025-02-13 01:26:39 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 01:27:14 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 01:27:14 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-13 01:27:14 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 01:27:14 [scrapy.extensions.telnet] INFO: Telnet Password: 5e20680338bb32e5
2025-02-13 01:27:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 01:27:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 01:27:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 01:27:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 01:27:14 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 01:27:14 [scrapy.core.engine] INFO: Spider opened
2025-02-13 01:27:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 01:27:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2025-02-13 01:27:18 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 01:27:40 [linkedin_post] INFO: Extracting posts from page 1
2025-02-13 01:28:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/search/results/all/?keywords=%22software%20engineer%22%2C%22hiring%22&origin=GLOBAL_SEARCH_HEADER&sid=1qU> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_post.py", line 84, in parse
    name = post.xpath('//span[contains(@class, "update-components-actor__single-line-truncate")]//span[@aria-hidden="true"]/text()').get().strip()
           ^^^^^^^^^^
AttributeError: 'str' object has no attribute 'xpath'
2025-02-13 01:28:15 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 01:28:15 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 01:28:15 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: linkedin_scraper/data/linkedin_posts_20250213_012714.json
2025-02-13 01:28:15 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/linkedin_posts_20250213_012714.csv
2025-02-13 01:28:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 441,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 174824,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 60.239355,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 12, 20, 28, 15, 146563, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1672597,
 'httpcompression/response_count': 1,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 15,
 'memusage/max': 124321792,
 'memusage/startup': 73547776,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2025, 2, 12, 20, 27, 14, 907208, tzinfo=datetime.timezone.utc)}
2025-02-13 01:28:15 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 01:31:22 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 01:31:22 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-13 01:31:22 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 01:31:22 [scrapy.extensions.telnet] INFO: Telnet Password: b8213727dbbdfaac
2025-02-13 01:31:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter']
2025-02-13 01:31:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOGSTATS_INTERVAL': 0,
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 01:31:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 01:31:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 01:31:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 01:31:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025
2025-02-13 01:52:13 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 01:52:13 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-13 01:52:13 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 01:52:13 [scrapy.extensions.telnet] INFO: Telnet Password: 1aac21bfadb16986
2025-02-13 01:52:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 01:52:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 01:52:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 01:52:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 01:52:13 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 01:52:13 [scrapy.core.engine] INFO: Spider opened
2025-02-13 01:52:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 01:52:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2025-02-13 01:52:17 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 01:52:23 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-13 01:52:23 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-13 01:52:25 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7878379c7a40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/53c91ee1fa46b21226e1a8fb05366070/cookie
2025-02-13 01:52:25 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7878379ec440>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/53c91ee1fa46b21226e1a8fb05366070/cookie
2025-02-13 01:52:25 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7878379ec200>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/53c91ee1fa46b21226e1a8fb05366070/cookie
2025-02-13 01:52:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/search/results/all/?keywords=%22software%20engineer%22%2C%22hiring%22&origin=GLOBAL_SEARCH_HEADER&sid=1qU> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7878379ec0e0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_post.py", line 53, in parse
    driver.add_cookie({"name": name, "value": value})
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 767, in add_cookie
    self.execute(Command.ADD_COOKIE, {"cookie": cookie_dict})
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=33209): Max retries exceeded with url: /session/53c91ee1fa46b21226e1a8fb05366070/cookie (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7878379ec0e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-13 01:52:25 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-13 01:53:21 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 01:53:21 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-13 01:53:21 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 01:53:21 [scrapy.extensions.telnet] INFO: Telnet Password: ccc1f4054de10755
2025-02-13 01:53:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 01:53:21 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 01:53:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 01:53:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 01:53:21 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 01:53:21 [scrapy.core.engine] INFO: Spider opened
2025-02-13 01:53:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 01:53:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2025-02-13 01:53:24 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 01:54:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/search/results/content/?keywords=%22software%20engineer%22%2C%22hiring%22&origin=SWITCH_SEARCH_VERTICAL&sid=1qU> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_post.py", line 78, in parse
    name = post.xpath('//span[contains(@class, "update-components-actor__single-line-truncate")]//span[@aria-hidden="true"]/text()').get().strip()
           ^^^^^^^^^^
AttributeError: 'str' object has no attribute 'xpath'
2025-02-13 01:54:15 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 01:54:15 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: linkedin_scraper/data/linkedin_posts_20250213_015321.json
2025-02-13 01:54:15 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: linkedin_scraper/data/linkedin_posts_20250213_015321.csv
2025-02-13 01:54:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 447,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 157929,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 53.759161,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 12, 20, 54, 15, 643604, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1398275,
 'httpcompression/response_count': 1,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 13,
 'memusage/max': 73359360,
 'memusage/startup': 73359360,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2025, 2, 12, 20, 53, 21, 884443, tzinfo=datetime.timezone.utc)}
2025-02-13 01:54:15 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 01:57:18 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 01:57:18 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.8.0-52-generic-x86_64-with-glibc2.39
2025-02-13 01:57:18 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 01:57:18 [scrapy.extensions.telnet] INFO: Telnet Password: 4d773ac0bac261b2
2025-02-13 01:57:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 01:57:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 01:57:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 01:57:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 01:57:18 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 01:57:18 [scrapy.core.engine] INFO: Spider opened
2025-02-13 01:57:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 01:57:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6026
2025-02-13 01:57:24 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 01:58:16 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 01:58:16 [scrapy.extensions.feedexport] INFO: Stored json feed (24 items) in: linkedin_scraper/data/linkedin_posts_20250213_015718.json
2025-02-13 01:58:16 [scrapy.extensions.feedexport] INFO: Stored csv feed (24 items) in: linkedin_scraper/data/linkedin_posts_20250213_015718.csv
2025-02-13 01:58:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 447,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 157869,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 57.141128,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 12, 20, 58, 16, 77735, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1398067,
 'httpcompression/response_count': 1,
 'item_scraped_count': 24,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73416704,
 'memusage/startup': 73416704,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 12, 20, 57, 18, 936607, tzinfo=datetime.timezone.utc)}
2025-02-13 01:58:16 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 18:35:00 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 18:35:00 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 18:35:00 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 18:35:00 [scrapy.extensions.telnet] INFO: Telnet Password: b884c8759c4b1111
2025-02-13 18:35:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 18:35:00 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 18:35:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 18:35:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 18:35:00 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 18:35:00 [scrapy.core.engine] INFO: Spider opened
2025-02-13 18:35:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 18:35:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-13 18:35:03 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 18:36:11 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2025-02-13 18:36:11 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 18:36:11 [scrapy.extensions.feedexport] INFO: Stored json feed (11 items) in: data/results_linkedin_post_2025-02-13T13-35-00+00-00.json
2025-02-13 18:36:11 [scrapy.extensions.feedexport] INFO: Stored csv feed (11 items) in: data/results_linkedin_post_2025-02-13T13-35-00+00-00.csv
2025-02-13 18:36:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 464,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 161794,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 71.388142,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 13, 36, 11, 712464, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1424321,
 'httpcompression/response_count': 1,
 'item_scraped_count': 11,
 'items_per_minute': None,
 'log_count/INFO': 14,
 'memusage/max': 122847232,
 'memusage/startup': 72912896,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 13, 13, 35, 0, 324322, tzinfo=datetime.timezone.utc)}
2025-02-13 18:36:11 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 18:40:57 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 18:40:57 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 18:40:57 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 18:40:57 [scrapy.extensions.telnet] INFO: Telnet Password: 6d5a0aaf69d0c44f
2025-02-13 18:40:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 18:40:57 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 18:40:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 18:40:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 18:40:57 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 18:40:57 [scrapy.core.engine] INFO: Spider opened
2025-02-13 18:40:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 18:40:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-13 18:41:00 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 18:42:02 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2025-02-13 18:42:02 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 18:42:02 [scrapy.extensions.feedexport] INFO: Stored json feed (55 items) in: data/results_linkedin_post_2025-02-13T13-40-57+00-00.json
2025-02-13 18:42:02 [scrapy.extensions.feedexport] INFO: Stored csv feed (55 items) in: data/results_linkedin_post_2025-02-13T13-40-57+00-00.csv
2025-02-13 18:42:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 464,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 161724,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 65.326978,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 13, 42, 2, 669754, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1424323,
 'httpcompression/response_count': 1,
 'item_scraped_count': 55,
 'items_per_minute': None,
 'log_count/INFO': 14,
 'memusage/max': 136302592,
 'memusage/startup': 73109504,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 13, 13, 40, 57, 342776, tzinfo=datetime.timezone.utc)}
2025-02-13 18:42:02 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 19:04:21 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 19:04:21 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 19:26:30 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 19:26:30 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 19:26:30 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 19:26:30 [scrapy.extensions.telnet] INFO: Telnet Password: 3f710242cf1d7618
2025-02-13 19:26:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 19:26:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 19:26:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 19:26:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 19:26:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 19:26:30 [scrapy.core.engine] INFO: Spider opened
2025-02-13 19:26:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 19:26:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-13 19:26:35 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 19:26:45 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-13 19:26:46 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-13 19:26:47 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e5e79abf9b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5cd016f8946649df099b5fc58fad758d/element
2025-02-13 19:26:47 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e5e79abf5c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5cd016f8946649df099b5fc58fad758d/element
2025-02-13 19:26:47 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e5e79abf980>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5cd016f8946649df099b5fc58fad758d/element
2025-02-13 19:26:47 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e5e79adc500>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5cd016f8946649df099b5fc58fad758d/element
2025-02-13 19:26:47 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e5e79adc710>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5cd016f8946649df099b5fc58fad758d/element
2025-02-13 19:26:47 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e5e79adc950>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5cd016f8946649df099b5fc58fad758d/element
2025-02-13 19:26:47 [google_jobs] INFO: Timeout: No jobs found within 15 seconds.
2025-02-13 19:26:47 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e5e79abf950>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5cd016f8946649df099b5fc58fad758d/elements
2025-02-13 19:26:47 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e5e79abf680>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5cd016f8946649df099b5fc58fad758d/elements
2025-02-13 19:26:47 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e5e79adc1a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5cd016f8946649df099b5fc58fad758d/elements
2025-02-13 19:26:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com/search?q=%22software+engineer%22,+%22remote%22&sca_esv=15a52cd0682ab787&ei=vPKtZ8-tCerY7M8PucuH6Ao&ved=2ahUKEwiuoZv838CLAxW5RaQEHXvjNrUQ3L8LegQIIBAN&uact=5&oq=%22software+engineer%22,+%22remote%22&gs_lp=Egxnd3Mtd2l6LXNlcnAiHSJzb2Z0d2FyZSBlbmdpbmVlciIsICJyZW1vdGUiMgsQABiABBiRAhiKBTILEAAYgAQYkQIYigUyCxAAGIAEGJECGIoFMgUQABiABDILEAAYgAQYkQIYigUyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABEj2SFDtCVitRnACeACQAQCYAcICoAG7MKoBCDAuMjAuOS4xuAEDyAEA-AEBmAINoALQEqgCFMICExAAGIAEGEMYtAIYigUY6gLYAQHCAhkQLhiABBjRAxhDGLQCGMcBGIoFGOoC2AEBwgIQEAAYAxi0AhjqAhiPAdgBAsICEBAuGAMYtAIY6gIYjwHYAQLCAhMQLhgDGNQCGLQCGOoCGI8B2AECwgIGEAAYBxgewgIREAAYgAQYkQIYsQMYyQMYigXCAgsQABiABBiSAxiKBcICCBAAGIAEGLEDwgILEAAYgAQYsQMYgwHCAggQABgHGAoYHsICCBAAGAcYCBgewgIKEAAYBxgIGAoYHsICDhAAGIAEGJECGLEDGIoFmAMe8QXIpZ9rtRWgA7oGBAgBGAe6BgYIAhABGAqSBwUyLjQuN6AHwbEB&sclient=gws-wiz-serp&jbr=sep:0> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7e5e79adcdd0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 61, in parse
    job_items = driver.find_elements(By.XPATH, '//div[@class="ZNyqGc"]')
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 926, in find_elements
    return self.execute(Command.FIND_ELEMENTS, {"using": by, "value": value})["value"] or []
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=53289): Max retries exceeded with url: /session/5cd016f8946649df099b5fc58fad758d/elements (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e5e79adcdd0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-13 19:26:47 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-13 19:32:14 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 19:32:14 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 19:32:14 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 19:32:14 [scrapy.extensions.telnet] INFO: Telnet Password: c3545d55fe869027
2025-02-13 19:32:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 19:32:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 19:32:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 19:32:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 19:32:14 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 19:32:14 [scrapy.core.engine] INFO: Spider opened
2025-02-13 19:32:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 19:32:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-13 19:32:20 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 19:33:18 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2025-02-13 19:33:18 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 19:33:18 [scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: data/google_jobs_2025-02-13T14-32-14+00-00.json
2025-02-13 19:33:18 [scrapy.extensions.feedexport] INFO: Stored csv feed (1 items) in: data/google_jobs_2025-02-13T14-32-14+00-00.csv
2025-02-13 19:33:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2342,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41890,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 63.982588,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 14, 33, 18, 782885, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74534,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'items_per_minute': None,
 'log_count/INFO': 14,
 'memusage/max': 118947840,
 'memusage/startup': 72908800,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 14, 32, 14, 800297, tzinfo=datetime.timezone.utc)}
2025-02-13 19:33:18 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 19:35:28 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 19:35:28 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 19:35:28 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 19:35:28 [scrapy.extensions.telnet] INFO: Telnet Password: d1524d6bf62cabd4
2025-02-13 19:35:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 19:35:28 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 19:35:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 19:35:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 19:35:28 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 19:35:28 [scrapy.core.engine] INFO: Spider opened
2025-02-13 19:35:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 19:35:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-13 19:35:34 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 19:35:54 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 19:35:54 [scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: data/google_jobs_2025-02-13T14-35-28+00-00.json
2025-02-13 19:35:54 [scrapy.extensions.feedexport] INFO: Stored csv feed (1 items) in: data/google_jobs_2025-02-13T14-35-28+00-00.csv
2025-02-13 19:35:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2343,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41445,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 26.126939,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 14, 35, 54, 556548, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74521,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 72871936,
 'memusage/startup': 72871936,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 14, 35, 28, 429609, tzinfo=datetime.timezone.utc)}
2025-02-13 19:35:54 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 19:38:02 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 19:38:02 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 19:38:02 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 19:38:02 [scrapy.extensions.telnet] INFO: Telnet Password: c45988cfe6bbbf8d
2025-02-13 19:38:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter']
2025-02-13 19:38:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOGSTATS_INTERVAL': 0,
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 19:38:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 19:38:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 19:38:02 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 19:38:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-13 19:51:05 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 19:51:05 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 19:51:05 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 19:51:05 [scrapy.extensions.telnet] INFO: Telnet Password: 8583417fe295c030
2025-02-13 19:51:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 19:51:05 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 19:51:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 19:51:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 19:51:05 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 19:51:05 [scrapy.core.engine] INFO: Spider opened
2025-02-13 19:51:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 19:51:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-13 19:51:11 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 19:51:28 [google_jobs] ERROR: Error extracting job details: Message: element click intercepted: Element <div jscontroller="b11o3b" class="EimVGf" data-share-url="https://www.google.com/search?ibp=htl;jobs&amp;q=%22software+engineer%22,+%22remote%22&amp;htidocid=_TJ6vr1666iN7Y-ZAAAAAA%3D%3D&amp;hl=en-PK&amp;shndl=37&amp;shmd=H4sIAAAAAAAA_xXMsQrCMBCAYVw7OTvdLJgUwUU3QQQnoQ9Q0ngmseldyJ3Y1Te3Lv_2_c131bRn50ekB3T81I-rCBcKiRAr7ODGAwi66iMwwZU5ZNycomqRo7Ui2QRRp8kbz5NlwoFn--JB_uklLruSnWK_P7SzKRS2647fEySCuxvTYukHYtercYQAAAA&amp;shmds=v1_AUFQtOMCsH42IHKPUf6n0-EBhAWxVRA159b9-r99zHbtLnmXDw&amp;source=sh/x/job/li/m1/1#fpstate=tldetail&amp;htivrt=jobs&amp;htiq=%22software+engineer%22,+%22remote%22&amp;htidocid=_TJ6vr1666iN7Y-ZAAAAAA%3D%3D" jsdata="UmMWcb;_;CSs9YU" jsaction="rcuQ6b:npT2md;DuVCmd:XftAS;KpCHeb:gByCrf;R3WaDd:AZSe2;h5M12e">...</div> is not clickable at point (530, 63). Other element would receive the click: <textarea class="gLFyf" aria-controls="Alh6id" aria-owns="Alh6id" value="&quot;software engineer&quot;, &quot;remote&quot;" aria-label="Search" placeholder="" aria-autocomplete="both" aria-expanded="false" aria-haspopup="false" autocapitalize="off" autocomplete="off" autocorrect="off" id="APjFqb" maxlength="2048" name="q" role="combobox" rows="1" spellcheck="false" jsaction="paste:puy29d" data-ved="0ahUKEwiQq7OF88CLAxWhUqQEHTDNIpYQ39UDCA0">...</textarea>
  (Session info: chrome=133.0.6943.53)
Stacktrace:
#0 0x62e913c06bba <unknown>
#1 0x62e9136a4790 <unknown>
#2 0x62e9136fca4c <unknown>
#3 0x62e9136fa8b5 <unknown>
#4 0x62e9136f7f52 <unknown>
#5 0x62e9136f7647 <unknown>
#6 0x62e9136e9c2d <unknown>
#7 0x62e91371ba52 <unknown>
#8 0x62e9136e95aa <unknown>
#9 0x62e91371bc1e <unknown>
#10 0x62e913741ccc <unknown>
#11 0x62e91371b823 <unknown>
#12 0x62e9136e7a88 <unknown>
#13 0x62e9136e8bf1 <unknown>
#14 0x62e913bd015b <unknown>
#15 0x62e913bd40e2 <unknown>
#16 0x62e913bbd01c <unknown>
#17 0x62e913bd4cd4 <unknown>
#18 0x62e913ba148f <unknown>
#19 0x62e913bf54f8 <unknown>
#20 0x62e913bf56c9 <unknown>
#21 0x62e913c05a36 <unknown>
#22 0x7a0b3689caa4 <unknown>
#23 0x7a0b36929c3c <unknown>

2025-02-13 19:51:29 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 19:51:29 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-13T14-51-05+00-00.json
2025-02-13 19:51:29 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-13T14-51-05+00-00.csv
2025-02-13 19:51:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2343,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41753,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 23.52495,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 14, 51, 29, 30193, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74377,
 'httpcompression/response_count': 1,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 13,
 'memusage/max': 73035776,
 'memusage/startup': 73035776,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 14, 51, 5, 505243, tzinfo=datetime.timezone.utc)}
2025-02-13 19:51:29 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 19:53:45 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 19:53:45 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 19:53:45 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 19:53:45 [scrapy.extensions.telnet] INFO: Telnet Password: 364dbfc93f0ec440
2025-02-13 19:53:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 19:53:45 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 19:53:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 19:53:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 19:53:46 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 19:53:46 [scrapy.core.engine] INFO: Spider opened
2025-02-13 19:53:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 19:53:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-13 19:53:51 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 19:54:12 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 19:54:12 [scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: data/google_jobs_2025-02-13T14-53-46+00-00.json
2025-02-13 19:54:12 [scrapy.extensions.feedexport] INFO: Stored csv feed (1 items) in: data/google_jobs_2025-02-13T14-53-46+00-00.csv
2025-02-13 19:54:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2340,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41507,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 26.45817,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 14, 54, 12, 479949, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74645,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73105408,
 'memusage/startup': 73105408,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 14, 53, 46, 21779, tzinfo=datetime.timezone.utc)}
2025-02-13 19:54:12 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 20:02:37 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 20:02:37 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 20:02:37 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 20:02:37 [scrapy.extensions.telnet] INFO: Telnet Password: 38ca7a84c8610d14
2025-02-13 20:02:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 20:02:37 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 20:02:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 20:02:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 20:02:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 20:02:38 [scrapy.core.engine] INFO: Spider opened
2025-02-13 20:02:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 20:02:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-13 20:02:42 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 20:03:04 [google_jobs] ERROR: Error extracting job details: Message: element click intercepted: Element <a class="MQUd2b" href="https://www.google.com/search?q=%22software+engineer%22,+%22remote%22&amp;sca_esv=15a52cd0682ab787&amp;ei=vPKtZ8-tCerY7M8PucuH6Ao&amp;uact=5&amp;oq=%22software+engineer%22,+%22remote%22&amp;gs_lp=Egxnd3Mtd2l6LXNlcnAiHSJzb2Z0d2FyZSBlbmdpbmVlciIsICJyZW1vdGUiMgsQABiABBiRAhiKBTILEAAYgAQYkQIYigUyCxAAGIAEGJECGIoFMgUQABiABDILEAAYgAQYkQIYigUyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABEj2SFDtCVitRnACeACQAQCYAcICoAG7MKoBCDAuMjAuOS4xuAEDyAEA-AEBmAINoALQEqgCFMICExAAGIAEGEMYtAIYigUY6gLYAQHCAhkQLhiABBjRAxhDGLQCGMcBGIoFGOoC2AEBwgIQEAAYAxi0AhjqAhiPAdgBAsICEBAuGAMYtAIY6gIYjwHYAQLCAhMQLhgDGNQCGLQCGOoCGI8B2AECwgIGEAAYBxgewgIREAAYgAQYkQIYsQMYyQMYigXCAgsQABiABBiSAxiKBcICCBAAGIAEGLEDwgILEAAYgAQYsQMYgwHCAggQABgHGAoYHsICCBAAGAcYCBgewgIKEAAYBxgIGAoYHsICDhAAGIAEGJECGLEDGIoFmAMe8QXIpZ9rtRWgA7oGBAgBGAe6BgYIAhABGAqSBwUyLjQuN6AHwbEB&amp;sclient=gws-wiz-serp&amp;jbr=sep:0&amp;sei=kwmuZ-GFMI2hkdUPlI6KgAg&amp;udm=8#vhid=vt%3D20/docid%3D_TJ6vr1666iN7Y-ZAAAAAA%3D%3D&amp;vssid=jobs-detail-viewer" tabindex="-1">...</a> is not clickable at point (530, 63). Other element would receive the click: <textarea class="gLFyf" aria-controls="Alh6id" aria-owns="Alh6id" value="&quot;software engineer&quot;, &quot;remote&quot;" aria-label="Search" placeholder="" aria-autocomplete="both" aria-expanded="false" aria-haspopup="false" autocapitalize="off" autocomplete="off" autocorrect="off" id="APjFqb" maxlength="2048" name="q" role="combobox" rows="1" spellcheck="false" jsaction="paste:puy29d" data-ved="0ahUKEwjUuI3P9cCLAxW2KvsDHY5NKFcQ39UDCA0">...</textarea>
  (Session info: chrome=133.0.6943.53)
Stacktrace:
#0 0x5f443c0d6bba <unknown>
#1 0x5f443bb74790 <unknown>
#2 0x5f443bbcca4c <unknown>
#3 0x5f443bbca8b5 <unknown>
#4 0x5f443bbc7f52 <unknown>
#5 0x5f443bbc7647 <unknown>
#6 0x5f443bbb9c2d <unknown>
#7 0x5f443bbeba52 <unknown>
#8 0x5f443bbb95aa <unknown>
#9 0x5f443bbebc1e <unknown>
#10 0x5f443bc11ccc <unknown>
#11 0x5f443bbeb823 <unknown>
#12 0x5f443bbb7a88 <unknown>
#13 0x5f443bbb8bf1 <unknown>
#14 0x5f443c0a015b <unknown>
#15 0x5f443c0a40e2 <unknown>
#16 0x5f443c08d01c <unknown>
#17 0x5f443c0a4cd4 <unknown>
#18 0x5f443c07148f <unknown>
#19 0x5f443c0c54f8 <unknown>
#20 0x5f443c0c56c9 <unknown>
#21 0x5f443c0d5a36 <unknown>
#22 0x7eef7f49caa4 <unknown>
#23 0x7eef7f529c3c <unknown>

2025-02-13 20:03:04 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 20:03:04 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-13T15-02-38+00-00.json
2025-02-13 20:03:04 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-13T15-02-38+00-00.csv
2025-02-13 20:03:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2341,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41444,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 26.167673,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 15, 3, 4, 197427, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74526,
 'httpcompression/response_count': 1,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 13,
 'memusage/max': 73338880,
 'memusage/startup': 73338880,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 15, 2, 38, 29754, tzinfo=datetime.timezone.utc)}
2025-02-13 20:03:04 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 20:04:07 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 20:04:07 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 20:04:07 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 20:04:07 [scrapy.extensions.telnet] INFO: Telnet Password: e2c7a42357dacfef
2025-02-13 20:04:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 20:04:07 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 20:04:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 20:04:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 20:04:07 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 20:04:07 [scrapy.core.engine] INFO: Spider opened
2025-02-13 20:04:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 20:04:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-13 20:04:13 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 20:04:40 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 20:04:40 [scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: data/google_jobs_2025-02-13T15-04-07+00-00.json
2025-02-13 20:04:40 [scrapy.extensions.feedexport] INFO: Stored csv feed (1 items) in: data/google_jobs_2025-02-13T15-04-07+00-00.csv
2025-02-13 20:04:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2340,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41524,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 32.744242,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 15, 4, 40, 686571, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74639,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73179136,
 'memusage/startup': 73179136,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 15, 4, 7, 942329, tzinfo=datetime.timezone.utc)}
2025-02-13 20:04:40 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 20:05:04 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 20:05:04 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 20:05:04 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 20:05:04 [scrapy.extensions.telnet] INFO: Telnet Password: ee5bc02af662cfb0
2025-02-13 20:05:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 20:05:04 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 20:05:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 20:05:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 20:05:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 20:05:04 [scrapy.core.engine] INFO: Spider opened
2025-02-13 20:05:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 20:05:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-13 20:05:10 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 20:05:38 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 20:05:38 [scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: data/google_jobs_2025-02-13T15-05-04+00-00.json
2025-02-13 20:05:38 [scrapy.extensions.feedexport] INFO: Stored csv feed (1 items) in: data/google_jobs_2025-02-13T15-05-04+00-00.csv
2025-02-13 20:05:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2340,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41342,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 34.071962,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 15, 5, 38, 513466, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74410,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73035776,
 'memusage/startup': 73035776,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 15, 5, 4, 441504, tzinfo=datetime.timezone.utc)}
2025-02-13 20:05:38 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 20:11:17 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 20:11:17 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 20:11:17 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 20:11:17 [scrapy.extensions.telnet] INFO: Telnet Password: e269a7d71578e8a5
2025-02-13 20:11:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 20:11:17 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 20:11:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 20:11:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 20:11:17 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 20:11:17 [scrapy.core.engine] INFO: Spider opened
2025-02-13 20:11:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 20:11:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-13 20:11:23 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 20:11:44 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 20:11:44 [scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: data/google_jobs_2025-02-13T15-11-17+00-00.json
2025-02-13 20:11:44 [scrapy.extensions.feedexport] INFO: Stored csv feed (1 items) in: data/google_jobs_2025-02-13T15-11-17+00-00.csv
2025-02-13 20:11:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2341,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41448,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 27.242561,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 15, 11, 44, 895051, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74535,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 72982528,
 'memusage/startup': 72982528,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 15, 11, 17, 652490, tzinfo=datetime.timezone.utc)}
2025-02-13 20:11:44 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 20:12:55 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 20:12:55 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 20:12:55 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 20:12:55 [scrapy.extensions.telnet] INFO: Telnet Password: 1a4a26a7f170831a
2025-02-13 20:12:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 20:12:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 20:12:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 20:12:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 20:12:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 20:12:55 [scrapy.core.engine] INFO: Spider opened
2025-02-13 20:12:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 20:12:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-13 20:13:00 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 20:13:27 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 20:13:27 [scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: data/google_jobs_2025-02-13T15-12-55+00-00.json
2025-02-13 20:13:27 [scrapy.extensions.feedexport] INFO: Stored csv feed (1 items) in: data/google_jobs_2025-02-13T15-12-55+00-00.csv
2025-02-13 20:13:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2343,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41488,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 32.186844,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 15, 13, 27, 238539, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74578,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73129984,
 'memusage/startup': 73129984,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 15, 12, 55, 51695, tzinfo=datetime.timezone.utc)}
2025-02-13 20:13:27 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 20:20:51 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 20:20:51 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 20:20:51 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 20:20:51 [scrapy.extensions.telnet] INFO: Telnet Password: ae53a7379fd02362
2025-02-13 20:20:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 20:20:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 20:20:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 20:20:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 20:20:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 20:20:51 [scrapy.core.engine] INFO: Spider opened
2025-02-13 20:20:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 20:20:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-13 20:20:56 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 20:21:21 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 20:21:21 [scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: data/google_jobs_2025-02-13T15-20-51+00-00.json
2025-02-13 20:21:21 [scrapy.extensions.feedexport] INFO: Stored csv feed (1 items) in: data/google_jobs_2025-02-13T15-20-51+00-00.csv
2025-02-13 20:21:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2343,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41211,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 30.514804,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 15, 21, 21, 906325, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74250,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73064448,
 'memusage/startup': 73064448,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 15, 20, 51, 391521, tzinfo=datetime.timezone.utc)}
2025-02-13 20:21:21 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 20:22:30 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 20:22:30 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 20:22:30 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 20:22:30 [scrapy.extensions.telnet] INFO: Telnet Password: 4eaee9b6e5184cda
2025-02-13 20:22:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 20:22:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 20:22:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 20:22:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 20:22:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 20:22:31 [scrapy.core.engine] INFO: Spider opened
2025-02-13 20:22:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 20:22:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-13 20:22:36 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 20:22:44 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-13 20:22:46 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-13 20:22:47 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x73fb24973ec0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/1242de05ae7eeac6d460f54d5e108927/execute/sync
2025-02-13 20:22:47 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x73fb24973b60>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/1242de05ae7eeac6d460f54d5e108927/execute/sync
2025-02-13 20:22:47 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x73fb24988350>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/1242de05ae7eeac6d460f54d5e108927/execute/sync
2025-02-13 20:22:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com/search?q=%22software+engineer%22,+%22remote%22> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x73fb24988110>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 184, in parse
    self.scroll_to_load_jobs(driver, max_scrolls=5)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 243, in scroll_to_load_jobs
    driver.execute_script("window.scrollBy(0, 1000);")
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=45319): Max retries exceeded with url: /session/1242de05ae7eeac6d460f54d5e108927/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x73fb24988110>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-13 20:22:47 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-13 20:23:20 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 20:23:20 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 20:23:20 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 20:23:20 [scrapy.extensions.telnet] INFO: Telnet Password: eaea71366e53945a
2025-02-13 20:23:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 20:23:20 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 20:23:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 20:23:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 20:23:20 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 20:23:20 [scrapy.core.engine] INFO: Spider opened
2025-02-13 20:23:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 20:23:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-13 20:23:26 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 20:23:51 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 20:23:51 [scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: data/google_jobs_2025-02-13T15-23-20+00-00.json
2025-02-13 20:23:51 [scrapy.extensions.feedexport] INFO: Stored csv feed (1 items) in: data/google_jobs_2025-02-13T15-23-20+00-00.csv
2025-02-13 20:23:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 720,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39692,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 31.044398,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 15, 23, 51, 856008, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74409,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73146368,
 'memusage/startup': 73146368,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 15, 23, 20, 811610, tzinfo=datetime.timezone.utc)}
2025-02-13 20:23:51 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 20:31:14 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 20:31:14 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 20:31:14 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 20:31:14 [scrapy.extensions.telnet] INFO: Telnet Password: 9a06d486391b0068
2025-02-13 20:31:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 20:31:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 20:31:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 20:31:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 20:31:14 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 20:31:14 [scrapy.core.engine] INFO: Spider opened
2025-02-13 20:31:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 20:31:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-13 20:31:18 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 20:31:36 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 20:31:36 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-13T15-31-14+00-00.json
2025-02-13 20:31:36 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-13T15-31-14+00-00.csv
2025-02-13 20:31:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2342,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41450,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 22.216618,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 15, 31, 36, 885695, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74535,
 'httpcompression/response_count': 1,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73187328,
 'memusage/startup': 73187328,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 15, 31, 14, 669077, tzinfo=datetime.timezone.utc)}
2025-02-13 20:31:36 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 20:32:58 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 20:32:58 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 20:32:58 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 20:32:58 [scrapy.extensions.telnet] INFO: Telnet Password: 39b56b2199f17f70
2025-02-13 20:32:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 20:32:58 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 20:32:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 20:32:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 20:32:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 20:32:58 [scrapy.core.engine] INFO: Spider opened
2025-02-13 20:32:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 20:32:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-13 20:33:04 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 20:33:59 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 10 items (at 10 items/min)
2025-02-13 20:34:58 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 27 items (at 17 items/min)
2025-02-13 20:35:59 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 45 items (at 18 items/min)
2025-02-13 20:36:03 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 20:36:03 [scrapy.extensions.feedexport] INFO: Stored json feed (46 items) in: data/google_jobs_2025-02-13T15-32-58+00-00.json
2025-02-13 20:36:03 [scrapy.extensions.feedexport] INFO: Stored csv feed (46 items) in: data/google_jobs_2025-02-13T15-32-58+00-00.csv
2025-02-13 20:36:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2343,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41588,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 185.35411,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 15, 36, 3, 976524, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74751,
 'httpcompression/response_count': 1,
 'item_scraped_count': 46,
 'items_per_minute': None,
 'log_count/INFO': 16,
 'memusage/max': 119156736,
 'memusage/startup': 73121792,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 15, 32, 58, 622414, tzinfo=datetime.timezone.utc)}
2025-02-13 20:36:03 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 20:45:03 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 20:45:03 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 20:45:03 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 20:45:03 [scrapy.extensions.telnet] INFO: Telnet Password: bbe99b8936c15171
2025-02-13 20:45:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 20:45:03 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 20:45:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 20:45:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 20:45:03 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 20:45:03 [scrapy.core.engine] INFO: Spider opened
2025-02-13 20:45:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 20:45:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-13 20:45:08 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 20:46:05 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 13 items (at 13 items/min)
2025-02-13 20:46:55 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 20:46:55 [scrapy.extensions.feedexport] INFO: Stored json feed (26 items) in: data/google_jobs_2025-02-13T15-45-03+00-00.json
2025-02-13 20:46:55 [scrapy.extensions.feedexport] INFO: Stored csv feed (26 items) in: data/google_jobs_2025-02-13T15-45-03+00-00.csv
2025-02-13 20:46:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2342,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41496,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 112.028434,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 15, 46, 55, 609501, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74607,
 'httpcompression/response_count': 1,
 'item_scraped_count': 26,
 'items_per_minute': None,
 'log_count/INFO': 14,
 'memusage/max': 119283712,
 'memusage/startup': 73113600,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 15, 45, 3, 581067, tzinfo=datetime.timezone.utc)}
2025-02-13 20:46:55 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 21:01:22 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 21:01:22 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 21:01:22 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 21:01:22 [scrapy.extensions.telnet] INFO: Telnet Password: f2d15fdd2f38d56b
2025-02-13 21:01:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 21:01:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 21:01:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 21:01:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 21:01:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 21:01:22 [scrapy.core.engine] INFO: Spider opened
2025-02-13 21:01:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 21:01:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-13 21:01:28 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 21:01:41 [google_jobs] ERROR: Error extracting job details: Message: invalid selector: Unable to locate an element with the xpath expression //div[containing(@class, "Emjfjd")] because of the following error:
SyntaxError: Failed to execute 'evaluate' on 'Document': The string '//div[containing(@class, "Emjfjd")]' is not a valid XPath expression.
  (Session info: chrome=133.0.6943.53); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalid-selector-exception
Stacktrace:
#0 0x61db6c44cbba <unknown>
#1 0x61db6beea790 <unknown>
#2 0x61db6bef14fa <unknown>
#3 0x61db6bef3a98 <unknown>
#4 0x61db6bef3b23 <unknown>
#5 0x61db6bf3b6a2 <unknown>
#6 0x61db6bf3be01 <unknown>
#7 0x61db6bf8a944 <unknown>
#8 0x61db6bf61a7d <unknown>
#9 0x61db6bf87ccc <unknown>
#10 0x61db6bf61823 <unknown>
#11 0x61db6bf2da88 <unknown>
#12 0x61db6bf2ebf1 <unknown>
#13 0x61db6c41615b <unknown>
#14 0x61db6c41a0e2 <unknown>
#15 0x61db6c40301c <unknown>
#16 0x61db6c41acd4 <unknown>
#17 0x61db6c3e748f <unknown>
#18 0x61db6c43b4f8 <unknown>
#19 0x61db6c43b6c9 <unknown>
#20 0x61db6c44ba36 <unknown>
#21 0x73d389e9caa4 <unknown>
#22 0x73d389f29c3c <unknown>

2025-02-13 21:01:44 [google_jobs] ERROR: Error extracting job details: Message: invalid selector: Unable to locate an element with the xpath expression //div[containing(@class, "Emjfjd")] because of the following error:
SyntaxError: Failed to execute 'evaluate' on 'Document': The string '//div[containing(@class, "Emjfjd")]' is not a valid XPath expression.
  (Session info: chrome=133.0.6943.53); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalid-selector-exception
Stacktrace:
#0 0x61db6c44cbba <unknown>
#1 0x61db6beea790 <unknown>
#2 0x61db6bef14fa <unknown>
#3 0x61db6bef3a98 <unknown>
#4 0x61db6bef3b23 <unknown>
#5 0x61db6bf3b6a2 <unknown>
#6 0x61db6bf3be01 <unknown>
#7 0x61db6bf8a944 <unknown>
#8 0x61db6bf61a7d <unknown>
#9 0x61db6bf87ccc <unknown>
#10 0x61db6bf61823 <unknown>
#11 0x61db6bf2da88 <unknown>
#12 0x61db6bf2ebf1 <unknown>
#13 0x61db6c41615b <unknown>
#14 0x61db6c41a0e2 <unknown>
#15 0x61db6c40301c <unknown>
#16 0x61db6c41acd4 <unknown>
#17 0x61db6c3e748f <unknown>
#18 0x61db6c43b4f8 <unknown>
#19 0x61db6c43b6c9 <unknown>
#20 0x61db6c44ba36 <unknown>
#21 0x73d389e9caa4 <unknown>
#22 0x73d389f29c3c <unknown>

2025-02-13 21:01:48 [google_jobs] ERROR: Error extracting job details: Message: invalid selector: Unable to locate an element with the xpath expression //div[containing(@class, "Emjfjd")] because of the following error:
SyntaxError: Failed to execute 'evaluate' on 'Document': The string '//div[containing(@class, "Emjfjd")]' is not a valid XPath expression.
  (Session info: chrome=133.0.6943.53); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalid-selector-exception
Stacktrace:
#0 0x61db6c44cbba <unknown>
#1 0x61db6beea790 <unknown>
#2 0x61db6bef14fa <unknown>
#3 0x61db6bef3a98 <unknown>
#4 0x61db6bef3b23 <unknown>
#5 0x61db6bf3b6a2 <unknown>
#6 0x61db6bf3be01 <unknown>
#7 0x61db6bf8a944 <unknown>
#8 0x61db6bf61a7d <unknown>
#9 0x61db6bf87ccc <unknown>
#10 0x61db6bf61823 <unknown>
#11 0x61db6bf2da88 <unknown>
#12 0x61db6bf2ebf1 <unknown>
#13 0x61db6c41615b <unknown>
#14 0x61db6c41a0e2 <unknown>
#15 0x61db6c40301c <unknown>
#16 0x61db6c41acd4 <unknown>
#17 0x61db6c3e748f <unknown>
#18 0x61db6c43b4f8 <unknown>
#19 0x61db6c43b6c9 <unknown>
#20 0x61db6c44ba36 <unknown>
#21 0x73d389e9caa4 <unknown>
#22 0x73d389f29c3c <unknown>

2025-02-13 21:01:51 [google_jobs] ERROR: Error extracting job details: Message: invalid selector: Unable to locate an element with the xpath expression //div[containing(@class, "Emjfjd")] because of the following error:
SyntaxError: Failed to execute 'evaluate' on 'Document': The string '//div[containing(@class, "Emjfjd")]' is not a valid XPath expression.
  (Session info: chrome=133.0.6943.53); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalid-selector-exception
Stacktrace:
#0 0x61db6c44cbba <unknown>
#1 0x61db6beea790 <unknown>
#2 0x61db6bef14fa <unknown>
#3 0x61db6bef3a98 <unknown>
#4 0x61db6bef3b23 <unknown>
#5 0x61db6bf3b6a2 <unknown>
#6 0x61db6bf3be01 <unknown>
#7 0x61db6bf8a944 <unknown>
#8 0x61db6bf61a7d <unknown>
#9 0x61db6bf87ccc <unknown>
#10 0x61db6bf61823 <unknown>
#11 0x61db6bf2da88 <unknown>
#12 0x61db6bf2ebf1 <unknown>
#13 0x61db6c41615b <unknown>
#14 0x61db6c41a0e2 <unknown>
#15 0x61db6c40301c <unknown>
#16 0x61db6c41acd4 <unknown>
#17 0x61db6c3e748f <unknown>
#18 0x61db6c43b4f8 <unknown>
#19 0x61db6c43b6c9 <unknown>
#20 0x61db6c44ba36 <unknown>
#21 0x73d389e9caa4 <unknown>
#22 0x73d389f29c3c <unknown>

2025-02-13 21:01:54 [google_jobs] ERROR: Error extracting job details: Message: invalid selector: Unable to locate an element with the xpath expression //div[containing(@class, "Emjfjd")] because of the following error:
SyntaxError: Failed to execute 'evaluate' on 'Document': The string '//div[containing(@class, "Emjfjd")]' is not a valid XPath expression.
  (Session info: chrome=133.0.6943.53); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalid-selector-exception
Stacktrace:
#0 0x61db6c44cbba <unknown>
#1 0x61db6beea790 <unknown>
#2 0x61db6bef14fa <unknown>
#3 0x61db6bef3a98 <unknown>
#4 0x61db6bef3b23 <unknown>
#5 0x61db6bf3b6a2 <unknown>
#6 0x61db6bf3be01 <unknown>
#7 0x61db6bf8a944 <unknown>
#8 0x61db6bf61a7d <unknown>
#9 0x61db6bf87ccc <unknown>
#10 0x61db6bf61823 <unknown>
#11 0x61db6bf2da88 <unknown>
#12 0x61db6bf2ebf1 <unknown>
#13 0x61db6c41615b <unknown>
#14 0x61db6c41a0e2 <unknown>
#15 0x61db6c40301c <unknown>
#16 0x61db6c41acd4 <unknown>
#17 0x61db6c3e748f <unknown>
#18 0x61db6c43b4f8 <unknown>
#19 0x61db6c43b6c9 <unknown>
#20 0x61db6c44ba36 <unknown>
#21 0x73d389e9caa4 <unknown>
#22 0x73d389f29c3c <unknown>

2025-02-13 21:01:57 [google_jobs] ERROR: Error extracting job details: Message: invalid selector: Unable to locate an element with the xpath expression //div[containing(@class, "Emjfjd")] because of the following error:
SyntaxError: Failed to execute 'evaluate' on 'Document': The string '//div[containing(@class, "Emjfjd")]' is not a valid XPath expression.
  (Session info: chrome=133.0.6943.53); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalid-selector-exception
Stacktrace:
#0 0x61db6c44cbba <unknown>
#1 0x61db6beea790 <unknown>
#2 0x61db6bef14fa <unknown>
#3 0x61db6bef3a98 <unknown>
#4 0x61db6bef3b23 <unknown>
#5 0x61db6bf3b6a2 <unknown>
#6 0x61db6bf3be01 <unknown>
#7 0x61db6bf8a944 <unknown>
#8 0x61db6bf61a7d <unknown>
#9 0x61db6bf87ccc <unknown>
#10 0x61db6bf61823 <unknown>
#11 0x61db6bf2da88 <unknown>
#12 0x61db6bf2ebf1 <unknown>
#13 0x61db6c41615b <unknown>
#14 0x61db6c41a0e2 <unknown>
#15 0x61db6c40301c <unknown>
#16 0x61db6c41acd4 <unknown>
#17 0x61db6c3e748f <unknown>
#18 0x61db6c43b4f8 <unknown>
#19 0x61db6c43b6c9 <unknown>
#20 0x61db6c44ba36 <unknown>
#21 0x73d389e9caa4 <unknown>
#22 0x73d389f29c3c <unknown>

2025-02-13 21:02:00 [google_jobs] ERROR: Error extracting job details: Message: invalid selector: Unable to locate an element with the xpath expression //div[containing(@class, "Emjfjd")] because of the following error:
SyntaxError: Failed to execute 'evaluate' on 'Document': The string '//div[containing(@class, "Emjfjd")]' is not a valid XPath expression.
  (Session info: chrome=133.0.6943.53); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalid-selector-exception
Stacktrace:
#0 0x61db6c44cbba <unknown>
#1 0x61db6beea790 <unknown>
#2 0x61db6bef14fa <unknown>
#3 0x61db6bef3a98 <unknown>
#4 0x61db6bef3b23 <unknown>
#5 0x61db6bf3b6a2 <unknown>
#6 0x61db6bf3be01 <unknown>
#7 0x61db6bf8a944 <unknown>
#8 0x61db6bf61a7d <unknown>
#9 0x61db6bf87ccc <unknown>
#10 0x61db6bf61823 <unknown>
#11 0x61db6bf2da88 <unknown>
#12 0x61db6bf2ebf1 <unknown>
#13 0x61db6c41615b <unknown>
#14 0x61db6c41a0e2 <unknown>
#15 0x61db6c40301c <unknown>
#16 0x61db6c41acd4 <unknown>
#17 0x61db6c3e748f <unknown>
#18 0x61db6c43b4f8 <unknown>
#19 0x61db6c43b6c9 <unknown>
#20 0x61db6c44ba36 <unknown>
#21 0x73d389e9caa4 <unknown>
#22 0x73d389f29c3c <unknown>

2025-02-13 21:02:03 [google_jobs] ERROR: Error extracting job details: Message: invalid selector: Unable to locate an element with the xpath expression //div[containing(@class, "Emjfjd")] because of the following error:
SyntaxError: Failed to execute 'evaluate' on 'Document': The string '//div[containing(@class, "Emjfjd")]' is not a valid XPath expression.
  (Session info: chrome=133.0.6943.53); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalid-selector-exception
Stacktrace:
#0 0x61db6c44cbba <unknown>
#1 0x61db6beea790 <unknown>
#2 0x61db6bef14fa <unknown>
#3 0x61db6bef3a98 <unknown>
#4 0x61db6bef3b23 <unknown>
#5 0x61db6bf3b6a2 <unknown>
#6 0x61db6bf3be01 <unknown>
#7 0x61db6bf8a944 <unknown>
#8 0x61db6bf61a7d <unknown>
#9 0x61db6bf87ccc <unknown>
#10 0x61db6bf61823 <unknown>
#11 0x61db6bf2da88 <unknown>
#12 0x61db6bf2ebf1 <unknown>
#13 0x61db6c41615b <unknown>
#14 0x61db6c41a0e2 <unknown>
#15 0x61db6c40301c <unknown>
#16 0x61db6c41acd4 <unknown>
#17 0x61db6c3e748f <unknown>
#18 0x61db6c43b4f8 <unknown>
#19 0x61db6c43b6c9 <unknown>
#20 0x61db6c44ba36 <unknown>
#21 0x73d389e9caa4 <unknown>
#22 0x73d389f29c3c <unknown>

2025-02-13 21:02:07 [google_jobs] ERROR: Error extracting job details: Message: invalid selector: Unable to locate an element with the xpath expression //div[containing(@class, "Emjfjd")] because of the following error:
SyntaxError: Failed to execute 'evaluate' on 'Document': The string '//div[containing(@class, "Emjfjd")]' is not a valid XPath expression.
  (Session info: chrome=133.0.6943.53); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalid-selector-exception
Stacktrace:
#0 0x61db6c44cbba <unknown>
#1 0x61db6beea790 <unknown>
#2 0x61db6bef14fa <unknown>
#3 0x61db6bef3a98 <unknown>
#4 0x61db6bef3b23 <unknown>
#5 0x61db6bf3b6a2 <unknown>
#6 0x61db6bf3be01 <unknown>
#7 0x61db6bf8a944 <unknown>
#8 0x61db6bf61a7d <unknown>
#9 0x61db6bf87ccc <unknown>
#10 0x61db6bf61823 <unknown>
#11 0x61db6bf2da88 <unknown>
#12 0x61db6bf2ebf1 <unknown>
#13 0x61db6c41615b <unknown>
#14 0x61db6c41a0e2 <unknown>
#15 0x61db6c40301c <unknown>
#16 0x61db6c41acd4 <unknown>
#17 0x61db6c3e748f <unknown>
#18 0x61db6c43b4f8 <unknown>
#19 0x61db6c43b6c9 <unknown>
#20 0x61db6c44ba36 <unknown>
#21 0x73d389e9caa4 <unknown>
#22 0x73d389f29c3c <unknown>

2025-02-13 21:02:10 [google_jobs] ERROR: Error extracting job details: Message: invalid selector: Unable to locate an element with the xpath expression //div[containing(@class, "Emjfjd")] because of the following error:
SyntaxError: Failed to execute 'evaluate' on 'Document': The string '//div[containing(@class, "Emjfjd")]' is not a valid XPath expression.
  (Session info: chrome=133.0.6943.53); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalid-selector-exception
Stacktrace:
#0 0x61db6c44cbba <unknown>
#1 0x61db6beea790 <unknown>
#2 0x61db6bef14fa <unknown>
#3 0x61db6bef3a98 <unknown>
#4 0x61db6bef3b23 <unknown>
#5 0x61db6bf3b6a2 <unknown>
#6 0x61db6bf3be01 <unknown>
#7 0x61db6bf8a944 <unknown>
#8 0x61db6bf61a7d <unknown>
#9 0x61db6bf87ccc <unknown>
#10 0x61db6bf61823 <unknown>
#11 0x61db6bf2da88 <unknown>
#12 0x61db6bf2ebf1 <unknown>
#13 0x61db6c41615b <unknown>
#14 0x61db6c41a0e2 <unknown>
#15 0x61db6c40301c <unknown>
#16 0x61db6c41acd4 <unknown>
#17 0x61db6c3e748f <unknown>
#18 0x61db6c43b4f8 <unknown>
#19 0x61db6c43b6c9 <unknown>
#20 0x61db6c44ba36 <unknown>
#21 0x73d389e9caa4 <unknown>
#22 0x73d389f29c3c <unknown>

2025-02-13 21:02:10 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 21:02:10 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-13T16-01-22+00-00.json
2025-02-13 21:02:10 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-13T16-01-22+00-00.csv
2025-02-13 21:02:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2340,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 42019,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 47.987963,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 16, 2, 10, 571254, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74735,
 'httpcompression/response_count': 1,
 'items_per_minute': None,
 'log_count/ERROR': 10,
 'log_count/INFO': 13,
 'memusage/max': 73031680,
 'memusage/startup': 73031680,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 16, 1, 22, 583291, tzinfo=datetime.timezone.utc)}
2025-02-13 21:02:10 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 21:04:26 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 21:04:26 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 21:04:26 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 21:04:26 [scrapy.extensions.telnet] INFO: Telnet Password: a3287da89ae37f77
2025-02-13 21:04:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 21:04:26 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 21:04:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 21:04:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 21:04:26 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 21:04:26 [scrapy.core.engine] INFO: Spider opened
2025-02-13 21:04:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 21:04:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-13 21:04:31 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 21:05:30 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 8 items (at 8 items/min)
2025-02-13 21:05:31 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-13 21:05:32 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-13 21:05:32 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78c28dc87cb0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fd989e5cca358a3b19c48b95e5325c98/element
2025-02-13 21:05:32 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78c28dcad280>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fd989e5cca358a3b19c48b95e5325c98/element
2025-02-13 21:05:32 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78c28dcad520>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fd989e5cca358a3b19c48b95e5325c98/element
2025-02-13 21:05:32 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=38295): Max retries exceeded with url: /session/fd989e5cca358a3b19c48b95e5325c98/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78c28dcad760>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-13 21:05:32 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78c28dcadac0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fd989e5cca358a3b19c48b95e5325c98/element/f.C2440D00EB92A4DC73A137E48C26710F.d.214D75DDA6DD7F27A39B4B1EC77A9921.e.113/click
2025-02-13 21:05:32 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78c28dcadcd0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fd989e5cca358a3b19c48b95e5325c98/element/f.C2440D00EB92A4DC73A137E48C26710F.d.214D75DDA6DD7F27A39B4B1EC77A9921.e.113/click
2025-02-13 21:05:32 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78c28dcadf40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fd989e5cca358a3b19c48b95e5325c98/element/f.C2440D00EB92A4DC73A137E48C26710F.d.214D75DDA6DD7F27A39B4B1EC77A9921.e.113/click
2025-02-13 21:05:32 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=38295): Max retries exceeded with url: /session/fd989e5cca358a3b19c48b95e5325c98/element/f.C2440D00EB92A4DC73A137E48C26710F.d.214D75DDA6DD7F27A39B4B1EC77A9921.e.113/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78c28dcae1b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-13 21:05:33 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-13 23:28:11 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 23:28:11 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 23:28:11 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 23:28:11 [scrapy.extensions.telnet] INFO: Telnet Password: 78d1d2f6719f80f8
2025-02-13 23:28:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 23:28:11 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 23:28:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 23:28:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 23:28:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 23:28:11 [scrapy.core.engine] INFO: Spider opened
2025-02-13 23:28:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 23:28:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-13 23:28:15 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 23:29:11 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 8 items (at 8 items/min)
2025-02-13 23:29:23 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 23:29:23 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-13T18-28-11+00-00.json
2025-02-13 23:29:23 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-13T18-28-11+00-00.csv
2025-02-13 23:29:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2342,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41426,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 71.726297,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 18, 29, 23, 23664, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74506,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 14,
 'memusage/max': 119037952,
 'memusage/startup': 73265152,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 18, 28, 11, 297367, tzinfo=datetime.timezone.utc)}
2025-02-13 23:29:23 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 23:36:07 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 23:36:07 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 23:36:07 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 23:36:07 [scrapy.extensions.telnet] INFO: Telnet Password: 489104cd4a3f55db
2025-02-13 23:36:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 23:36:07 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 23:36:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 23:36:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 23:36:07 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 23:36:07 [scrapy.core.engine] INFO: Spider opened
2025-02-13 23:36:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 23:36:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-13 23:36:12 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 23:36:38 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 23:36:38 [scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: data/google_jobs_2025-02-13T18-36-07+00-00.json
2025-02-13 23:36:38 [scrapy.extensions.feedexport] INFO: Stored csv feed (1 items) in: data/google_jobs_2025-02-13T18-36-07+00-00.csv
2025-02-13 23:36:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2342,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41263,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 30.781497,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 18, 36, 38, 757451, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74309,
 'httpcompression/response_count': 1,
 'item_scraped_count': 1,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73056256,
 'memusage/startup': 73056256,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 18, 36, 7, 975954, tzinfo=datetime.timezone.utc)}
2025-02-13 23:36:38 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 23:41:34 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 23:41:34 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 23:41:34 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 23:41:34 [scrapy.extensions.telnet] INFO: Telnet Password: 4a86b6bac01d0cbe
2025-02-13 23:41:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 23:41:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 23:41:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 23:41:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 23:41:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 23:41:34 [scrapy.core.engine] INFO: Spider opened
2025-02-13 23:41:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 23:41:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-13 23:41:39 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 23:41:41 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-13 23:41:42 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-13 23:41:43 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f70db55f590>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c81ba684f60d4ce21818cb768d8f2a4f/element
2025-02-13 23:41:43 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f70db5903e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c81ba684f60d4ce21818cb768d8f2a4f/element
2025-02-13 23:41:43 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f70db590260>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c81ba684f60d4ce21818cb768d8f2a4f/element
2025-02-13 23:41:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.google.com/search?q=%22software+engineer%22,+%22remote%22&sca_esv=15a52cd0682ab787&ei=vPKtZ8-tCerY7M8PucuH6Ao&ved=2ahUKEwiuoZv838CLAxW5RaQEHXvjNrUQ3L8LegQIIBAN&uact=5&oq=%22software+engineer%22,+%22remote%22&gs_lp=Egxnd3Mtd2l6LXNlcnAiHSJzb2Z0d2FyZSBlbmdpbmVlciIsICJyZW1vdGUiMgsQABiABBiRAhiKBTILEAAYgAQYkQIYigUyCxAAGIAEGJECGIoFMgUQABiABDILEAAYgAQYkQIYigUyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABEj2SFDtCVitRnACeACQAQCYAcICoAG7MKoBCDAuMjAuOS4xuAEDyAEA-AEBmAINoALQEqgCFMICExAAGIAEGEMYtAIYigUY6gLYAQHCAhkQLhiABBjRAxhDGLQCGMcBGIoFGOoC2AEBwgIQEAAYAxi0AhjqAhiPAdgBAsICEBAuGAMYtAIY6gIYjwHYAQLCAhMQLhgDGNQCGLQCGOoCGI8B2AECwgIGEAAYBxgewgIREAAYgAQYkQIYsQMYyQMYigXCAgsQABiABBiSAxiKBcICCBAAGIAEGLEDwgILEAAYgAQYsQMYgwHCAggQABgHGAoYHsICCBAAGAcYCBgewgIKEAAYBxgIGAoYHsICDhAAGIAEGJECGLEDGIoFmAMe8QXIpZ9rtRWgA7oGBAgBGAe6BgYIAhABGAqSBwUyLjQuN6AHwbEB&sclient=gws-wiz-serp&jbr=sep:0> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f70db590050>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 183, in parse
    load_more_button = driver.find_element(By.XPATH, '//div[@class="TOQyFc U48fD"]//a')
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 888, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=49339): Max retries exceeded with url: /session/c81ba684f60d4ce21818cb768d8f2a4f/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f70db590050>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-13 23:41:43 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-13 23:41:55 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 23:41:55 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 23:41:55 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 23:41:55 [scrapy.extensions.telnet] INFO: Telnet Password: 5340443e87d77ad2
2025-02-13 23:41:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 23:41:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 23:41:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 23:41:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 23:41:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 23:41:55 [scrapy.core.engine] INFO: Spider opened
2025-02-13 23:41:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 23:41:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-13 23:42:01 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 23:42:45 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 23:42:45 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-13T18-41-55+00-00.json
2025-02-13 23:42:45 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-13T18-41-55+00-00.csv
2025-02-13 23:42:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2342,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41453,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 50.057291,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 18, 42, 45, 766926, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74546,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73195520,
 'memusage/startup': 73195520,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 18, 41, 55, 709635, tzinfo=datetime.timezone.utc)}
2025-02-13 23:42:45 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 23:44:18 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 23:44:18 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 23:44:18 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 23:44:18 [scrapy.extensions.telnet] INFO: Telnet Password: f34938e56eb1983b
2025-02-13 23:44:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 23:44:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 23:44:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 23:44:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 23:44:18 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 23:44:18 [scrapy.core.engine] INFO: Spider opened
2025-02-13 23:44:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 23:44:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-13 23:44:23 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 23:45:07 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 23:45:07 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-13T18-44-18+00-00.json
2025-02-13 23:45:07 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-13T18-44-18+00-00.csv
2025-02-13 23:45:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2342,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41443,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 48.314645,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 18, 45, 7, 119671, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74538,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73080832,
 'memusage/startup': 73080832,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 18, 44, 18, 805026, tzinfo=datetime.timezone.utc)}
2025-02-13 23:45:07 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-13 23:45:50 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-13 23:45:50 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-13 23:45:50 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-13 23:45:50 [scrapy.extensions.telnet] INFO: Telnet Password: 3b2f48b88f278b76
2025-02-13 23:45:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-13 23:45:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-13 23:45:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-13 23:45:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-13 23:45:50 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-13 23:45:50 [scrapy.core.engine] INFO: Spider opened
2025-02-13 23:45:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-13 23:45:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-13 23:45:56 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-13 23:46:43 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-13 23:46:43 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-13T18-45-50+00-00.json
2025-02-13 23:46:43 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-13T18-45-50+00-00.csv
2025-02-13 23:46:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2342,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41363,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 53.050768,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 18, 46, 43, 905703, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74425,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73125888,
 'memusage/startup': 73125888,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 18, 45, 50, 854935, tzinfo=datetime.timezone.utc)}
2025-02-13 23:46:43 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 00:08:44 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 00:08:44 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 00:08:44 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 00:08:44 [scrapy.extensions.telnet] INFO: Telnet Password: 4918fad4e9ecbba5
2025-02-14 00:08:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 00:08:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 00:08:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 00:08:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 00:08:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 00:08:44 [scrapy.core.engine] INFO: Spider opened
2025-02-14 00:08:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 00:08:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 00:08:49 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 00:09:48 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 12 items (at 12 items/min)
2025-02-14 00:10:42 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 00:10:42 [scrapy.extensions.feedexport] INFO: Stored json feed (28 items) in: data/google_jobs_2025-02-13T19-08-44+00-00.json
2025-02-14 00:10:42 [scrapy.extensions.feedexport] INFO: Stored csv feed (28 items) in: data/google_jobs_2025-02-13T19-08-44+00-00.csv
2025-02-14 00:10:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2009,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41227,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 117.723387,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 19, 10, 42, 34602, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74673,
 'httpcompression/response_count': 1,
 'item_scraped_count': 28,
 'items_per_minute': None,
 'log_count/INFO': 14,
 'memusage/max': 119066624,
 'memusage/startup': 73162752,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 19, 8, 44, 311215, tzinfo=datetime.timezone.utc)}
2025-02-14 00:10:42 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 00:19:57 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 00:19:57 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 00:19:57 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 00:19:57 [scrapy.extensions.telnet] INFO: Telnet Password: 7d5232a60fe60100
2025-02-14 00:19:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 00:19:57 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 00:19:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 00:19:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 00:19:57 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 00:19:57 [scrapy.core.engine] INFO: Spider opened
2025-02-14 00:19:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 00:19:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 00:20:03 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 00:20:10 [google_jobs] INFO: No load more button
2025-02-14 00:20:23 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 00:20:23 [scrapy.extensions.feedexport] INFO: Stored json feed (3 items) in: data/google_jobs_2025-02-13T19-19-57+00-00.json
2025-02-14 00:20:23 [scrapy.extensions.feedexport] INFO: Stored csv feed (3 items) in: data/google_jobs_2025-02-13T19-19-57+00-00.csv
2025-02-14 00:20:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2007,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41720,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 26.011463,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 19, 20, 23, 451738, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74771,
 'httpcompression/response_count': 1,
 'item_scraped_count': 3,
 'items_per_minute': None,
 'log_count/INFO': 14,
 'memusage/max': 73179136,
 'memusage/startup': 73179136,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 19, 19, 57, 440275, tzinfo=datetime.timezone.utc)}
2025-02-14 00:20:23 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 00:21:34 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 00:21:34 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 00:21:34 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 00:21:34 [scrapy.extensions.telnet] INFO: Telnet Password: 47b8c1bc45099034
2025-02-14 00:21:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 00:21:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 00:21:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 00:21:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 00:21:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 00:21:34 [scrapy.core.engine] INFO: Spider opened
2025-02-14 00:21:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 00:21:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 00:21:39 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 00:21:45 [google_jobs] INFO: No load more button
2025-02-14 00:21:49 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 00:21:50 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-14 00:21:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a9bf023bc20>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e58ae1621678065f2c497d05c32c8caf/element
2025-02-14 00:21:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a9bf026c230>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e58ae1621678065f2c497d05c32c8caf/element
2025-02-14 00:21:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a9bf026c740>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e58ae1621678065f2c497d05c32c8caf/element
2025-02-14 00:21:50 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=39063): Max retries exceeded with url: /session/e58ae1621678065f2c497d05c32c8caf/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a9bf026c9b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:21:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a9bf026cce0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e58ae1621678065f2c497d05c32c8caf/elements
2025-02-14 00:21:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a9bf026cec0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e58ae1621678065f2c497d05c32c8caf/elements
2025-02-14 00:21:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a9bf026d100>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e58ae1621678065f2c497d05c32c8caf/elements
2025-02-14 00:21:50 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=39063): Max retries exceeded with url: /session/e58ae1621678065f2c497d05c32c8caf/elements (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a9bf026d340>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:21:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a9bf026c530>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e58ae1621678065f2c497d05c32c8caf/elements
2025-02-14 00:21:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a9bf026c9b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e58ae1621678065f2c497d05c32c8caf/elements
2025-02-14 00:21:50 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a9bf026c7a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e58ae1621678065f2c497d05c32c8caf/elements
2025-02-14 00:21:50 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=39063): Max retries exceeded with url: /session/e58ae1621678065f2c497d05c32c8caf/elements (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a9bf026c500>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:21:50 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-14 00:24:51 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 00:24:51 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 00:24:51 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 00:24:51 [scrapy.extensions.telnet] INFO: Telnet Password: 9fe2db547767b38b
2025-02-14 00:24:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 00:24:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 00:24:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 00:24:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 00:24:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 00:24:51 [scrapy.core.engine] INFO: Spider opened
2025-02-14 00:24:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 00:24:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 00:24:56 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 00:25:37 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 00:25:37 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-13T19-24-51+00-00.json
2025-02-14 00:25:37 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-13T19-24-51+00-00.csv
2025-02-14 00:25:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2007,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41100,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 46.604775,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 19, 25, 37, 975714, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74509,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73031680,
 'memusage/startup': 73031680,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 19, 24, 51, 370939, tzinfo=datetime.timezone.utc)}
2025-02-14 00:25:37 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 00:34:29 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 00:34:29 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 00:34:29 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 00:34:29 [scrapy.extensions.telnet] INFO: Telnet Password: 7e08f02821ad4362
2025-02-14 00:34:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 00:34:29 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 00:34:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 00:34:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 00:34:29 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 00:34:29 [scrapy.core.engine] INFO: Spider opened
2025-02-14 00:34:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 00:34:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 00:34:35 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 00:34:46 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 00:34:46 [google_jobs] ERROR: Error extracting job details: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2025-02-14 00:34:46 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c6b22ae0440>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fa943d7f84913ac80d21718c1a6e8fa5/element/f.AF4F2636D3B2D5159C69DE3718A31663.d.8CC762DA9F4E7F3B2BA592496078B0DE.e.76/click
2025-02-14 00:34:46 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c6b22ae01a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fa943d7f84913ac80d21718c1a6e8fa5/element/f.AF4F2636D3B2D5159C69DE3718A31663.d.8CC762DA9F4E7F3B2BA592496078B0DE.e.76/click
2025-02-14 00:34:46 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c6b22ae0b30>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fa943d7f84913ac80d21718c1a6e8fa5/element/f.AF4F2636D3B2D5159C69DE3718A31663.d.8CC762DA9F4E7F3B2BA592496078B0DE.e.76/click
2025-02-14 00:34:46 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=57155): Max retries exceeded with url: /session/fa943d7f84913ac80d21718c1a6e8fa5/element/f.AF4F2636D3B2D5159C69DE3718A31663.d.8CC762DA9F4E7F3B2BA592496078B0DE.e.76/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c6b22ae0da0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:34:46 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c6b22ae1190>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fa943d7f84913ac80d21718c1a6e8fa5/element/f.AF4F2636D3B2D5159C69DE3718A31663.d.8CC762DA9F4E7F3B2BA592496078B0DE.e.49/click
2025-02-14 00:34:46 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c6b22ae13a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fa943d7f84913ac80d21718c1a6e8fa5/element/f.AF4F2636D3B2D5159C69DE3718A31663.d.8CC762DA9F4E7F3B2BA592496078B0DE.e.49/click
2025-02-14 00:34:46 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c6b22ae1610>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fa943d7f84913ac80d21718c1a6e8fa5/element/f.AF4F2636D3B2D5159C69DE3718A31663.d.8CC762DA9F4E7F3B2BA592496078B0DE.e.49/click
2025-02-14 00:34:46 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=57155): Max retries exceeded with url: /session/fa943d7f84913ac80d21718c1a6e8fa5/element/f.AF4F2636D3B2D5159C69DE3718A31663.d.8CC762DA9F4E7F3B2BA592496078B0DE.e.49/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c6b22ae1880>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:34:46 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-14 00:34:46 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-13T19-34-29+00-00.json
2025-02-14 00:34:46 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-13T19-34-29+00-00.csv
2025-02-14 00:34:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2009,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41204,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 17.476764,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2025, 2, 13, 19, 34, 46, 840865, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74664,
 'httpcompression/response_count': 1,
 'items_per_minute': None,
 'log_count/ERROR': 3,
 'log_count/INFO': 14,
 'log_count/WARNING': 6,
 'memusage/max': 73113600,
 'memusage/startup': 73113600,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 19, 34, 29, 364101, tzinfo=datetime.timezone.utc)}
2025-02-14 00:34:46 [scrapy.core.engine] INFO: Spider closed (shutdown)
2025-02-14 00:35:30 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 00:35:30 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 00:35:30 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 00:35:30 [scrapy.extensions.telnet] INFO: Telnet Password: eadce213b46abe27
2025-02-14 00:35:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 00:35:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 00:35:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 00:35:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 00:35:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 00:35:30 [scrapy.core.engine] INFO: Spider opened
2025-02-14 00:35:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 00:35:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 00:35:35 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 00:35:58 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 00:35:58 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4876b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.189/element
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a8890>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.189/element
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a8aa0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.189/element
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.189/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a8ce0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9070>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.160/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9280>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.160/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a94f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.160/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.160/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a8650>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a8c50>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.161/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a8920>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.161/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9640>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.161/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.161/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9880>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9c70>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.162/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9e80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.162/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa0f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.162/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.162/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa360>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa750>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.163/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa960>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.163/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aabd0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.163/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.163/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aae40>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4ab230>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.164/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9bb0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.164/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aae70>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.164/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.164/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aac60>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa7e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.165/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9df0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.165/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa360>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.165/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.165/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa150>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9cd0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.166/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a8cb0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.166/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9970>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.166/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.166/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a85f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a8d70>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.167/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4ab3b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.167/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4ab530>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.167/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.167/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4ab7a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4abb90>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.168/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a8ad0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.168/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4ab890>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.168/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.168/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4ab500>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a8b30>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.169/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a8f50>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.169/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a88f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.169/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.169/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a97f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9d90>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.170/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa2a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.170/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa000>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.170/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.170/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa2d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa960>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.171/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aac60>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.171/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aaf30>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.171/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.171/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4abd10>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e0050>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.172/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aad80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.172/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4abce0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.172/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.172/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aae10>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa4b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.173/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa150>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.173/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9df0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.173/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.173/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa1e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a8500>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.146/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a8d40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.146/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9880>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.146/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.146/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9820>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a84d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.174/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4ab500>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.174/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4ab800>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.174/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.174/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e0170>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a8cb0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.175/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9910>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.175/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a85f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.175/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.175/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa990>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa240>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.176/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9df0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.176/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa0c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.176/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.176/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aaae0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aad20>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.177/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aaed0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.177/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aae70>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.177/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.177/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e0320>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e0560>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.178/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e0770>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.178/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e09e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.178/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.178/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e0c50>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aad80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.179/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4abc80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.179/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9a60>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.179/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.179/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa150>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa0c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.180/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa330>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.180/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa9c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.180/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.180/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9670>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4abad0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.181/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9790>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.181/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a8b00>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.181/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.181/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e00b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e10d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.182/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e12e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.182/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e1550>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.182/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.182/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9910>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a8a40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.148/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4abd40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.148/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4a9640>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.148/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.148/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa9c0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa0c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.183/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aaea0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.183/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa060>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.183/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.183/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4abef0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa720>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.184/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e0320>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.184/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e0da0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.184/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.184/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e1730>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e1a30>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.185/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e1c40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.185/click
2025-02-14 00:36:00 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4e1eb0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.185/click
2025-02-14 00:36:00 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=50797): Max retries exceeded with url: /session/52d05b3139cfbdcee9a749c0239f4343/element/f.E7F89B7D201D3D7D1B1337ED36E15A6D.d.35504AE385D0BD909533B79E8B33AB22.e.185/click (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a826e4aa2a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 00:36:00 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-14 00:36:10 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 00:36:10 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 00:36:10 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 00:36:10 [scrapy.extensions.telnet] INFO: Telnet Password: e079e1d17c605afb
2025-02-14 00:36:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 00:36:10 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 00:36:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 00:36:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 00:36:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 00:36:11 [scrapy.core.engine] INFO: Spider opened
2025-02-14 00:36:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 00:36:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 00:36:16 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 00:37:16 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 8 items (at 8 items/min)
2025-02-14 00:37:27 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 00:37:27 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-13T19-36-11+00-00.json
2025-02-14 00:37:27 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-13T19-36-11+00-00.csv
2025-02-14 00:37:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2006,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41199,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 76.675603,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 19, 37, 27, 699074, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74671,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 14,
 'memusage/max': 119205888,
 'memusage/startup': 73166848,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 19, 36, 11, 23471, tzinfo=datetime.timezone.utc)}
2025-02-14 00:37:27 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 00:39:29 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 00:39:29 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 00:39:29 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 00:39:29 [scrapy.extensions.telnet] INFO: Telnet Password: 3949078f4ab1cf5c
2025-02-14 00:39:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 00:39:29 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 00:39:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 00:39:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 00:39:29 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 00:39:29 [scrapy.core.engine] INFO: Spider opened
2025-02-14 00:39:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 00:39:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 00:39:34 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 00:40:19 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 00:40:19 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-13T19-39-29+00-00.json
2025-02-14 00:40:19 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-13T19-39-29+00-00.csv
2025-02-14 00:40:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2008,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41094,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 49.983312,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 19, 40, 19, 193457, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74492,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 72937472,
 'memusage/startup': 72937472,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 19, 39, 29, 210145, tzinfo=datetime.timezone.utc)}
2025-02-14 00:40:19 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 00:46:17 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 00:46:17 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 00:46:17 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 00:46:17 [scrapy.extensions.telnet] INFO: Telnet Password: ef517cf60944c670
2025-02-14 00:46:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 00:46:17 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 00:46:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 00:46:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 00:46:17 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 00:46:17 [scrapy.core.engine] INFO: Spider opened
2025-02-14 00:46:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 00:46:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 00:46:22 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 00:47:22 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 8 items (at 8 items/min)
2025-02-14 00:47:33 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 00:47:33 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-13T19-46-17+00-00.json
2025-02-14 00:47:33 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-13T19-46-17+00-00.csv
2025-02-14 00:47:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2008,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41287,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 76.627637,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 19, 47, 33, 795225, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74772,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 14,
 'memusage/max': 119029760,
 'memusage/startup': 73043968,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 19, 46, 17, 167588, tzinfo=datetime.timezone.utc)}
2025-02-14 00:47:33 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 01:09:34 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 01:09:34 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 01:09:34 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 01:09:34 [scrapy.extensions.telnet] INFO: Telnet Password: 8e948d23b2372aff
2025-02-14 01:09:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 01:09:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 01:09:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 01:09:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 01:09:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 01:09:34 [scrapy.core.engine] INFO: Spider opened
2025-02-14 01:09:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 01:09:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 01:09:39 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 01:10:36 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 6 items (at 6 items/min)
2025-02-14 01:10:59 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 01:10:59 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-13T20-09-34+00-00.json
2025-02-14 01:10:59 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-13T20-09-34+00-00.csv
2025-02-14 01:10:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2006,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41577,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 84.826853,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 20, 10, 59, 621988, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74549,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 14,
 'memusage/max': 119160832,
 'memusage/startup': 73056256,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 20, 9, 34, 795135, tzinfo=datetime.timezone.utc)}
2025-02-14 01:10:59 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 01:19:50 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 01:19:50 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 01:19:50 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 01:19:50 [scrapy.extensions.telnet] INFO: Telnet Password: 0e760c2cdc988df0
2025-02-14 01:19:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 01:19:50 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 01:19:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 01:19:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 01:19:50 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 01:19:50 [scrapy.core.engine] INFO: Spider opened
2025-02-14 01:19:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 01:19:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 01:19:54 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 01:20:31 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 01:20:31 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-14 01:20:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x709a8b9eb470>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fd265d4cf32d533952103044b3cc116c/element
2025-02-14 01:20:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x709a8ba10bc0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fd265d4cf32d533952103044b3cc116c/element
2025-02-14 01:20:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x709a8ba10e00>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fd265d4cf32d533952103044b3cc116c/element
2025-02-14 01:20:33 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=45835): Max retries exceeded with url: /session/fd265d4cf32d533952103044b3cc116c/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x709a8ba11040>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 01:20:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x709a8ba11310>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fd265d4cf32d533952103044b3cc116c/elements
2025-02-14 01:20:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x709a8ba114f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fd265d4cf32d533952103044b3cc116c/elements
2025-02-14 01:20:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x709a8ba11730>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fd265d4cf32d533952103044b3cc116c/elements
2025-02-14 01:20:33 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=45835): Max retries exceeded with url: /session/fd265d4cf32d533952103044b3cc116c/elements (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x709a8ba11970>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 01:20:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x709a8ba11ca0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fd265d4cf32d533952103044b3cc116c/elements
2025-02-14 01:20:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x709a8ba11e80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fd265d4cf32d533952103044b3cc116c/elements
2025-02-14 01:20:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x709a8ba120c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/fd265d4cf32d533952103044b3cc116c/elements
2025-02-14 01:20:33 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=45835): Max retries exceeded with url: /session/fd265d4cf32d533952103044b3cc116c/elements (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x709a8ba12300>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 01:20:33 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-14 01:22:38 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 01:22:38 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 01:22:38 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 01:22:38 [scrapy.extensions.telnet] INFO: Telnet Password: fb199b8d1a5d910e
2025-02-14 01:22:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 01:22:38 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 01:22:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 01:22:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 01:22:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 01:22:38 [scrapy.core.engine] INFO: Spider opened
2025-02-14 01:22:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 01:22:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 01:22:43 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 01:23:36 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 01:23:36 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-13T20-22-38+00-00.json
2025-02-14 01:23:36 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-13T20-22-38+00-00.csv
2025-02-14 01:23:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2006,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41044,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 58.566103,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 20, 23, 36, 788614, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74456,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73162752,
 'memusage/startup': 73162752,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 20, 22, 38, 222511, tzinfo=datetime.timezone.utc)}
2025-02-14 01:23:36 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 01:26:32 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 01:26:32 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 01:26:32 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 01:26:32 [scrapy.extensions.telnet] INFO: Telnet Password: 2c389241da21d620
2025-02-14 01:26:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 01:26:32 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 01:26:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 01:26:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 01:26:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 01:26:32 [scrapy.core.engine] INFO: Spider opened
2025-02-14 01:26:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 01:26:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 01:26:36 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 01:27:25 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 01:27:25 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-13T20-26-32+00-00.json
2025-02-14 01:27:25 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-13T20-26-32+00-00.csv
2025-02-14 01:27:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2009,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41141,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 53.048397,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 20, 27, 25, 179982, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74569,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73273344,
 'memusage/startup': 73273344,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 20, 26, 32, 131585, tzinfo=datetime.timezone.utc)}
2025-02-14 01:27:25 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 01:28:51 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 01:28:51 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 01:28:51 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 01:28:51 [scrapy.extensions.telnet] INFO: Telnet Password: 0d6122f42dd23abd
2025-02-14 01:28:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 01:28:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 01:28:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 01:28:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 01:28:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 01:28:51 [scrapy.core.engine] INFO: Spider opened
2025-02-14 01:28:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 01:28:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 01:28:55 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 01:29:14 [google_jobs] ERROR: Error extracting job details: 'list' object has no attribute 'get_attribute'
2025-02-14 01:29:19 [google_jobs] ERROR: Error extracting job details: 'list' object has no attribute 'get_attribute'
2025-02-14 01:29:23 [google_jobs] ERROR: Error extracting job details: 'list' object has no attribute 'get_attribute'
2025-02-14 01:29:26 [google_jobs] ERROR: Error extracting job details: 'list' object has no attribute 'get_attribute'
2025-02-14 01:29:29 [google_jobs] ERROR: Error extracting job details: 'list' object has no attribute 'get_attribute'
2025-02-14 01:29:33 [google_jobs] ERROR: Error extracting job details: 'list' object has no attribute 'get_attribute'
2025-02-14 01:29:36 [google_jobs] ERROR: Error extracting job details: 'list' object has no attribute 'get_attribute'
2025-02-14 01:29:40 [google_jobs] ERROR: Error extracting job details: 'list' object has no attribute 'get_attribute'
2025-02-14 01:29:43 [google_jobs] ERROR: Error extracting job details: 'list' object has no attribute 'get_attribute'
2025-02-14 01:29:46 [google_jobs] ERROR: Error extracting job details: 'list' object has no attribute 'get_attribute'
2025-02-14 01:29:46 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 01:29:46 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-13T20-28-51+00-00.json
2025-02-14 01:29:46 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-13T20-28-51+00-00.csv
2025-02-14 01:29:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2007,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41696,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 54.936193,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 20, 29, 46, 521042, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74735,
 'httpcompression/response_count': 1,
 'items_per_minute': None,
 'log_count/ERROR': 10,
 'log_count/INFO': 13,
 'memusage/max': 72704000,
 'memusage/startup': 72704000,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 20, 28, 51, 584849, tzinfo=datetime.timezone.utc)}
2025-02-14 01:29:46 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 01:30:17 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 01:30:17 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 01:30:17 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 01:30:17 [scrapy.extensions.telnet] INFO: Telnet Password: 02a83f5ce83fa344
2025-02-14 01:30:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 01:30:17 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 01:30:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 01:30:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 01:30:17 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 01:30:17 [scrapy.core.engine] INFO: Spider opened
2025-02-14 01:30:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 01:30:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 01:30:23 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 01:31:14 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 01:31:14 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-13T20-30-17+00-00.json
2025-02-14 01:31:14 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-13T20-30-17+00-00.csv
2025-02-14 01:31:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2008,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41462,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 56.28471,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 20, 31, 14, 161721, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74417,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73039872,
 'memusage/startup': 73039872,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 20, 30, 17, 877011, tzinfo=datetime.timezone.utc)}
2025-02-14 01:31:14 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 01:42:57 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 01:42:57 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 01:42:57 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 01:42:57 [scrapy.extensions.telnet] INFO: Telnet Password: d565d7c180187bdb
2025-02-14 01:42:57 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 01:42:57 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 01:42:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 01:42:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 01:42:57 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 01:42:57 [scrapy.core.engine] INFO: Spider opened
2025-02-14 01:42:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 01:42:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 01:43:02 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 01:43:49 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 01:43:49 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-13T20-42-57+00-00.json
2025-02-14 01:43:49 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-13T20-42-57+00-00.csv
2025-02-14 01:43:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2007,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 41005,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'elapsed_time_seconds': 51.967745,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 13, 20, 43, 49, 364858, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 74396,
 'httpcompression/response_count': 1,
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73543680,
 'memusage/startup': 73543680,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2025, 2, 13, 20, 42, 57, 397113, tzinfo=datetime.timezone.utc)}
2025-02-14 01:43:49 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 18:28:51 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 18:28:51 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 18:28:51 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 18:28:51 [scrapy.extensions.telnet] INFO: Telnet Password: d2ee5bc974a79a7c
2025-02-14 18:28:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 18:28:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 18:28:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 18:28:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 18:28:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 18:28:51 [scrapy.core.engine] INFO: Spider opened
2025-02-14 18:28:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 18:28:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 18:28:55 [root] WARNING: Received 429 Too Many Requests. Retrying in 17.49 seconds...
2025-02-14 18:29:13 [root] WARNING: Received 429 Too Many Requests. Retrying in 16.31 seconds...
2025-02-14 18:29:16 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 18:29:16 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-14 18:29:29 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-14 18:30:24 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 18:30:24 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 18:30:24 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 18:30:24 [scrapy.extensions.telnet] INFO: Telnet Password: 2f996d5f6b04d836
2025-02-14 18:30:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 18:30:24 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 18:30:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 18:30:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 18:30:24 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 18:30:24 [scrapy.core.engine] INFO: Spider opened
2025-02-14 18:30:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 18:30:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 18:30:29 [root] WARNING: Received 429 Too Many Requests. Retrying in 10.02 seconds...
2025-02-14 18:30:39 [root] WARNING: Received 429 Too Many Requests. Retrying in 10.86 seconds...
2025-02-14 18:30:42 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 18:30:42 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-14 18:30:50 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-14 18:34:55 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 18:34:55 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 18:34:55 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 18:34:55 [scrapy.extensions.telnet] INFO: Telnet Password: a3db6566abd53bd1
2025-02-14 18:34:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 18:34:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 18:34:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 18:34:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 18:34:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 18:34:55 [scrapy.core.engine] INFO: Spider opened
2025-02-14 18:34:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 18:34:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 18:35:00 [root] WARNING: Received 429 Too Many Requests. Retrying in 18.52 seconds...
2025-02-14 18:35:07 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 18:35:07 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-14 18:35:18 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-14 18:38:07 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 18:38:07 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 18:38:07 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 18:38:07 [scrapy.extensions.telnet] INFO: Telnet Password: f11c60217c146409
2025-02-14 18:38:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 18:38:07 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 18:38:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 18:38:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 18:38:07 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 18:38:07 [scrapy.core.engine] INFO: Spider opened
2025-02-14 18:38:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 18:38:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 18:38:12 [root] WARNING: Received 429 Too Many Requests. Retrying in 14.43 seconds...
2025-02-14 18:38:26 [root] WARNING: Received 429 Too Many Requests. Retrying in 13.34 seconds...
2025-02-14 18:38:40 [root] WARNING: Received 429 Too Many Requests. Retrying in 13.00 seconds...
2025-02-14 18:38:54 [root] WARNING: Received 429 Too Many Requests. Retrying in 15.58 seconds...
2025-02-14 18:39:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 18:39:09 [root] WARNING: Received 429 Too Many Requests. Retrying in 10.88 seconds...
2025-02-14 18:39:20 [root] WARNING: Received 429 Too Many Requests. Retrying in 13.08 seconds...
2025-02-14 18:39:33 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3D%2522django%2520developer%2522%252C%2520%2522karachi%2522%26sca_esv%3D35c83397f18a25ed%26hl%3Den%26source%3Dhp%26ei%3DQEGuZ6WlMbiikdUPiMHMyAs%26iflsig%3DACkRmUkAAAAAZ65PUHFIHzpy83qFu_lo1aiN2beWvWkN%26ved%3D2ahUKEwiF4NSYrMGLAxVoVaQEHdQYCR4Q3L8LegQIIhAN%26uact%3D5%26oq%3D%2522django%2520developer%2522%252C%2520%2522karachi%2522%26gs_lp%3DEgdnd3Mtd2l6Ih0iZGphbmdvIGRldmVsb3BlciIsICJrYXJhY2hpIjIGEAAYCBgeMgYQABgIGB4yCxAAGIAEGIYDGIoFMgsQABiABBiGAxiKBTILEAAYgAQYhgMYigUyCxAAGIAEGIYDGIoFMggQABiABBiiBDIIEAAYgAQYogQyBRAAGO8FMggQABiABBiiBEiLrRhQAFi1qhhwCXgAkAEBmAHZAqABgzqqAQkwLjE3LjE3LjG4AQPIAQD4AQGYAg2gAvcQwgIGEAAYBxgewgIOEC4Y0QMYBxjHARgKGB7CAggQLhiABBixA8ICCBAAGIAEGLEDwgILEAAYgAQYsQMYgwHCAgUQABiABMICBRAuGIAEmAMAkgcFMy42LjSgB9rAAQ%26sclient%3Dgws-wiz%26jbr%3Dsep:0%26udm%3D8&hl=en&q=EgR3SWCJGMCOvb0GIjCxN9qkl8zDc_JsANZIlwNipc7drTYx-rWNyjBc5IjpM-2kcu_gMEaOfWTwLSrT1zgyAXJaAUM> (failed 6 times): 429 Unknown Status
2025-02-14 18:39:34 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://www.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3D%2522django%2520developer%2522%252C%2520%2522karachi%2522%26sca_esv%3D35c83397f18a25ed%26hl%3Den%26source%3Dhp%26ei%3DQEGuZ6WlMbiikdUPiMHMyAs%26iflsig%3DACkRmUkAAAAAZ65PUHFIHzpy83qFu_lo1aiN2beWvWkN%26ved%3D2ahUKEwiF4NSYrMGLAxVoVaQEHdQYCR4Q3L8LegQIIhAN%26uact%3D5%26oq%3D%2522django%2520developer%2522%252C%2520%2522karachi%2522%26gs_lp%3DEgdnd3Mtd2l6Ih0iZGphbmdvIGRldmVsb3BlciIsICJrYXJhY2hpIjIGEAAYCBgeMgYQABgIGB4yCxAAGIAEGIYDGIoFMgsQABiABBiGAxiKBTILEAAYgAQYhgMYigUyCxAAGIAEGIYDGIoFMggQABiABBiiBDIIEAAYgAQYogQyBRAAGO8FMggQABiABBiiBEiLrRhQAFi1qhhwCXgAkAEBmAHZAqABgzqqAQkwLjE3LjE3LjG4AQPIAQD4AQGYAg2gAvcQwgIGEAAYBxgewgIOEC4Y0QMYBxjHARgKGB7CAggQLhiABBixA8ICCBAAGIAEGLEDwgILEAAYgAQYsQMYgwHCAgUQABiABMICBRAuGIAEmAMAkgcFMy42LjSgB9rAAQ%26sclient%3Dgws-wiz%26jbr%3Dsep:0%26udm%3D8&hl=en&q=EgR3SWCJGMCOvb0GIjCxN9qkl8zDc_JsANZIlwNipc7drTYx-rWNyjBc5IjpM-2kcu_gMEaOfWTwLSrT1zgyAXJaAUM>: HTTP status code is not handled or not allowed
2025-02-14 18:39:34 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 18:39:34 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-14T13-38-07+00-00.json
2025-02-14 18:39:34 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-14T13-38-07+00-00.csv
2025-02-14 18:39:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 9139,
 'downloader/request_count': 7,
 'downloader/request_method_count/GET': 7,
 'downloader/response_bytes': 37327,
 'downloader/response_count': 7,
 'downloader/response_status_count/302': 1,
 'downloader/response_status_count/429': 6,
 'elapsed_time_seconds': 86.35409,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 14, 13, 39, 34, 37882, tzinfo=datetime.timezone.utc),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/429': 1,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 14,
 'log_count/WARNING': 6,
 'memusage/max': 80850944,
 'memusage/startup': 73240576,
 'response_received_count': 1,
 'responses_per_minute': None,
 'retry/count': 5,
 'retry/max_reached': 1,
 'retry/reason_count/429 Unknown Status': 5,
 'scheduler/dequeued': 7,
 'scheduler/dequeued/memory': 7,
 'scheduler/enqueued': 7,
 'scheduler/enqueued/memory': 7,
 'start_time': datetime.datetime(2025, 2, 14, 13, 38, 7, 683792, tzinfo=datetime.timezone.utc)}
2025-02-14 18:39:34 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 18:41:43 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 18:41:43 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 18:41:43 [root] INFO: Initializing ChromeDriver...
2025-02-14 18:41:45 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 18:41:47 [root] INFO: ChromeDriver initialized successfully.
2025-02-14 18:41:47 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 18:41:47 [scrapy.extensions.telnet] INFO: Telnet Password: fe6d4a92ad1d0c63
2025-02-14 18:41:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 18:41:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 18:41:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 18:41:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 18:41:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 18:41:47 [scrapy.core.engine] INFO: Spider opened
2025-02-14 18:41:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 18:41:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 18:41:51 [root] WARNING: Received 429 Too Many Requests. Retrying in 15.93 seconds...
2025-02-14 18:42:07 [root] WARNING: Received 429 Too Many Requests. Retrying in 19.46 seconds...
2025-02-14 18:42:24 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 18:42:24 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-14 18:42:27 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-14 18:43:49 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 18:43:49 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 18:43:49 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 18:43:49 [scrapy.extensions.telnet] INFO: Telnet Password: 4882033df56b0600
2025-02-14 18:43:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.throttle.AutoThrottle']
2025-02-14 18:43:49 [scrapy.crawler] INFO: Overridden settings:
{'AUTOTHROTTLE_ENABLED': True,
 'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 7.779295018932621,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 18:43:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 18:43:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 18:43:49 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 18:43:49 [scrapy.core.engine] INFO: Spider opened
2025-02-14 18:43:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 18:43:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 18:44:00 [root] WARNING: Received 429 Too Many Requests. Retrying in 19.86 seconds...
2025-02-14 18:44:04 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 18:44:04 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-14 18:44:20 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-14 18:52:56 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 18:52:56 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 18:52:56 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 18:52:56 [scrapy.extensions.telnet] INFO: Telnet Password: e37b6704ca2e5df9
2025-02-14 18:52:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.throttle.AutoThrottle']
2025-02-14 18:52:56 [scrapy.crawler] INFO: Overridden settings:
{'AUTOTHROTTLE_ENABLED': True,
 'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 5.8027928240603135,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 18:52:57 [scrapy_fake_useragent.middleware] INFO: Error loading User-Agent provider: scrapy_fake_useragent.providers.FakeUserAgentProvider
2025-02-14 18:52:57 [scrapy_fake_useragent.middleware] INFO: Unable to load any of the User-Agent providers
2025-02-14 18:52:57 [scrapy_fake_useragent.middleware] INFO: Using '<class 'scrapy_fake_useragent.providers.FixedUserAgentProvider'>' as the User-Agent provider
2025-02-14 18:52:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 18:52:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 18:52:57 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 18:52:57 [scrapy.core.engine] INFO: Spider opened
2025-02-14 18:52:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 18:52:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 18:53:02 [root] WARNING: Received 429 Too Many Requests. Retrying in 19.40 seconds...
2025-02-14 18:53:15 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 18:53:16 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-14 18:53:22 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-14 18:55:03 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 18:55:03 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 18:55:04 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 18:55:05 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 18:55:05 [scrapy.extensions.telnet] INFO: Telnet Password: 787f0f2e8746a17d
2025-02-14 18:55:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.throttle.AutoThrottle']
2025-02-14 18:55:05 [scrapy.crawler] INFO: Overridden settings:
{'AUTOTHROTTLE_ENABLED': True,
 'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 1,
 'DOWNLOAD_DELAY': 9.295532094883288,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 18:55:05 [scrapy_fake_useragent.middleware] INFO: Error loading User-Agent provider: scrapy_fake_useragent.providers.FakeUserAgentProvider
2025-02-14 18:55:05 [scrapy_fake_useragent.middleware] INFO: Unable to load any of the User-Agent providers
2025-02-14 18:55:05 [scrapy_fake_useragent.middleware] INFO: Using '<class 'scrapy_fake_useragent.providers.FixedUserAgentProvider'>' as the User-Agent provider
2025-02-14 18:55:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 18:55:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 18:55:05 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 18:55:05 [scrapy.core.engine] INFO: Spider opened
2025-02-14 18:55:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 18:55:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 18:55:13 [root] WARNING: Received 429 Too Many Requests. Retrying in 13.07 seconds...
2025-02-14 18:55:23 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 18:55:23 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-14 18:55:26 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-14 19:12:29 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 19:12:29 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 19:12:29 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 19:12:29 [scrapy.extensions.telnet] INFO: Telnet Password: a51aa19d8a0933d4
2025-02-14 19:12:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 19:12:29 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 19:12:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 19:12:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 19:12:29 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 19:12:29 [scrapy.core.engine] INFO: Spider opened
2025-02-14 19:12:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 19:12:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 19:12:30 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 19:12:36 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 58, in start_requests
    yield from self.parse(response)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 71, in parse
    self.driver.get(response.url)
    ^^^^^^^^^^^
AttributeError: 'GoogleJobsSpider' object has no attribute 'driver'
2025-02-14 19:12:36 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 19:12:36 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-14T14-12-29+00-00.json
2025-02-14 19:12:36 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-14T14-12-29+00-00.csv
2025-02-14 19:12:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 7.635178,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 14, 14, 12, 36, 765985, tzinfo=datetime.timezone.utc),
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 13,
 'memusage/max': 72970240,
 'memusage/startup': 72970240,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 14, 14, 12, 29, 130807, tzinfo=datetime.timezone.utc)}
2025-02-14 19:12:36 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 19:13:55 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 19:13:55 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 19:13:55 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 19:13:55 [scrapy.extensions.telnet] INFO: Telnet Password: 705b2510b8e2dba8
2025-02-14 19:13:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 19:13:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 19:13:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 19:13:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 19:13:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 19:13:55 [scrapy.core.engine] INFO: Spider opened
2025-02-14 19:13:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 19:13:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 19:14:00 [root] WARNING: Received 429 Too Many Requests. Retrying in 16.75 seconds...
2025-02-14 19:14:03 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 19:14:04 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-14 19:14:17 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-14 19:15:14 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 19:15:14 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 19:15:14 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 19:15:14 [scrapy.extensions.telnet] INFO: Telnet Password: 56baf58479787c5a
2025-02-14 19:15:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 19:15:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 19:15:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 19:15:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 19:15:14 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 19:15:14 [scrapy.core.engine] INFO: Spider opened
2025-02-14 19:15:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 19:15:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 19:15:15 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 19:15:22 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 19:15:28 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 58, in start_requests
    yield from self.parse(response)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 74, in parse
    location_not_now = driver.find_element(By.XPATH, '//div[@class="mpQYc"]//div[@class="sjVJQd"]')
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 888, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 429, in execute
    self.error_handler.check_response(response)
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py", line 232, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {"method":"xpath","selector":"//div[@class="mpQYc"]//div[@class="sjVJQd"]"}
  (Session info: chrome=133.0.6943.53); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception
Stacktrace:
#0 0x5d847fe4dbba <unknown>
#1 0x5d847f8eb790 <unknown>
#2 0x5d847f93cc80 <unknown>
#3 0x5d847f93ce01 <unknown>
#4 0x5d847f98b944 <unknown>
#5 0x5d847f962a7d <unknown>
#6 0x5d847f988ccc <unknown>
#7 0x5d847f962823 <unknown>
#8 0x5d847f92ea88 <unknown>
#9 0x5d847f92fbf1 <unknown>
#10 0x5d847fe1715b <unknown>
#11 0x5d847fe1b0e2 <unknown>
#12 0x5d847fe0401c <unknown>
#13 0x5d847fe1bcd4 <unknown>
#14 0x5d847fde848f <unknown>
#15 0x5d847fe3c4f8 <unknown>
#16 0x5d847fe3c6c9 <unknown>
#17 0x5d847fe4ca36 <unknown>
#18 0x7f8b7de9caa4 <unknown>
#19 0x7f8b7df29c3c <unknown>

2025-02-14 19:15:28 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 19:15:28 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-14T14-15-14+00-00.json
2025-02-14 19:15:28 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-14T14-15-14+00-00.csv
2025-02-14 19:15:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 14.394852,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 14, 14, 15, 28, 823155, tzinfo=datetime.timezone.utc),
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 14,
 'memusage/max': 73211904,
 'memusage/startup': 73211904,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 14, 14, 15, 14, 428303, tzinfo=datetime.timezone.utc)}
2025-02-14 19:15:28 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 19:18:37 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 19:18:37 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 19:18:37 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 19:18:37 [scrapy.extensions.telnet] INFO: Telnet Password: 7a9eea039261cc08
2025-02-14 19:18:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 19:18:37 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 19:18:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 19:18:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 19:18:37 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 19:18:37 [scrapy.core.engine] INFO: Spider opened
2025-02-14 19:18:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 19:18:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 19:18:38 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 19:18:45 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 19:18:57 [google_jobs] INFO: No location prompt found.
2025-02-14 19:19:07 [google_jobs] INFO: No 'Load more jobs' button found.
2025-02-14 19:19:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 4 items (at 4 items/min)
2025-02-14 19:20:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 14 items (at 10 items/min)
2025-02-14 19:21:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 25 items (at 11 items/min)
2025-02-14 19:22:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 35 items (at 10 items/min)
2025-02-14 19:23:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 45 items (at 10 items/min)
2025-02-14 19:24:06 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 19:24:06 [scrapy.extensions.feedexport] INFO: Stored json feed (49 items) in: data/google_jobs_2025-02-14T14-18-37+00-00.json
2025-02-14 19:24:06 [scrapy.extensions.feedexport] INFO: Stored csv feed (49 items) in: data/google_jobs_2025-02-14T14-18-37+00-00.csv
2025-02-14 19:24:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 328.190023,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 14, 14, 24, 6, 19513, tzinfo=datetime.timezone.utc),
 'item_scraped_count': 49,
 'items_per_minute': None,
 'log_count/INFO': 21,
 'memusage/max': 117592064,
 'memusage/startup': 73031680,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 14, 14, 18, 37, 829490, tzinfo=datetime.timezone.utc)}
2025-02-14 19:24:06 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 19:27:21 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 19:27:21 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 19:27:21 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 19:27:21 [scrapy.extensions.telnet] INFO: Telnet Password: 8a4a9e7d0029e369
2025-02-14 19:27:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 19:27:21 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 19:27:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 19:27:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 19:27:21 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 19:27:21 [scrapy.core.engine] INFO: Spider opened
2025-02-14 19:27:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 19:27:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 19:27:22 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 19:27:26 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 19:27:27 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-14 19:27:27 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x798132d85130>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/3bcc22ccb4cb6823e59a4c5b98186d3a/source
2025-02-14 19:27:27 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x798132fc5520>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/3bcc22ccb4cb6823e59a4c5b98186d3a/source
2025-02-14 19:27:27 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x798132d84fb0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/3bcc22ccb4cb6823e59a4c5b98186d3a/source
2025-02-14 19:27:27 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x798132d84950>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 194, in start_requests
    search_page_html = driver.page_source
                       ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=52709): Max retries exceeded with url: /session/3bcc22ccb4cb6823e59a4c5b98186d3a/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x798132d84950>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 19:27:28 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 19:27:28 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-14T14-27-21+00-00.json
2025-02-14 19:27:28 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-14T14-27-21+00-00.csv
2025-02-14 19:27:43 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 19:27:43 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 19:27:43 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 19:27:43 [scrapy.extensions.telnet] INFO: Telnet Password: 4860f1e8623f10ef
2025-02-14 19:27:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 19:27:43 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 19:27:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 19:27:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 19:27:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 19:27:43 [scrapy.core.engine] INFO: Spider opened
2025-02-14 19:27:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 19:27:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 19:27:44 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 19:27:51 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 19:28:02 [google_jobs] INFO: No location prompt found.
2025-02-14 19:28:04 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 19:28:04 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-14 19:28:06 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e13f4275be0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e02bf4768924b93ce7573349ba09caa5/element
2025-02-14 19:28:06 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e13f4274b60>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e02bf4768924b93ce7573349ba09caa5/element
2025-02-14 19:28:06 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e13f4275fd0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e02bf4768924b93ce7573349ba09caa5/element
2025-02-14 19:28:06 [google_jobs] INFO: No 'Load more jobs' button found.
2025-02-14 19:28:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e13f4276090>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e02bf4768924b93ce7573349ba09caa5/element
2025-02-14 19:28:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e13f4276540>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e02bf4768924b93ce7573349ba09caa5/element
2025-02-14 19:28:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e13f42768a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e02bf4768924b93ce7573349ba09caa5/element
2025-02-14 19:28:09 [google_jobs] INFO: Timeout: No jobs found.
2025-02-14 19:28:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e13f4277800>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e02bf4768924b93ce7573349ba09caa5/elements
2025-02-14 19:28:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e13f4277aa0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e02bf4768924b93ce7573349ba09caa5/elements
2025-02-14 19:28:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e13f4277c80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e02bf4768924b93ce7573349ba09caa5/elements
2025-02-14 19:28:09 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7e13f4274cb0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 203, in start_requests
    yield from self.parse(response)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 256, in parse
    job_items = driver.find_elements(By.XPATH, '//a[@class="MQUd2b"]')
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 926, in find_elements
    return self.execute(Command.FIND_ELEMENTS, {"using": by, "value": value})["value"] or []
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=49859): Max retries exceeded with url: /session/e02bf4768924b93ce7573349ba09caa5/elements (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e13f4274cb0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 19:28:09 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 19:28:09 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-14T14-27-43+00-00.json
2025-02-14 19:28:09 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-14T14-27-43+00-00.csv
2025-02-14 19:30:38 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 19:30:38 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 19:30:38 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 19:30:38 [scrapy.extensions.telnet] INFO: Telnet Password: 426253c325d4b965
2025-02-14 19:30:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 19:30:38 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 19:30:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 19:30:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 19:30:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 19:30:38 [scrapy.core.engine] INFO: Spider opened
2025-02-14 19:30:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 19:30:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 19:30:42 [root] WARNING: Received 429 Too Many Requests. Retrying in 19.95 seconds...
2025-02-14 19:30:46 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 19:30:46 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-14 19:31:02 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-14 19:35:55 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 19:35:55 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 19:35:55 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 19:35:55 [scrapy.extensions.telnet] INFO: Telnet Password: 159a7ad67fde0b27
2025-02-14 19:35:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 19:35:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 19:35:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 19:35:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 19:35:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 19:35:55 [scrapy.core.engine] INFO: Spider opened
2025-02-14 19:35:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 19:35:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 19:36:07 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3D%2522django%2520developer%2522%252C%2520%2522karachi%2522%26sca_esv%3D35c83397f18a25ed%26hl%3Den%26source%3Dhp%26ei%3DQEGuZ6WlMbiikdUPiMHMyAs%26iflsig%3DACkRmUkAAAAAZ65PUHFIHzpy83qFu_lo1aiN2beWvWkN%26ved%3D2ahUKEwiF4NSYrMGLAxVoVaQEHdQYCR4Q3L8LegQIIhAN%26uact%3D5%26oq%3D%2522django%2520developer%2522%252C%2520%2522karachi%2522%26gs_lp%3DEgdnd3Mtd2l6Ih0iZGphbmdvIGRldmVsb3BlciIsICJrYXJhY2hpIjIGEAAYCBgeMgYQABgIGB4yCxAAGIAEGIYDGIoFMgsQABiABBiGAxiKBTILEAAYgAQYhgMYigUyCxAAGIAEGIYDGIoFMggQABiABBiiBDIIEAAYgAQYogQyBRAAGO8FMggQABiABBiiBEiLrRhQAFi1qhhwCXgAkAEBmAHZAqABgzqqAQkwLjE3LjE3LjG4AQPIAQD4AQGYAg2gAvcQwgIGEAAYBxgewgIOEC4Y0QMYBxjHARgKGB7CAggQLhiABBixA8ICCBAAGIAEGLEDwgILEAAYgAQYsQMYgwHCAgUQABiABMICBRAuGIAEmAMAkgcFMy42LjSgB9rAAQ%26sclient%3Dgws-wiz%26jbr%3Dsep:0%26udm%3D8&hl=en&q=EgR3SWCJGMypvb0GIjD63PwSc_6zOr6goOZ0eMMDn8OvDpg0bBXQbpEidEkjuyyTCE7zhqk87E6k_HdGIy0yAXJaAUM> (failed 3 times): 429 Unknown Status
2025-02-14 19:36:07 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <429 https://www.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3D%2522django%2520developer%2522%252C%2520%2522karachi%2522%26sca_esv%3D35c83397f18a25ed%26hl%3Den%26source%3Dhp%26ei%3DQEGuZ6WlMbiikdUPiMHMyAs%26iflsig%3DACkRmUkAAAAAZ65PUHFIHzpy83qFu_lo1aiN2beWvWkN%26ved%3D2ahUKEwiF4NSYrMGLAxVoVaQEHdQYCR4Q3L8LegQIIhAN%26uact%3D5%26oq%3D%2522django%2520developer%2522%252C%2520%2522karachi%2522%26gs_lp%3DEgdnd3Mtd2l6Ih0iZGphbmdvIGRldmVsb3BlciIsICJrYXJhY2hpIjIGEAAYCBgeMgYQABgIGB4yCxAAGIAEGIYDGIoFMgsQABiABBiGAxiKBTILEAAYgAQYhgMYigUyCxAAGIAEGIYDGIoFMggQABiABBiiBDIIEAAYgAQYogQyBRAAGO8FMggQABiABBiiBEiLrRhQAFi1qhhwCXgAkAEBmAHZAqABgzqqAQkwLjE3LjE3LjG4AQPIAQD4AQGYAg2gAvcQwgIGEAAYBxgewgIOEC4Y0QMYBxjHARgKGB7CAggQLhiABBixA8ICCBAAGIAEGLEDwgILEAAYgAQYsQMYgwHCAgUQABiABMICBRAuGIAEmAMAkgcFMy42LjSgB9rAAQ%26sclient%3Dgws-wiz%26jbr%3Dsep:0%26udm%3D8&hl=en&q=EgR3SWCJGMypvb0GIjD63PwSc_6zOr6goOZ0eMMDn8OvDpg0bBXQbpEidEkjuyyTCE7zhqk87E6k_HdGIy0yAXJaAUM>: HTTP status code is not handled or not allowed
2025-02-14 19:36:07 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 19:36:07 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-14T14-35-55+00-00.json
2025-02-14 19:36:07 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-14T14-35-55+00-00.csv
2025-02-14 19:36:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5002,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 20256,
 'downloader/response_count': 4,
 'downloader/response_status_count/302': 1,
 'downloader/response_status_count/429': 3,
 'elapsed_time_seconds': 11.324154,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 14, 14, 36, 7, 173521, tzinfo=datetime.timezone.utc),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/429': 1,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 13,
 'memusage/max': 73097216,
 'memusage/startup': 73097216,
 'response_received_count': 1,
 'responses_per_minute': None,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/429 Unknown Status': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2025, 2, 14, 14, 35, 55, 849367, tzinfo=datetime.timezone.utc)}
2025-02-14 19:36:07 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 19:37:26 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 19:37:26 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 19:37:26 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 19:37:26 [scrapy.extensions.telnet] INFO: Telnet Password: 1b48871d83298650
2025-02-14 19:37:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 19:37:26 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 19:37:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 19:37:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 19:37:26 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 19:37:26 [twisted] CRITICAL: Unhandled error in Deferred:
2025-02-14 19:37:26 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/twisted/internet/defer.py", line 2017, in _inlineCallbacks
    result = context.run(gen.send, result)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/crawler.py", line 153, in crawl
    start_requests = iter(self.spider.start_requests())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 34, in start_requests
    self.parse()
TypeError: GoogleJobsSpider.parse() missing 1 required positional argument: 'response'
2025-02-14 19:52:14 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 19:52:14 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 19:52:14 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 19:52:14 [scrapy.extensions.telnet] INFO: Telnet Password: 2a5d6343caa954ae
2025-02-14 19:52:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 19:52:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 19:52:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 19:52:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 19:52:14 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 19:52:14 [scrapy.core.engine] INFO: Spider opened
2025-02-14 19:52:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 19:52:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 19:52:15 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 19:52:23 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 19:52:30 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 48, in start_requests
    yield from self.parse(response)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 65, in parse
    location_not_now = driver.find_element(By.XPATH, '//div[@class="mpQYc"]//div[@class="sjVJQd"]')
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 888, in find_element
    return self.execute(Command.FIND_ELEMENT, {"using": by, "value": value})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 429, in execute
    self.error_handler.check_response(response)
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py", line 232, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {"method":"xpath","selector":"//div[@class="mpQYc"]//div[@class="sjVJQd"]"}
  (Session info: chrome=133.0.6943.53); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception
Stacktrace:
#0 0x5d17a98e0bba <unknown>
#1 0x5d17a937e790 <unknown>
#2 0x5d17a93cfc80 <unknown>
#3 0x5d17a93cfe01 <unknown>
#4 0x5d17a941e944 <unknown>
#5 0x5d17a93f5a7d <unknown>
#6 0x5d17a941bccc <unknown>
#7 0x5d17a93f5823 <unknown>
#8 0x5d17a93c1a88 <unknown>
#9 0x5d17a93c2bf1 <unknown>
#10 0x5d17a98aa15b <unknown>
#11 0x5d17a98ae0e2 <unknown>
#12 0x5d17a989701c <unknown>
#13 0x5d17a98aecd4 <unknown>
#14 0x5d17a987b48f <unknown>
#15 0x5d17a98cf4f8 <unknown>
#16 0x5d17a98cf6c9 <unknown>
#17 0x5d17a98dfa36 <unknown>
#18 0x7a0999e9caa4 <unknown>
#19 0x7a0999f29c3c <unknown>

2025-02-14 19:52:30 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 19:52:30 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-14T14-52-14+00-00.json
2025-02-14 19:52:30 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-14T14-52-14+00-00.csv
2025-02-14 19:52:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 16.260458,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 14, 14, 52, 30, 458401, tzinfo=datetime.timezone.utc),
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 14,
 'memusage/max': 73195520,
 'memusage/startup': 73195520,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 14, 14, 52, 14, 197943, tzinfo=datetime.timezone.utc)}
2025-02-14 19:52:30 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 19:54:26 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 19:54:26 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 19:54:26 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 19:54:26 [scrapy.extensions.telnet] INFO: Telnet Password: ce7f8317f9172816
2025-02-14 19:54:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 19:54:26 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 19:54:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 19:54:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 19:54:26 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 19:54:26 [scrapy.core.engine] INFO: Spider opened
2025-02-14 19:54:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 19:54:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 19:54:27 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 19:54:34 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 19:54:40 [google_jobs] INFO: No location not now button
2025-02-14 19:54:43 [google_jobs] INFO: No load more button
2025-02-14 19:55:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 8 items (at 8 items/min)
2025-02-14 19:56:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 20 items (at 12 items/min)
2025-02-14 19:57:12 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 19:57:12 [scrapy.extensions.feedexport] INFO: Stored json feed (29 items) in: data/google_jobs_2025-02-14T14-54-26+00-00.json
2025-02-14 19:57:12 [scrapy.extensions.feedexport] INFO: Stored csv feed (29 items) in: data/google_jobs_2025-02-14T14-54-26+00-00.csv
2025-02-14 19:57:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 165.312407,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 14, 14, 57, 12, 4430, tzinfo=datetime.timezone.utc),
 'item_scraped_count': 29,
 'items_per_minute': None,
 'log_count/INFO': 18,
 'memusage/max': 117456896,
 'memusage/startup': 73158656,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 14, 14, 54, 26, 692023, tzinfo=datetime.timezone.utc)}
2025-02-14 19:57:12 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 20:18:21 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 20:18:21 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 20:18:21 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 20:18:21 [scrapy.extensions.telnet] INFO: Telnet Password: 5c874a9bd7bb4edc
2025-02-14 20:18:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 20:18:21 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 20:18:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 20:18:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 20:18:21 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 20:18:21 [scrapy.core.engine] INFO: Spider opened
2025-02-14 20:18:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 20:18:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 20:18:21 [scrapy.core.scraper] ERROR: Error downloading <GET https://www.google.com/search?q=%22django%20developer%22%2C%20%22karachi%22&sca_esv=35c83397f18a25ed&hl=en&source=hp&ei=QEGuZ6WlMbiikdUPiMHMyAs&iflsig=ACkRmUkAAAAAZ65PUHFIHzpy83qFu_lo1aiN2beWvWkN&ved=2ahUKEwiF4NSYrMGLAxVoVaQEHdQYCR4Q3L8LegQIIhAN&uact=5&oq=%22django%20developer%22%2C%20%22karachi%22&gs_lp=Egdnd3Mtd2l6Ih0iZGphbmdvIGRldmVsb3BlciIsICJrYXJhY2hpIjIGEAAYCBgeMgYQABgIGB4yCxAAGIAEGIYDGIoFMgsQABiABBiGAxiKBTILEAAYgAQYhgMYigUyCxAAGIAEGIYDGIoFMggQABiABBiiBDIIEAAYgAQYogQyBRAAGO8FMggQABiABBiiBEiLrRhQAFi1qhhwCXgAkAEBmAHZAqABgzqqAQkwLjE3LjE3LjG4AQPIAQD4AQGYAg2gAvcQwgIGEAAYBxgewgIOEC4Y0QMYBxjHARgKGB7CAggQLhiABBixA8ICCBAAGIAEGLEDwgILEAAYgAQYsQMYgwHCAgUQABiABMICBRAuGIAEmAMAkgcFMy42LjSgB9rAAQ&sclient=gws-wiz&jbr=sep:0&udm=8>
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/twisted/internet/defer.py", line 2013, in _inlineCallbacks
    result = context.run(
             ^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/twisted/python/failure.py", line 467, in throwExceptionIntoGenerator
    return g.throw(self.value.with_traceback(self.tb))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/downloader/middleware.py", line 68, in process_request
    return (yield download_func(request, spider))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 104, in mustbe_deferred
    result = f(*args, **kw)
             ^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/downloader/handlers/__init__.py", line 102, in download_request
    raise NotSupported(
scrapy.exceptions.NotSupported: Unsupported URL scheme 'https': no handler available for that scheme
2025-02-14 20:18:21 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 20:18:21 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-14T15-18-21+00-00.json
2025-02-14 20:18:21 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-14T15-18-21+00-00.csv
2025-02-14 20:18:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/scrapy.exceptions.NotSupported': 1,
 'downloader/request_bytes': 871,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'elapsed_time_seconds': 0.319621,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 14, 15, 18, 21, 535266, tzinfo=datetime.timezone.utc),
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 12,
 'memusage/max': 73142272,
 'memusage/startup': 73142272,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 14, 15, 18, 21, 215645, tzinfo=datetime.timezone.utc)}
2025-02-14 20:18:21 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 20:23:32 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 20:23:32 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 20:23:32 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 20:23:32 [scrapy.extensions.telnet] INFO: Telnet Password: 1768725aeab57083
2025-02-14 20:23:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 20:23:32 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 20:23:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 20:23:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 20:23:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 20:23:32 [scrapy.core.engine] INFO: Spider opened
2025-02-14 20:23:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 20:23:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 20:23:36 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 20:23:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:23:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:23:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:23:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:23:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:23:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:23:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:23:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:23:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:23:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:23:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:23:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:23:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:23:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:23:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:23:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:23:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:23:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:23:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:23:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:23:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:23:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:23:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:23:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:23:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:23:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:23:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:23:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:23:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:23:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:23:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:23:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:23:58 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 20:24:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:24:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:24:35 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2025-02-14 20:24:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:24:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:24:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:24:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:24:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:24:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:24:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:24:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:24:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:24:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:24:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:24:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:24:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:24:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:24:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:24:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:24:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:24:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:24:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:24:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:24:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:24:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:24:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:24:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:24:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:24:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:24:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:24:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:24:42 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 20:25:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:25:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:25:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:25:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:25:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:25:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:25:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:25:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:25:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:25:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:25:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:25:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:25:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:25:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:25:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:25:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:25:29 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 20:26:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:26:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:26:21 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 2 pages/min), scraped 3 items (at 2 items/min)
2025-02-14 20:26:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:26:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:26:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:26:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:26:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:26:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:26:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:26:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:26:26 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 20:27:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:27:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:27:15 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 1 pages/min), scraped 4 items (at 1 items/min)
2025-02-14 20:27:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:27:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:27:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:27:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:27:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:27:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:27:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:27:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:27:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:27:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:27:21 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 20:28:16 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 20:28:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:28:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:28:57 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 3 pages/min), scraped 6 items (at 2 items/min)
2025-02-14 20:29:06 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 20:29:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:29:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:29:57 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 7 items (at 1 items/min)
2025-02-14 20:30:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:32 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 7 items (at 0 items/min)
2025-02-14 20:30:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:30:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:30:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:32 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 7 items (at 0 items/min)
2025-02-14 20:31:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:31:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:31:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:32 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 7 items (at 0 items/min)
2025-02-14 20:32:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:32:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:32:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:32 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 7 items (at 0 items/min)
2025-02-14 20:33:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:33:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:33:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:32 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 7 items (at 0 items/min)
2025-02-14 20:34:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:34:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:34:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:32 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 7 items (at 0 items/min)
2025-02-14 20:35:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:35:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:35:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:32 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 7 items (at 0 items/min)
2025-02-14 20:36:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:36:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:36:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-14 20:37:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:08 [linkedin_job] INFO: Total unique job URLs collected so far: 10
2025-02-14 20:37:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:08 [linkedin_job] INFO: Total unique job URLs collected so far: 11
2025-02-14 20:37:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:08 [linkedin_job] INFO: Total unique job URLs collected so far: 17
2025-02-14 20:37:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:09 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:09 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:09 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:09 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:10 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:10 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:10 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:10 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:10 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:11 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:11 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:11 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:11 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:11 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:11 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:12 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:12 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:12 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:12 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:37:12 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:37:13 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 20:38:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:38:04 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:38:04 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 1 pages/min), scraped 8 items (at 1 items/min)
2025-02-14 20:38:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:38:09 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:38:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:38:09 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:38:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:38:09 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:38:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:38:09 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:38:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:38:09 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:38:10 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 20:39:00 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 1 pages/min), scraped 9 items (at 1 items/min)
2025-02-14 20:39:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:39:03 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:39:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:39:03 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:39:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:39:03 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:39:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:39:04 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:39:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:39:04 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:39:05 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 20:39:44 [scrapy.extensions.logstats] INFO: Crawled 11 pages (at 1 pages/min), scraped 10 items (at 1 items/min)
2025-02-14 20:39:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:39:48 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:39:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:39:48 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:39:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:39:49 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:39:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:39:49 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:39:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:39:49 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:39:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:39:49 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:39:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:39:49 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:39:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:39:49 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:39:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:39:50 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:39:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:39:50 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:39:51 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 20:40:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:40:26 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:40:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:40:30 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:40:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:40:30 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 20:40:31 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 20:41:11 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 20:41:11 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-14 20:41:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79315d28a930>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/bd7b6c242a3b71bebc3fbcc37da31d94/source
2025-02-14 20:41:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79315d28a3f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/bd7b6c242a3b71bebc3fbcc37da31d94/source
2025-02-14 20:41:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79315d28a0f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/bd7b6c242a3b71bebc3fbcc37da31d94/source
2025-02-14 20:41:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4153113501/?eBP=CwEAAAGVBQ8asnxcfzlUS_-akiQpJfjpD0-iuRim34RAINLJ15tUeFoOoVMBLZ9leS5b1jXsTkerjpXEvDhjixFwOH9bLNMP2bMFoDl7Ha_hvDVlK6iQLO3b7n583Pu_Gy3RilFHjosWw02_vknvMT5xLIAZrs9oaMtduKGIV9wXrwmdxFzXELjB_xaPwCika_artjQAnYDTso4d1ZIvKtuQkaLy6pLHZdGPi9Qwf1qZtgTpLAphpn6T6EwnMyGOOtOfeoeJSDZwaQBY-T6Pdu_cRd92x2WgpXdjAzSlLM0mmCf-8WmSlaqCvhJuvrB6imGTxg3ssxuIVRfNgNWuSVwuItGxQskrM6P1Up2rZF2VnRbIT1JkBT76o0xaawTMtVp0SVFsZgEzg2ymQY37x9H3yASP8UXCmdvtwLB44JcF6Ec0EveOVR6ppk43xnpKm-w_74wzQi9eOvSiuXof6w&refId=rqz8gokMTS29Bnh4yMX23w%3D%3D&trackingId=m8RK1AKvWoZN7rvYUN1vYQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x79315d28a5a0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 176, in parse_job_page
    job_page_html = driver.page_source
                    ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56335): Max retries exceeded with url: /session/bd7b6c242a3b71bebc3fbcc37da31d94/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79315d28a5a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 20:41:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 20:41:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79315db6fc80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/6127ab91f4275086d6103b4c4d4c3d01/element
2025-02-14 20:41:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79315d174710>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/6127ab91f4275086d6103b4c4d4c3d01/element
2025-02-14 20:41:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79315d174950>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/6127ab91f4275086d6103b4c4d4c3d01/element
2025-02-14 20:41:13 [linkedin_job] INFO: Timeout: No job links found within 15 seconds.
2025-02-14 20:41:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79315d174d40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/6127ab91f4275086d6103b4c4d4c3d01/source
2025-02-14 20:41:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79315d174ef0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/6127ab91f4275086d6103b4c4d4c3d01/source
2025-02-14 20:41:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79315d175070>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/6127ab91f4275086d6103b4c4d4c3d01/source
2025-02-14 20:41:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x79315d288d10>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 82, in parse
    search_page_html = driver.page_source
                       ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=42831): Max retries exceeded with url: /session/6127ab91f4275086d6103b4c4d4c3d01/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79315d288d10>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 20:41:13 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 3 pages/min), scraped 11 items (at 1 items/min)
2025-02-14 20:41:13 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-14 20:41:13 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.linkedin.com/jobs/view/4122797356/?eBP=CwEAAAGVBQ8asQFLsNx4W0igeEBhIiMD8jZ06J4YRUtVVJpuzE0N4JouCJ7SP5MaJuNeL7Tulo4Gbb9nyF4Cc4dxUYysgYJfHQs8p3dUhyF2qFmPKFPd3QJHLv6l-YMjOrOg__1z4iut3obGz2mUCMVBJyIuZcT1rtsyDT5TTS4co6uGTrtUPz7RL9dpdpIlF2x-Rxg6hA6Evo7ibJjHwgnomMFkJq3luuJMdtCGgll6bBTBH16o9Bu5U0nDOhlLPTPfiq6XK2g0oicubBxLexFOqS4hpAgtH8l2fyCgx2tLADFOqt4Qjx4hxSkxRbLxwJT-m17sa0iqUphvowUvAVj3jR36ys3GWgpnoo5TNpceeetdHOPR4RZJ04t2uAxeFDa878xl2Rj5Bri9lA8UEFgckBRQ--C2JFcjvMKRawVLGtVF7CEcXgoLCqamTxfOWtD_wFG90A3SH9znqnZ9rQ&refId=rqz8gokMTS29Bnh4yMX23w%3D%3D&trackingId=dRDSNGBbRpfxFp%2Fx2u8LsQ%3D%3D&trk=flagship3_search_srp_jobs. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2025-02-14 20:41:13 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.linkedin.com/jobs/view/4152267529/?eBP=CwEAAAGVBQ8asQZk2Tz9RXLNKqxzshECCcuIBFAiukproNExZanjumkDnThoahMbzMTidZ4Ym4lN2NFvAeaEZnmXdeEJNG_pzyvaEwxHU0sn0d_HLvJ23yY6zphp9_g_dm6MNBm7SWOU3Gv6v7GwjCRjv_QHadC8MJ4SkNkMvklSWggyv6_oDU1FwtGIGpA1dtNgY0fCJRcR0ExTaspn3qlQ7rCOgOt2P3Oim-aXWfbJIadhoVlqxchxwArYsDcsgGkT3SJJV8YrVBJYoDLhQQli1QCEFF5EEYkp46HToeMD2Icm9HFaEcMndZYete6ZfcwEG6Pfr8vClBQnjS1CivZ4KZul59x9m99o5Fbc8NLQoaLOTwF5l0NhUvLHvA2fC11m_afl55YLTwQF6E0dqH8f6P4XlTKQgNkHvKenxs5KEzypxiiywdYZCOBMPcU6CsUFSwKgcSN9gbY6H7E&refId=rqz8gokMTS29Bnh4yMX23w%3D%3D&trackingId=P1DSxmO7c5dC8p79%2FeSVBA%3D%3D&trk=flagship3_search_srp_jobs. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2025-02-14 20:41:13 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.linkedin.com/jobs/view/4143251646/?eBP=CwEAAAGVBQ8ascK2w6NaZ0WsCirogFVIU2yHs7mqwY6Wego9LHbxIy9VKTFmnWDRIl5S7rm52nRu1LnsEtbAqkl285hHChAt5TvmP4ZFWgi-QFE8KC4-Ecl_Awtj5W4_dX_LOAAjYS4IiIfNCjTTWBNDE8atK-J8Tq7snRWdpNmAqNLTBnCTXh6oBLB2hnjbVllGE1Tw4wjdtg51FqTAJ9_joPsPQaef_S8NVIG5DP7ZE8jLqDYT5SGrQLJb5yWwe1XdbHf3AvGD2vNLAbRK2WTCGhs1RFqR8R-UDPUG44aF3dUpFYoW0W-VQecRr7YvKxLkyK3u99K8o8UWDOcbtwj6_T7RoEC-vMvvZf303urPJQ5DkK7uWoZYYCEgIGXe2pkyYi81bt6sJDrgbHf9zu4GzhPFTOc0eIg16YiOXtOXK56cHkNnV60TRYhvV6eS_q7tlsRKPX-pajcBeXbfQP9pB0CFDLGGPApzBm3PshzKT7L6FG3Q4z_v1tZY8-KuRA&refId=rqz8gokMTS29Bnh4yMX23w%3D%3D&trackingId=VrnWJSELJMGksgPZgcGZ5A%3D%3D&trk=flagship3_search_srp_jobs. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2025-02-14 20:41:13 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.linkedin.com/jobs/view/4153120028/?eBP=CwEAAAGVBQ8asg7wbwyLJ-7BeFtlA0qQKYTfp46p8YB2-zG1Xal0Sqf0G2K4lU6uIYfBIsX3xEgb_sYi0JdPorOa_6HzTciMTB3Ha6v5tfHLwbLImeFIR-FwrMhn8otBtlN50_DdjF1qSCx6o-GcnqKpuf9a_T-sAF7644ca0eRE1xjTMkgXkd0IFIKnnf0twf7LDeW1T-MuL0dhjrOWoBNPYd2jjLxGzftjRCtGdCxtup74V-P-Oz-Mo6LsSGUykLXqb7YtO71ue3O57h7_Kkln2ebr3KyJbzCEVEFfw_oZ9TZb7PUkAxqzyZBYlcMYdPbJuXXCz7ajA2EF5IHvlolvc53zxSADcQjzkmza9sC9q0vb4JBLLI4Swg0zIgLNPtdcgIqpx11oPoxFpWrKHUqu7r99PYauI8I66NFWymsUn9iKa8p-KdPMzpsyBbz2PgXrslNOmkLXSHG4Ytp_rw&refId=rqz8gokMTS29Bnh4yMX23w%3D%3D&trackingId=PsbMg%2B8wLnapC8yiTXAC4g%3D%3D&trk=flagship3_search_srp_jobs. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2025-02-14 21:38:47 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 21:38:47 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 21:38:47 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 21:38:47 [scrapy.extensions.telnet] INFO: Telnet Password: 12ba913fe873090c
2025-02-14 21:38:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 21:38:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 21:38:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 21:38:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 21:38:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 21:38:47 [scrapy.core.engine] INFO: Spider opened
2025-02-14 21:38:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 21:38:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 21:38:52 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 21:39:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-14 21:40:25 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-14 21:40:25 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 21:40:30 [linkedin_job] INFO: Extracting job listings from page 2
2025-02-14 21:41:28 [linkedin_job] INFO: Total unique job URLs collected so far: 50
2025-02-14 21:41:28 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 21:41:35 [linkedin_job] INFO: Extracting job listings from page 3
2025-02-14 21:42:28 [linkedin_job] INFO: Total unique job URLs collected so far: 75
2025-02-14 21:42:28 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 21:42:33 [linkedin_job] INFO: Extracting job listings from page 4
2025-02-14 21:43:35 [linkedin_job] INFO: Total unique job URLs collected so far: 100
2025-02-14 21:43:35 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 21:43:41 [linkedin_job] INFO: Extracting job listings from page 5
2025-02-14 21:44:39 [linkedin_job] INFO: Total unique job URLs collected so far: 125
2025-02-14 21:44:39 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 21:44:43 [linkedin_job] INFO: Extracting job listings from page 6
2025-02-14 21:45:39 [linkedin_job] INFO: Total unique job URLs collected so far: 150
2025-02-14 21:45:39 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 21:45:44 [linkedin_job] INFO: Extracting job listings from page 7
2025-02-14 21:45:51 [linkedin_job] INFO: Total unique job URLs collected so far: 153
2025-02-14 21:45:51 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 21:45:51 [linkedin_job] INFO: ✅ Final total job URLs: 153
2025-02-14 21:45:55 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 21:46:43 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 21:47:36 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 4 pages/min), scraped 2 items (at 2 items/min)
2025-02-14 21:47:42 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 21:48:26 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 21:49:06 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 0 pages/min), scraped 4 items (at 2 items/min)
2025-02-14 21:49:17 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 21:50:08 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 1 pages/min), scraped 5 items (at 1 items/min)
2025-02-14 21:50:15 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 21:51:00 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 2 pages/min), scraped 6 items (at 1 items/min)
2025-02-14 21:51:04 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.linkedin.com/jobs/view/4149448662/?eBP=CwEAAAGVBVkZ7PD52t8Ua4GmNUIsKUYVTCy3wu8cfaIvXYEQ_qKUAS5wlzwDHGvTEn2YfGZ5gWhdkOGQfc_PU5opbOYMgJ_ZJ6vqAgPItiu1Uifa6Zhe8qgVHYyulc7joddpKXwK8PRGJwATQsxvfLjR-eeHJ_f-XbdfbNsGPJzrHj5hhT1kmG58yetC6AlGTJe8HqiHgkeoh-tyOuHR_biRl4PugmrHgzuh9CoARXY3M7MIfPa3RqKXrIeDwpBMuHBIwZOO4hHjBU9i_BdkFcm0vHfNvl7q1oQuNlxviYmcefVmVVwJs3Ohbph4Q02N9cB1kBj5Hv1HLz99lGNXN1VoR8ON-usF6VPmFNByPzgBrARMKGYjkZnPRCGm1orPX04JoaIII6gOKmfNKU2uuN3coJ6V6IxnYVE6yUGwgJsz-SL8wGStV-fYzVeenrDuU5CtPErxhjPRX0NqrtcGtomHeCiQD_wNKNlUL-MMmmNA1dfSdv-1kH8YbgdkZlzp4uEKxSU4Xmlz9Ll30mr0gg&refId=HKW0CLBKUZ9FISTpXt3cCA%3D%3D&trackingId=kYG9V5c87%2BgLrk1KSzhg3w%3D%3D&trk=flagship3_search_srp_jobs. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2025-02-14 21:51:05 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 21:51:51 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 1 pages/min), scraped 7 items (at 1 items/min)
2025-02-14 21:51:56 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 21:52:49 [scrapy.extensions.logstats] INFO: Crawled 11 pages (at 2 pages/min), scraped 8 items (at 1 items/min)
2025-02-14 21:52:50 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 21:52:53 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-14 21:52:53 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-14 21:52:55 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b9b91d6a960>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/16b5750c8513b77931644505260311dc/cookie
2025-02-14 21:52:55 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b9b91d681a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/16b5750c8513b77931644505260311dc/cookie
2025-02-14 21:52:55 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b9b91d68dd0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/16b5750c8513b77931644505260311dc/cookie
2025-02-14 21:52:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4151304743/?eBP=CwEAAAGVBVgex4lzy0LO11OvCp0hqtHB9-U1MZY9v4GFUECshdbvZmU4tgntKMl8RiDmdCYtJ244NC2V-m21oR86xaXbZlgSM-grVBA_VagXmsoR747qO-qzNo5l8hSU1jeP3bjTxPTgGw3Mll9gGCS8SqBu3pGjgm1dRLbKV07Kpau7F6Hl_nsVKIlwF2BK147khC_xCM1DG4dx3R0fqQirQnZp3Jpw9L9knAZDa-8kQQJnWD-MPjWEYU-mMHWwbplMOAX7zHVdbkZbSZ0a6q13krnxsz0WZdSBDHjwD98agz1pdyWl9LQV4CSXmBtM6hHrVQZSZxYyeHqKn-1ZvlbuKFinkLLJGqeCNiYvrQmU_E7mCjcFYlIDdHUtOutV4pJ199InHfKHBR5DQcQ_-UkV_h85VQ3P0zeblwnwoiTpHZ3YRCESy4V4-HpGqW27MVrV08vi7OTEColB-co48LCtGaT3oFtzOxSiLC829E1kjORWir9PXb89IPqMWKhjNmLt4g&refId=ELCj6oEdGF%2Bp6Kbyui3rzA%3D%3D&trackingId=gQLjuUeX9IUBeHE37WFH%2Bg%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7b9b926f6b70>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 157, in parse_job_page
    driver.add_cookie({"name": name, "value": value})
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 767, in add_cookie
    self.execute(Command.ADD_COOKIE, {"cookie": cookie_dict})
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=57373): Max retries exceeded with url: /session/16b5750c8513b77931644505260311dc/cookie (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b9b926f6b70>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-14 21:52:57 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 21:53:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4150270561/?eBP=CwEAAAGVBVkZ7J10F9tlP96b37ICUzK3RvgpiV8C9khmumj3Gi6UiY1gYnqmV684-opO0hStfHi7LOSocf0eFQn0QqsIK68ELk2E-L7fuSahozyiL_Vj6I53qXbVSde8l-CPvow5eaCkQInDX4Y7EwEnX5u2_Xf24ZdmIli98oFIt7kbWhWuycJrY2TXma_lshzb3MCVMseaES7iGOrYVV-DyNhIEIspaouZ_DnHMGuAWDxipUvBJDMxePuuZEXWkO4FTOPlROYDSLwfZiu1HLLlkjpftMMxGGQVUQTMNMoY_y37mfw0iH7GT7YmQ3mmb1i3KyJfqGqLRIS85P7KorJ6IyS3NhMkHmMNMm0x603WzkEQ4FI0SaDdJD_XLh0xS4fZnhePXsL4hz3jxjKYY1pC4YNbImQkLD0ZqTZG-wSW347FvS223h_RdVCsU0aezHk9eBKNkVJkBCV4jO5__gE_9DMmdC1yVhGxBmmF5YrCFEaMewP4vjO6-vdgXg&refId=HKW0CLBKUZ9FISTpXt3cCA%3D%3D&trackingId=TM151rEv1ArZeyuNDEVqIg%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 165, in parse_job_page
    self.human_scroll(driver)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 240, in human_scroll
    driver.execute_script("window.scrollBy(0, 500);")
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 429, in execute
    self.error_handler.check_response(response)
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py", line 232, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: invalid session id: session deleted as the browser has closed the connection
from disconnected: not connected to DevTools
  (Session info: chrome=133.0.6943.53)
Stacktrace:
#0 0x57cf43098bba <unknown>
#1 0x57cf42b36790 <unknown>
#2 0x57cf42b1c31e <unknown>
#3 0x57cf42b44d79 <unknown>
#4 0x57cf42bb6389 <unknown>
#5 0x57cf42bd3639 <unknown>
#6 0x57cf42bad823 <unknown>
#7 0x57cf42b79a88 <unknown>
#8 0x57cf42b7abf1 <unknown>
#9 0x57cf4306215b <unknown>
#10 0x57cf430660e2 <unknown>
#11 0x57cf4304f01c <unknown>
#12 0x57cf43066cd4 <unknown>
#13 0x57cf4303348f <unknown>
#14 0x57cf430874f8 <unknown>
#15 0x57cf430876c9 <unknown>
#16 0x57cf43097a36 <unknown>
#17 0x798816e9caa4 <unknown>
#18 0x798816f29c3c <unknown>

2025-02-14 21:53:15 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-14 22:06:35 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 22:06:35 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 22:06:35 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 22:06:35 [scrapy.extensions.telnet] INFO: Telnet Password: c301bd0b8f1b8715
2025-02-14 22:06:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2025-02-14 22:06:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOGSTATS_INTERVAL': 0,
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 22:06:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 22:06:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 22:06:35 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 22:06:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 22:08:18 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 22:08:18 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 22:08:18 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 22:08:18 [scrapy.extensions.telnet] INFO: Telnet Password: 5b6cfe4f64d10f94
2025-02-14 22:08:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2025-02-14 22:08:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOGSTATS_INTERVAL': 0,
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 22:08:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 22:08:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 22:08:18 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 22:08:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 22:09:47 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 22:09:47 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 22:09:47 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 22:09:47 [scrapy.extensions.telnet] INFO: Telnet Password: da27ee9a7f421b07
2025-02-14 22:09:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
2025-02-14 22:09:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOGSTATS_INTERVAL': 0,
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 22:09:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 22:09:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 22:09:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 22:09:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-14 22:17:07 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 22:17:07 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 22:17:07 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 22:17:07 [scrapy.extensions.telnet] INFO: Telnet Password: 03be2f56aa805d2f
2025-02-14 22:17:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 22:17:07 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 22:17:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 22:17:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 22:17:07 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 22:17:07 [scrapy.core.engine] INFO: Spider opened
2025-02-14 22:17:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 22:17:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-14 22:17:10 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 22:17:28 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 22:17:28 [scrapy.extensions.feedexport] INFO: Stored json feed (6 items) in: data/results_linkedin_post_2025-02-14T17-17-07+00-00.json
2025-02-14 22:17:28 [scrapy.extensions.feedexport] INFO: Stored csv feed (6 items) in: data/results_linkedin_post_2025-02-14T17-17-07+00-00.csv
2025-02-14 22:17:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 464,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 161067,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 21.252732,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 14, 17, 17, 28, 384220, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1412451,
 'httpcompression/response_count': 1,
 'item_scraped_count': 6,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73228288,
 'memusage/startup': 73228288,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 14, 17, 17, 7, 131488, tzinfo=datetime.timezone.utc)}
2025-02-14 22:17:28 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 22:23:44 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 22:23:44 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 22:23:44 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 22:23:44 [scrapy.extensions.telnet] INFO: Telnet Password: 2b3c5f52c9696e2d
2025-02-14 22:23:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 22:23:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 22:23:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 22:23:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 22:23:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 22:23:44 [scrapy.core.engine] INFO: Spider opened
2025-02-14 22:23:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 22:23:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-14 22:23:47 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 22:24:08 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 22:24:08 [scrapy.extensions.feedexport] INFO: Stored json feed (6 items) in: data/results_linkedin_post_2025-02-14T17-23-44+00-00.json
2025-02-14 22:24:08 [scrapy.extensions.feedexport] INFO: Stored csv feed (6 items) in: data/results_linkedin_post_2025-02-14T17-23-44+00-00.csv
2025-02-14 22:24:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 464,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 161044,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 24.175897,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 14, 17, 24, 8, 400059, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1412391,
 'httpcompression/response_count': 1,
 'item_scraped_count': 6,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73011200,
 'memusage/startup': 73011200,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 14, 17, 23, 44, 224162, tzinfo=datetime.timezone.utc)}
2025-02-14 22:24:08 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-14 22:40:27 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-14 22:40:27 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-14 22:40:27 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-14 22:40:27 [scrapy.extensions.telnet] INFO: Telnet Password: 4810bc9e8e047210
2025-02-14 22:40:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-14 22:40:27 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-14 22:40:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-14 22:40:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-14 22:40:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-14 22:40:27 [scrapy.core.engine] INFO: Spider opened
2025-02-14 22:40:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-14 22:40:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024
2025-02-14 22:40:28 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 22:40:34 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-14 22:41:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 8 items (at 8 items/min)
2025-02-14 22:42:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 20 items (at 12 items/min)
2025-02-14 22:43:08 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-14 22:43:08 [scrapy.extensions.feedexport] INFO: Stored json feed (28 items) in: data/google_jobs_2025-02-14T17-40-27+00-00.json
2025-02-14 22:43:08 [scrapy.extensions.feedexport] INFO: Stored csv feed (28 items) in: data/google_jobs_2025-02-14T17-40-27+00-00.csv
2025-02-14 22:43:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 160.339547,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 14, 17, 43, 8, 89747, tzinfo=datetime.timezone.utc),
 'item_scraped_count': 28,
 'items_per_minute': None,
 'log_count/INFO': 16,
 'memusage/max': 118247424,
 'memusage/startup': 73285632,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 14, 17, 40, 27, 750200, tzinfo=datetime.timezone.utc)}
2025-02-14 22:43:08 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-17 18:34:51 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-17 18:34:51 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-17 18:34:51 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-17 18:34:51 [scrapy.extensions.telnet] INFO: Telnet Password: 747600fcf8fbf4a9
2025-02-17 18:34:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-17 18:34:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-17 18:34:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-17 18:34:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-17 18:34:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-17 18:34:51 [scrapy.core.engine] INFO: Spider opened
2025-02-17 18:34:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-17 18:34:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-17 18:34:54 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-17 18:35:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:35:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:35:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:35:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:35:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:35:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:35:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:35:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:35:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:35:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:35:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:35:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:35:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:35:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:35:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:35:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:35:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:35:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:35:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:35:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:35:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:35:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:35:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:35:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:35:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:35:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:35:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:35:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:35:12 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-17 18:36:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:36:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:36:03 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2025-02-17 18:36:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:36:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:36:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:36:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:36:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:36:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:36:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:36:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:36:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:36:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:36:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:36:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:36:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:36:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:36:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:36:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:36:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:36:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:36:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:36:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:36:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:36:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:36:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:36:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:36:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:36:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:36:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:36:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:36:10 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-17 18:36:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:36:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:36:57 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 2 pages/min), scraped 2 items (at 1 items/min)
2025-02-17 18:37:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:37:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:37:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:37:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:37:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:37:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:37:02 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-17 18:37:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:37:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:37:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:37:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:37:56 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 0 pages/min), scraped 3 items (at 1 items/min)
2025-02-17 18:37:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:37:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:37:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:37:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:37:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:37:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:37:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:37:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-17 18:37:57 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-17 18:38:28 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-17 18:38:28 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-17 18:38:31 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f116dff7c20>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/d88d64ad6de23e263556221ec907af4a/execute/sync
2025-02-17 18:38:31 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f116dff78c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/d88d64ad6de23e263556221ec907af4a/execute/sync
2025-02-17 18:38:31 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f116dff73b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/d88d64ad6de23e263556221ec907af4a/execute/sync
2025-02-17 18:38:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4136143237/?eBP=CwEAAAGVFB6eyt3Hg0H_1i1x73XzsNECyxvrJhj9M-bR20uHRHAiyau6joknVpOi6uEoYm3m6SPaWXUNzt_4bptLRrjoAbG-R9dwA1vtPMsT3NrUC4RQWOTcod26k71hGFhdmGrqWsWc_br8BC_1ocCttsP5JzrT0DwrrNaL3pn0NSXkbAn7OI1uaY7pc9GemZeZqTFPtYDlV53qBNKYqjyiWLqHkWH8nFY9uh_OeilUaslh6OglwQymftDTYL46_gZvwp3dytvvHJ5rOdLcQslXABFr1GmvwKVBeE-uUtfO-ZczMEsNsEDdvKkHUYucqebMRsx015QTDZMNV3oxR7Tm5SYNQ8eiJx2McOV_UMyeQHKOl7T5iHOBN25lAtY0e4sXCsrCAyAAmXgSu3q4qtWZNb_fF3OoY1BOeo5XqJxZYvg2wyWlJ_E0JRjWOQMdKnNtbrtORZPJ8mK-Zz8H8132nMR1pGpPEK4yjXqlVgPfs6VXHjnVI1OlImmGeDxf3zyLWHfTG-82KhEMMK-NQ0GgMl9SFi610jeMryOy&refId=iUsn4c0l1hQPpFxV1j2hhg%3D%3D&trackingId=Zim5woQwPfiyd0P2nzOzBA%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f116dff75f0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 165, in parse_job_page
    self.human_scroll(driver)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 240, in human_scroll
    driver.execute_script("window.scrollBy(0, 500);")
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=52119): Max retries exceeded with url: /session/d88d64ad6de23e263556221ec907af4a/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f116dff75f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-17 18:38:32 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-17 18:38:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4152738175/?eBP=CwEAAAGVFB6eyvl_e1vf9C2stfsbJksG2MEXdZTGVN7Wm-s6TfswPCuNCDxs0qyKIwz_9AjW4yxUk1WGDC7ZtOWBWd88wMVVaJ8tBYJsJzxvvfHl5LnfCaKeVSWy2btZ_B4uxi0LI2u0sz-YbLtfouPqXKsq6UN7M2tdsTwZN8uOQOEXqYS6eeu1kdF2pwrMoBMjxdqKy9cKq0-V_UYPM6tVAFHdNW2cqqczCBSRLfwgifJWgaNiJXY5qwJSLD1SQHcVwuczNwbF742EQ3oHkxoCX6zi84WYuXH6yNvdndiCzxCfpbRUIZXhSGOnW2HW-_Qoj0ZsGyc79KCHSVEBE54lI4zN9O89PDsUP_T12xU8B36HOAAglQPfi7AA2Flyx2aJqh1pyj1a8TWTqWKnRjiIqy5Y1iw5DVyLsJm56eRvDVDqn57xDgPFyJgkE3FXLEvkrbfjExCPBCQI47QbW06DY-bQok66oI0NxCArtWdgeQ&refId=iUsn4c0l1hQPpFxV1j2hhg%3D%3D&trackingId=kSK8CzoedcOet97Wbpu3tQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 161, in parse_job_page
    driver.get(response.url)
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 665, in get
    return super().get(url)
           ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 454, in get
    self.execute(Command.GET, {"url": url})
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 429, in execute
    self.error_handler.check_response(response)
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py", line 232, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: invalid session id: session deleted as the browser has closed the connection
from disconnected: not connected to DevTools
  (Session info: chrome=133.0.6943.53)
Stacktrace:
#0 0x613d0ce95bba <unknown>
#1 0x613d0c933790 <unknown>
#2 0x613d0c91931e <unknown>
#3 0x613d0c941d79 <unknown>
#4 0x613d0c9b3389 <unknown>
#5 0x613d0c9d0639 <unknown>
#6 0x613d0c9aa823 <unknown>
#7 0x613d0c976a88 <unknown>
#8 0x613d0c977bf1 <unknown>
#9 0x613d0ce5f15b <unknown>
#10 0x613d0ce630e2 <unknown>
#11 0x613d0ce4c01c <unknown>
#12 0x613d0ce63cd4 <unknown>
#13 0x613d0ce3048f <unknown>
#14 0x613d0ce844f8 <unknown>
#15 0x613d0ce846c9 <unknown>
#16 0x613d0ce94a36 <unknown>
#17 0x7a4d1069caa4 <unknown>
#18 0x7a4d10729c3c <unknown>

2025-02-17 18:38:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-17 18:38:42 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f116df47cb0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e282762aed3f1f5979fa7de2c415284b/element
2025-02-17 18:38:42 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f116dff7170>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e282762aed3f1f5979fa7de2c415284b/element
2025-02-17 18:38:42 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f116dff7c50>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e282762aed3f1f5979fa7de2c415284b/element
2025-02-17 18:38:42 [linkedin_job] INFO: Timeout: No job links found within 15 seconds.
2025-02-17 18:38:42 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f116dff74d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e282762aed3f1f5979fa7de2c415284b/source
2025-02-17 18:38:42 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f116dff73b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e282762aed3f1f5979fa7de2c415284b/source
2025-02-17 18:38:42 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f116dff7860>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e282762aed3f1f5979fa7de2c415284b/source
2025-02-17 18:38:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4138535120&geoId=101022442&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f116d639d60>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 82, in parse
    search_page_html = driver.page_source
                       ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=38477): Max retries exceeded with url: /session/e282762aed3f1f5979fa7de2c415284b/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f116d639d60>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-17 18:38:42 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-17 18:50:37 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-17 18:50:37 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-17 18:50:37 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-17 18:50:37 [scrapy.extensions.telnet] INFO: Telnet Password: 4468aad1bade462f
2025-02-17 18:50:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-17 18:50:37 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-17 18:50:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-17 18:50:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-17 18:50:37 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-17 18:50:37 [scrapy.core.engine] INFO: Spider opened
2025-02-17 18:50:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-17 18:50:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-17 18:50:42 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-17 18:50:58 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-17 18:50:58 [scrapy.extensions.feedexport] INFO: Stored json feed (5 items) in: data/linkedin_post_2025-02-17T13-50-37+00-00.json
2025-02-17 18:50:58 [scrapy.extensions.feedexport] INFO: Stored csv feed (5 items) in: data/linkedin_post_2025-02-17T13-50-37+00-00.csv
2025-02-17 18:50:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 464,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 160206,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 20.210802,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 17, 13, 50, 58, 81263, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1427254,
 'httpcompression/response_count': 1,
 'item_scraped_count': 5,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73011200,
 'memusage/startup': 73011200,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 17, 13, 50, 37, 870461, tzinfo=datetime.timezone.utc)}
2025-02-17 18:50:58 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-17 18:53:59 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-17 18:53:59 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-17 18:53:59 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-17 18:53:59 [scrapy.extensions.telnet] INFO: Telnet Password: 9e596cdec50a5bd7
2025-02-17 18:53:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-17 18:53:59 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-17 18:53:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-17 18:53:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-17 18:53:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-17 18:53:59 [scrapy.core.engine] INFO: Spider opened
2025-02-17 18:53:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-17 18:53:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-17 18:54:01 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-17 18:54:08 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-17 18:54:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 8 items (at 8 items/min)
2025-02-17 18:55:10 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-17 18:55:10 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-17T13-53-59+00-00.json
2025-02-17 18:55:10 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-17T13-53-59+00-00.csv
2025-02-17 18:55:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 70.334848,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 17, 13, 55, 10, 171664, tzinfo=datetime.timezone.utc),
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 15,
 'memusage/max': 118792192,
 'memusage/startup': 73240576,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 17, 13, 53, 59, 836816, tzinfo=datetime.timezone.utc)}
2025-02-17 18:55:10 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-18 00:46:03 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 00:46:03 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 00:46:03 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 00:46:03 [scrapy.extensions.telnet] INFO: Telnet Password: 42d4fd42b9a11046
2025-02-18 00:46:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 00:46:03 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 00:46:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 00:46:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 00:46:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 00:46:04 [scrapy.core.engine] INFO: Spider opened
2025-02-18 00:46:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 00:46:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 00:46:07 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 00:46:30 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-18 00:46:30 [scrapy.extensions.feedexport] INFO: Stored json feed (11 items) in: data/linkedin_post_2025-02-17T19-46-04+00-00.json
2025-02-18 00:46:30 [scrapy.extensions.feedexport] INFO: Stored csv feed (11 items) in: data/linkedin_post_2025-02-17T19-46-04+00-00.csv
2025-02-18 00:46:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 464,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 160219,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 26.645251,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 17, 19, 46, 30, 669452, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1427877,
 'httpcompression/response_count': 1,
 'item_scraped_count': 11,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73109504,
 'memusage/startup': 73109504,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 17, 19, 46, 4, 24201, tzinfo=datetime.timezone.utc)}
2025-02-18 00:46:30 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-18 18:31:21 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 18:31:21 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 18:31:21 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 18:31:21 [scrapy.extensions.telnet] INFO: Telnet Password: 8014225fc5d70ab9
2025-02-18 18:31:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 18:31:21 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 18:31:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 18:31:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 18:31:21 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 18:31:21 [scrapy.core.engine] INFO: Spider opened
2025-02-18 18:31:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 18:31:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 18:31:25 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 18:31:53 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-18 18:31:53 [scrapy.extensions.feedexport] INFO: Stored json feed (12 items) in: data/linkedin_post_2025-02-18T13-31-21+00-00.json
2025-02-18 18:31:53 [scrapy.extensions.feedexport] INFO: Stored csv feed (12 items) in: data/linkedin_post_2025-02-18T13-31-21+00-00.csv
2025-02-18 18:31:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 440,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 159046,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 31.751562,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 18, 13, 31, 53, 420528, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1411746,
 'httpcompression/response_count': 1,
 'item_scraped_count': 12,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 72355840,
 'memusage/startup': 72355840,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 18, 13, 31, 21, 668966, tzinfo=datetime.timezone.utc)}
2025-02-18 18:31:53 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-18 18:34:03 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 18:34:03 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 18:34:03 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 18:34:03 [scrapy.extensions.telnet] INFO: Telnet Password: b4b50358bea2eb7a
2025-02-18 18:34:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 18:34:03 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 18:34:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 18:34:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 18:34:03 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 18:34:03 [scrapy.core.engine] INFO: Spider opened
2025-02-18 18:34:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 18:34:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 18:34:07 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 18:34:33 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-18 18:34:33 [scrapy.extensions.feedexport] INFO: Stored json feed (11 items) in: data/linkedin_post_2025-02-18T13-34-03+00-00.json
2025-02-18 18:34:33 [scrapy.extensions.feedexport] INFO: Stored csv feed (11 items) in: data/linkedin_post_2025-02-18T13-34-03+00-00.csv
2025-02-18 18:34:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 464,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 158741,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 29.303611,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 18, 13, 34, 33, 192090, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1408763,
 'httpcompression/response_count': 1,
 'item_scraped_count': 11,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 72892416,
 'memusage/startup': 72892416,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 18, 13, 34, 3, 888479, tzinfo=datetime.timezone.utc)}
2025-02-18 18:34:33 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-18 18:35:14 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 18:35:14 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 18:35:14 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 18:35:14 [scrapy.extensions.telnet] INFO: Telnet Password: f5f4611b0fbae173
2025-02-18 18:35:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 18:35:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 18:35:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 18:35:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 18:35:14 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 18:35:14 [scrapy.core.engine] INFO: Spider opened
2025-02-18 18:35:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 18:35:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 18:35:18 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 18:35:46 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-18 18:35:46 [scrapy.extensions.feedexport] INFO: Stored json feed (12 items) in: data/linkedin_post_2025-02-18T13-35-14+00-00.json
2025-02-18 18:35:46 [scrapy.extensions.feedexport] INFO: Stored csv feed (12 items) in: data/linkedin_post_2025-02-18T13-35-14+00-00.csv
2025-02-18 18:35:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 440,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 159087,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 32.329111,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 18, 13, 35, 46, 837737, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1411784,
 'httpcompression/response_count': 1,
 'item_scraped_count': 12,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 72896512,
 'memusage/startup': 72896512,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 18, 13, 35, 14, 508626, tzinfo=datetime.timezone.utc)}
2025-02-18 18:35:46 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-18 18:50:26 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 18:50:26 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 18:50:26 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 18:50:26 [scrapy.extensions.telnet] INFO: Telnet Password: 0f42d76bc75c883c
2025-02-18 18:50:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 18:50:26 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 18:50:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 18:50:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 18:50:26 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 18:50:26 [scrapy.core.engine] INFO: Spider opened
2025-02-18 18:50:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 18:50:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 18:50:30 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 18:50:58 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-18 18:50:58 [scrapy.extensions.feedexport] INFO: Stored json feed (12 items) in: data/linkedin_post_2025-02-18T13-50-26+00-00.json
2025-02-18 18:50:58 [scrapy.extensions.feedexport] INFO: Stored csv feed (12 items) in: data/linkedin_post_2025-02-18T13-50-26+00-00.csv
2025-02-18 18:50:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 463,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 162642,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 31.368497,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 18, 13, 50, 58, 68626, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1440089,
 'httpcompression/response_count': 1,
 'item_scraped_count': 12,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73031680,
 'memusage/startup': 73031680,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 18, 13, 50, 26, 700129, tzinfo=datetime.timezone.utc)}
2025-02-18 18:50:58 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-18 18:56:15 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 18:56:15 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 18:56:15 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 18:56:15 [scrapy.extensions.telnet] INFO: Telnet Password: 368facd2e47376df
2025-02-18 18:56:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 18:56:15 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 18:56:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 18:56:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 18:56:15 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 18:56:15 [scrapy.core.engine] INFO: Spider opened
2025-02-18 18:56:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 18:56:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 18:56:18 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 18:56:42 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-18 18:56:42 [scrapy.extensions.feedexport] INFO: Stored json feed (12 items) in: data/linkedin_post_2025-02-18T13-56-15+00-00.json
2025-02-18 18:56:42 [scrapy.extensions.feedexport] INFO: Stored csv feed (12 items) in: data/linkedin_post_2025-02-18T13-56-15+00-00.csv
2025-02-18 18:56:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 455,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 162716,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 26.866798,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 18, 13, 56, 42, 675739, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1440063,
 'httpcompression/response_count': 1,
 'item_scraped_count': 12,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 73146368,
 'memusage/startup': 73146368,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 18, 13, 56, 15, 808941, tzinfo=datetime.timezone.utc)}
2025-02-18 18:56:42 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-18 20:19:14 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 20:19:14 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 20:19:14 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 20:19:14 [scrapy.extensions.telnet] INFO: Telnet Password: f6cbd54e06c45836
2025-02-18 20:19:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 20:19:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 20:19:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 20:19:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 20:19:14 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 20:19:14 [scrapy.core.engine] INFO: Spider opened
2025-02-18 20:19:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 20:19:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 20:19:17 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:19:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:19:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:19:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:19:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:19:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:19:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:19:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:19:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:19:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:19:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:19:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:19:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:19:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:19:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:19:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:19:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:19:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:19:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:19:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:19:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:19:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:19:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:19:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:19:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:19:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:19:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:19:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:19:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:19:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:19:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:19:45 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:20:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:20:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:20:27 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2025-02-18 20:20:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:20:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:20:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:20:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:20:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:20:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:20:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:20:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:20:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:20:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:20:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:20:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:20:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:20:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:20:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:20:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:20:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:20:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:20:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:20:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:20:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:20:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:20:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:20:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:20:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:20:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:20:34 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:21:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:21:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:21:17 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 1 pages/min), scraped 2 items (at 1 items/min)
2025-02-18 20:21:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:21:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:21:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:21:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:21:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:21:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:21:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:21:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:21:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:21:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:21:22 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:22:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:22:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:22:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:22:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:22:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:22:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:22:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:22:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:22:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:22:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:22:10 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:23:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:23:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:23:12 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 2 pages/min), scraped 4 items (at 2 items/min)
2025-02-18 20:23:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:23:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:23:17 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 1 pages/min), scraped 4 items (at 0 items/min)
2025-02-18 20:23:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:23:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:23:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:23:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:23:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:23:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:23:19 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:23:23 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-18 20:23:23 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-18 20:23:25 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcea4d253a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/07ca0a0362b86ad672eae36022788a91/cookie
2025-02-18 20:23:25 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcea4d24290>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/07ca0a0362b86ad672eae36022788a91/cookie
2025-02-18 20:23:25 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcea4d24890>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/07ca0a0362b86ad672eae36022788a91/cookie
2025-02-18 20:23:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4153177904/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0LLcaB7eauNQ4VuSh7AZNA%3D%3D&trackingId=ErISs6MDLAzEU7SBvGeygA%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?keywords=python%20developer%2C%20django&location=United%20States&f_TPR=r86400&f_WT=3,2)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fcea4d25460>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 215, in parse_job_page
    driver.add_cookie({"name": name, "value": value})
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 767, in add_cookie
    self.execute(Command.ADD_COOKIE, {"cookie": cookie_dict})
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=46835): Max retries exceeded with url: /session/07ca0a0362b86ad672eae36022788a91/cookie (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcea4d25460>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-18 20:23:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:23:25 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcea4cf0800>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f243a5269d21a469dc692e0eae0f1965/element
2025-02-18 20:23:25 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcea4461e50>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f243a5269d21a469dc692e0eae0f1965/element
2025-02-18 20:23:25 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcea4462240>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f243a5269d21a469dc692e0eae0f1965/element
2025-02-18 20:23:25 [linkedin_job] INFO: Timeout: No job links found within 15 seconds.
2025-02-18 20:23:25 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcea4462630>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f243a5269d21a469dc692e0eae0f1965/source
2025-02-18 20:23:25 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcea44627e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f243a5269d21a469dc692e0eae0f1965/source
2025-02-18 20:23:25 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcea4462960>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f243a5269d21a469dc692e0eae0f1965/source
2025-02-18 20:23:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?keywords=python%20developer%2C%20django&location=United%20States&f_TPR=r86400&f_WT=3,2> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fcea4462ae0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 140, in parse
    search_page_html = driver.page_source
                       ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=39147): Max retries exceeded with url: /session/f243a5269d21a469dc692e0eae0f1965/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcea4462ae0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-18 20:23:26 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:23:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4145965578/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0LLcaB7eauNQ4VuSh7AZNA%3D%3D&trackingId=4mlPdCRnc9pqGNDKP%2BCVEQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?keywords=python%20developer%2C%20django&location=United%20States&f_TPR=r86400&f_WT=3,2)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 211, in parse_job_page
    driver.get("https://www.linkedin.com")
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 665, in get
    return super().get(url)
           ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 454, in get
    self.execute(Command.GET, {"url": url})
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 429, in execute
    self.error_handler.check_response(response)
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py", line 232, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: invalid session id: session deleted as the browser has closed the connection
from disconnected: unable to send message to renderer
  (Session info: chrome=133.0.6943.53)
Stacktrace:
#0 0x63928aabdbba <unknown>
#1 0x63928a55b790 <unknown>
#2 0x63928a54196c <unknown>
#3 0x63928a5418e5 <unknown>
#4 0x63928a5400ff <unknown>
#5 0x63928a540d8f <unknown>
#6 0x63928a55076b <unknown>
#7 0x63928a56974f <unknown>
#8 0x63928a56fe6b <unknown>
#9 0x63928a5414a0 <unknown>
#10 0x63928a5692ba <unknown>
#11 0x63928a5f9017 <unknown>
#12 0x63928a5d2823 <unknown>
#13 0x63928a59ea88 <unknown>
#14 0x63928a59fbf1 <unknown>
#15 0x63928aa8715b <unknown>
#16 0x63928aa8b0e2 <unknown>
#17 0x63928aa7401c <unknown>
#18 0x63928aa8bcd4 <unknown>
#19 0x63928aa5848f <unknown>
#20 0x63928aaac4f8 <unknown>
#21 0x63928aaac6c9 <unknown>
#22 0x63928aabca36 <unknown>
#23 0x761ca749caa4 <unknown>
#24 0x761ca7529c3c <unknown>

2025-02-18 20:23:32 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-18 20:23:32 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.linkedin.com/jobs/view/4156891424/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0LLcaB7eauNQ4VuSh7AZNA%3D%3D&trackingId=vyN78gLurKgk4L6SVhkibQ%3D%3D&trk=flagship3_search_srp_jobs. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2025-02-18 20:26:18 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 20:26:18 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 20:26:18 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 20:26:18 [scrapy.extensions.telnet] INFO: Telnet Password: df6650b517d964ca
2025-02-18 20:26:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 20:26:18 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 20:26:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 20:26:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 20:26:18 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 20:26:18 [scrapy.core.engine] INFO: Spider opened
2025-02-18 20:26:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 20:26:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 20:26:21 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:26:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:26:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:26:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:26:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:26:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:26:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:26:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:26:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:26:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:26:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:26:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:26:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:26:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:26:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:26:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:26:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:26:41 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:26:44 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-18 20:26:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4157024038/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MLzmCHqL1e9%2Fy7E93oG9Lw%3D%3D&trackingId=xn58hmJtWUF1VhO1AI1mNA%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?currentJobId=4143249768&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 212, in parse_job_page
    driver.get("https://www.linkedin.com")
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 665, in get
    return super().get(url)
           ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 454, in get
    self.execute(Command.GET, {"url": url})
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/util.py", line 38, in reraise
    raise value.with_traceback(tb)
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2025-02-18 20:26:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:26:44 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79b06df81730>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/51dd34ec3f2a41a35e9edae2b47ceec3/element
2025-02-18 20:26:44 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79b06c2094f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/51dd34ec3f2a41a35e9edae2b47ceec3/element
2025-02-18 20:26:44 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79b06c20b260>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/51dd34ec3f2a41a35e9edae2b47ceec3/element
2025-02-18 20:26:44 [linkedin_job] INFO: Timeout: No job links found within 15 seconds.
2025-02-18 20:26:44 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79b06c20b680>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/51dd34ec3f2a41a35e9edae2b47ceec3/source
2025-02-18 20:26:44 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79b06c20b830>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/51dd34ec3f2a41a35e9edae2b47ceec3/source
2025-02-18 20:26:44 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79b06c20b9b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/51dd34ec3f2a41a35e9edae2b47ceec3/source
2025-02-18 20:26:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4143249768&keywords=Lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x79b06c20bb30>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 141, in parse
    search_page_html = driver.page_source
                       ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=41701): Max retries exceeded with url: /session/51dd34ec3f2a41a35e9edae2b47ceec3/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79b06c20bb30>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-18 20:26:44 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-18 20:26:44 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-18 20:29:52 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 20:29:52 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 20:29:52 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 20:29:52 [scrapy.extensions.telnet] INFO: Telnet Password: 35426a8679cab8fa
2025-02-18 20:29:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 20:29:52 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 20:29:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 20:29:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 20:29:52 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 20:29:52 [scrapy.core.engine] INFO: Spider opened
2025-02-18 20:29:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 20:29:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 20:29:54 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:30:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:31:11 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-18 20:31:11 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 20:31:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:32:08 [linkedin_job] INFO: Total unique job URLs collected so far: 25
2025-02-18 20:32:08 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 20:32:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:32:16 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-18 20:32:17 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-18 20:32:18 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7463e4bf4890>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5e6f51fdaedc1f06ae1bb2b1f889c663/execute/sync
2025-02-18 20:32:18 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7463e4c48590>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5e6f51fdaedc1f06ae1bb2b1f889c663/execute/sync
2025-02-18 20:32:18 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7463e4c48980>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5e6f51fdaedc1f06ae1bb2b1f889c663/execute/sync
2025-02-18 20:32:18 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7463e4c48e90>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5e6f51fdaedc1f06ae1bb2b1f889c663/element
2025-02-18 20:32:18 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7463e4c490d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5e6f51fdaedc1f06ae1bb2b1f889c663/element
2025-02-18 20:32:18 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7463e4c2bfe0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5e6f51fdaedc1f06ae1bb2b1f889c663/element
2025-02-18 20:32:18 [linkedin_job] INFO: Timeout: No job links found within 15 seconds.
2025-02-18 20:32:18 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7463e4c488c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5e6f51fdaedc1f06ae1bb2b1f889c663/source
2025-02-18 20:32:18 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7463e4c48c50>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5e6f51fdaedc1f06ae1bb2b1f889c663/source
2025-02-18 20:32:18 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7463e4c489e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5e6f51fdaedc1f06ae1bb2b1f889c663/source
2025-02-18 20:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4136443544&geoId=101022442&keywords=lua%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7463e4c48470>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 141, in parse
    search_page_html = driver.page_source
                       ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=42805): Max retries exceeded with url: /session/5e6f51fdaedc1f06ae1bb2b1f889c663/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7463e4c48470>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-18 20:32:18 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-18 20:33:15 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 20:33:15 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 20:33:15 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 20:33:15 [scrapy.extensions.telnet] INFO: Telnet Password: bab674c54677dc41
2025-02-18 20:33:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 20:33:15 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 20:33:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 20:33:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 20:33:15 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 20:33:15 [scrapy.core.engine] INFO: Spider opened
2025-02-18 20:33:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 20:33:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 20:33:20 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:33:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:33:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:33:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:33:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:33:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:33:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:33:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:33:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:33:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:33:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:33:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:33:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:33:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:33:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:33:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:33:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:33:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:33:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:33:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:33:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:33:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:33:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:33:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:33:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:33:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:33:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:33:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:33:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:33:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:33:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:33:40 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:34:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:44 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2025-02-18 20:34:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:34:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:34:53 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:35:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:35:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:35:47 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 2 pages/min), scraped 2 items (at 1 items/min)
2025-02-18 20:35:53 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:37:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:37:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:37:09 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 0 pages/min), scraped 3 items (at 1 items/min)
2025-02-18 20:37:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:37:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:37:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:37:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:37:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:37:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:37:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:37:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:37:15 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:38:11 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-18 20:38:11 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-18 20:38:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:38:12 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7aea047a8800>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5e903d930dd67956cd63236d6bcc7211/element
2025-02-18 20:38:12 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7aea04848b30>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5e903d930dd67956cd63236d6bcc7211/element
2025-02-18 20:38:12 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7aea04848a40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5e903d930dd67956cd63236d6bcc7211/element
2025-02-18 20:38:12 [linkedin_job] INFO: Timeout: No job links found within 15 seconds.
2025-02-18 20:38:12 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7aea0484afc0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5e903d930dd67956cd63236d6bcc7211/source
2025-02-18 20:38:12 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7aea0484b8c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5e903d930dd67956cd63236d6bcc7211/source
2025-02-18 20:38:12 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7aea0484aff0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/5e903d930dd67956cd63236d6bcc7211/source
2025-02-18 20:38:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?currentJobId=4108512688&geoId=101022442&keywords=mern%20developer&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7aea0484a390>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 141, in parse
    search_page_html = driver.page_source
                       ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=56161): Max retries exceeded with url: /session/5e903d930dd67956cd63236d6bcc7211/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7aea0484a390>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-18 20:38:12 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 1 pages/min), scraped 4 items (at 1 items/min)
2025-02-18 20:38:17 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-18 20:39:09 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 20:39:09 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 20:39:09 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 20:39:09 [scrapy.extensions.telnet] INFO: Telnet Password: 9c13696ba45a63d7
2025-02-18 20:39:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 20:39:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 20:39:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 20:39:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 20:39:09 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 20:39:09 [scrapy.core.engine] INFO: Spider opened
2025-02-18 20:39:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 20:39:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 20:39:13 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:39:26 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-18 20:39:27 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-18 20:39:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:39:29 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff0eab68740>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f466000f7d3ff31e0d88dbd4edc76eb4/element
2025-02-18 20:39:29 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff0eab68170>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f466000f7d3ff31e0d88dbd4edc76eb4/element
2025-02-18 20:39:29 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff0eab68260>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f466000f7d3ff31e0d88dbd4edc76eb4/element
2025-02-18 20:39:29 [linkedin_job] INFO: Timeout: No job links found within 15 seconds.
2025-02-18 20:39:29 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff0eab68f80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f466000f7d3ff31e0d88dbd4edc76eb4/source
2025-02-18 20:39:29 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff0eab690a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f466000f7d3ff31e0d88dbd4edc76eb4/source
2025-02-18 20:39:29 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff0eab692b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f466000f7d3ff31e0d88dbd4edc76eb4/source
2025-02-18 20:39:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?keywords=python%20developer%2C%20django&location=United%20States&f_TPR=r86400&f_WT=3,2> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7ff0eab69460>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 137, in parse
    search_page_html = driver.page_source
                       ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=37183): Max retries exceeded with url: /session/f466000f7d3ff31e0d88dbd4edc76eb4/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7ff0eab69460>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-18 20:39:29 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-18 20:51:34 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 20:51:34 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 20:51:34 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 20:51:34 [scrapy.extensions.telnet] INFO: Telnet Password: 6a4d833da2544626
2025-02-18 20:51:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 20:51:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 20:51:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 20:51:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 20:51:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 20:51:34 [scrapy.core.engine] INFO: Spider opened
2025-02-18 20:51:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 20:51:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 20:51:37 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:52:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:52:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:52:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:52:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:52:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:52:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:52:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:52:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:52:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:52:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:52:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:52:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:52:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:52:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:52:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:52:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:52:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:52:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:52:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:52:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:52:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:52:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:52:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:52:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:52:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:52:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:52:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:52:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:52:13 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:52:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:52:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:52:57 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 1 items (at 1 items/min)
2025-02-18 20:53:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:04 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:53:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:47 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 1 pages/min), scraped 2 items (at 1 items/min)
2025-02-18 20:53:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:53:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:53:53 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:54:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:54:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:54:38 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 1 pages/min), scraped 3 items (at 1 items/min)
2025-02-18 20:54:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:54:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:54:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:54:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:54:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:54:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:54:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:54:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:54:44 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:55:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:55:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:55:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:55:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:55:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:55:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:55:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:55:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:55:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:55:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:55:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:55:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:55:31 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:56:27 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:57:11 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 20:58:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:03 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 4 pages/min), scraped 7 items (at 4 items/min)
2025-02-18 20:58:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:34 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 7 items (at 0 items/min)
2025-02-18 20:58:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:58:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:58:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:06 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:06 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:07 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:08 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:08 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:09 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:09 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:10 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:10 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:11 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:11 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:12 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:12 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:13 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:13 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:14 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:14 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:15 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:15 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:16 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:16 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:17 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:17 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:18 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:18 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:19 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:19 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:20 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:20 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:21 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:21 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:22 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:22 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:23 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:23 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:24 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:24 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:25 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:25 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:26 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:26 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:27 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:27 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:28 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:28 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:29 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:29 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:30 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:30 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:31 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:31 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:32 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:32 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:33 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:33 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:34 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:34 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 7 items (at 0 items/min)
2025-02-18 20:59:34 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:35 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:35 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:36 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:36 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:37 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:37 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:38 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:38 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:39 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:39 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:40 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:40 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:41 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:41 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:42 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:42 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:43 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:43 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:44 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:44 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:45 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:46 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:46 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:47 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:47 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:48 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:48 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:49 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:49 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:50 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:50 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:51 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:51 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:52 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:52 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:53 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:53 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:54 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:54 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:55 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:55 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:56 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:56 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:57 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:57 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:58 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:58 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 20:59:59 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 20:59:59 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:00 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:00 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:01 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:02 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:02 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:03 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:03 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:04 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:04 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:05 [linkedin_job] INFO: Total unique job URLs collected so far: 7
2025-02-18 21:00:05 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-18 21:00:05 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 21:00:05 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7abe86cc17f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c0c1cfd27499a321b7ffc06786593d7a/element
2025-02-18 21:00:05 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7abe861d6ff0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c0c1cfd27499a321b7ffc06786593d7a/element
2025-02-18 21:00:05 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7abe8613bb30>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c0c1cfd27499a321b7ffc06786593d7a/element
2025-02-18 21:00:05 [linkedin_job] INFO: Timeout: No job links found within 15 seconds.
2025-02-18 21:00:05 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7abe86146b40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c0c1cfd27499a321b7ffc06786593d7a/source
2025-02-18 21:00:05 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7abe86a4ef00>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c0c1cfd27499a321b7ffc06786593d7a/source
2025-02-18 21:00:05 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7abe861d5a90>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c0c1cfd27499a321b7ffc06786593d7a/source
2025-02-18 21:00:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?keywords=python%20developer%2C%20django&location=United%20States&f_TPR=r86400&f_WT=3,2> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7abe861d7230>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 137, in parse
    search_page_html = driver.page_source
                       ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=34919): Max retries exceeded with url: /session/c0c1cfd27499a321b7ffc06786593d7a/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7abe861d7230>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-18 21:00:05 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-18 21:00:05 [scrapy.extensions.feedexport] INFO: Stored json feed (7 items) in: data/linkedin_job_2025-02-18T15-51-34+00-00.json
2025-02-18 21:00:05 [scrapy.extensions.feedexport] INFO: Stored csv feed (7 items) in: data/linkedin_job_2025-02-18T15-51-34+00-00.csv
2025-02-18 21:00:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7100,
 'downloader/request_count': 8,
 'downloader/request_method_count/GET': 8,
 'downloader/response_bytes': 1203228,
 'downloader/response_count': 8,
 'downloader/response_status_count/200': 8,
 'dupefilter/filtered': 5600,
 'elapsed_time_seconds': 510.852915,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2025, 2, 18, 16, 0, 5, 532529, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 10649256,
 'httpcompression/response_count': 8,
 'item_scraped_count': 7,
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 1631,
 'log_count/WARNING': 6,
 'memusage/max': 696758272,
 'memusage/startup': 73236480,
 'request_depth_max': 1,
 'response_received_count': 8,
 'responses_per_minute': None,
 'scheduler/dequeued': 8,
 'scheduler/dequeued/memory': 8,
 'scheduler/enqueued': 8,
 'scheduler/enqueued/memory': 8,
 'spider_exceptions/MaxRetryError': 1,
 'start_time': datetime.datetime(2025, 2, 18, 15, 51, 34, 679614, tzinfo=datetime.timezone.utc)}
2025-02-18 21:00:05 [scrapy.core.engine] INFO: Spider closed (shutdown)
2025-02-18 21:17:22 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 21:17:22 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 21:17:22 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 21:17:22 [scrapy.extensions.telnet] INFO: Telnet Password: 1219b6d0f8fc7011
2025-02-18 21:17:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 21:17:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 21:17:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 21:17:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 21:17:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 21:17:22 [scrapy.core.engine] INFO: Spider opened
2025-02-18 21:17:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 21:17:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 21:17:23 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 21:17:29 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 21:17:34 [google_jobs] INFO: No location not now button
2025-02-18 21:17:54 [google_jobs] INFO: Timeout: No jobs found within 15 seconds.
2025-02-18 21:17:54 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-18 21:17:54 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-18T16-17-22+00-00.json
2025-02-18 21:17:54 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-18T16-17-22+00-00.csv
2025-02-18 21:17:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 32.290723,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 18, 16, 17, 54, 535258, tzinfo=datetime.timezone.utc),
 'items_per_minute': None,
 'log_count/INFO': 16,
 'memusage/max': 73048064,
 'memusage/startup': 73048064,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 18, 16, 17, 22, 244535, tzinfo=datetime.timezone.utc)}
2025-02-18 21:17:54 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-18 21:25:22 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 21:25:22 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 21:25:22 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 21:25:22 [scrapy.extensions.telnet] INFO: Telnet Password: 9d30c52662e71b58
2025-02-18 21:25:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 21:25:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 21:25:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 21:25:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 21:25:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 21:25:22 [scrapy.core.engine] INFO: Spider opened
2025-02-18 21:25:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 21:25:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 21:25:23 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 21:25:30 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 21:25:35 [google_jobs] INFO: No location not now button
2025-02-18 21:26:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 9 items (at 9 items/min)
2025-02-18 21:26:27 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-18 21:26:27 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-18T16-25-22+00-00.json
2025-02-18 21:26:27 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-18T16-25-22+00-00.csv
2025-02-18 21:26:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 65.332177,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 18, 16, 26, 27, 801724, tzinfo=datetime.timezone.utc),
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 16,
 'memusage/max': 113577984,
 'memusage/startup': 73183232,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 18, 16, 25, 22, 469547, tzinfo=datetime.timezone.utc)}
2025-02-18 21:26:27 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-18 23:54:02 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 23:54:02 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 23:54:02 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 23:54:02 [scrapy.extensions.telnet] INFO: Telnet Password: 5a3754f8b32f713e
2025-02-18 23:54:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 23:54:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 23:54:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 23:54:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 23:54:02 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 23:54:02 [scrapy.core.engine] INFO: Spider opened
2025-02-18 23:54:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 23:54:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 23:54:03 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 23:54:09 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-18 23:54:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72b47587d280>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/90dab2d3821da04ee327e09fd5c87b43/source
2025-02-18 23:54:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72b475c16360>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/90dab2d3821da04ee327e09fd5c87b43/source
2025-02-18 23:54:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72b47587cc80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/90dab2d3821da04ee327e09fd5c87b43/source
2025-02-18 23:54:09 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x72b47587d130>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 55, in start_requests
    page_html = driver.page_source
                ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=44513): Max retries exceeded with url: /session/90dab2d3821da04ee327e09fd5c87b43/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72b47587d130>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-18 23:54:09 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-18 23:54:09 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-18T18-54-02+00-00.json
2025-02-18 23:54:09 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-18T18-54-02+00-00.csv
2025-02-18 23:54:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 7.590959,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 18, 18, 54, 9, 760944, tzinfo=datetime.timezone.utc),
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 14,
 'log_count/WARNING': 3,
 'memusage/max': 73277440,
 'memusage/startup': 73277440,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 18, 18, 54, 2, 169985, tzinfo=datetime.timezone.utc)}
2025-02-18 23:54:09 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-18 23:54:13 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 23:54:13 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 23:54:13 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 23:54:13 [scrapy.extensions.telnet] INFO: Telnet Password: 4978b0f429b584b1
2025-02-18 23:54:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 23:54:13 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 23:54:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 23:54:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 23:54:13 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 23:54:13 [scrapy.core.engine] INFO: Spider opened
2025-02-18 23:54:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 23:54:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 23:54:18 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 23:54:40 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-18 23:54:40 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-18 23:54:45 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-18 23:54:45 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d60826087d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/df751dc5f229bd6e838411359c44092c/element
2025-02-18 23:54:45 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d6082608380>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/df751dc5f229bd6e838411359c44092c/element
2025-02-18 23:54:45 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d6082608140>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/df751dc5f229bd6e838411359c44092c/element
2025-02-18 23:54:45 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d6082608ec0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/df751dc5f229bd6e838411359c44092c/element
2025-02-18 23:54:45 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d60826090a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/df751dc5f229bd6e838411359c44092c/element
2025-02-18 23:54:45 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d6082609310>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/df751dc5f229bd6e838411359c44092c/element
2025-02-18 23:54:45 [linkedin_job] INFO: Timeout: No job links found within 15 seconds.
2025-02-18 23:54:45 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d6082608830>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/df751dc5f229bd6e838411359c44092c/source
2025-02-18 23:54:45 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d6082608bf0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/df751dc5f229bd6e838411359c44092c/source
2025-02-18 23:54:45 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d60826080e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/df751dc5f229bd6e838411359c44092c/source
2025-02-18 23:54:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/search/?keywords=python%20developer%2C%20django&location=United%20States&f_TPR=r86400&f_WT=3,2> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7d60826083e0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 137, in parse
    search_page_html = driver.page_source
                       ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=32967): Max retries exceeded with url: /session/df751dc5f229bd6e838411359c44092c/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7d60826083e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-18 23:54:45 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-18 23:54:54 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-18 23:54:54 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-18 23:54:54 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-18 23:54:54 [scrapy.extensions.telnet] INFO: Telnet Password: a276fa6dc84a5f1c
2025-02-18 23:54:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-18 23:54:54 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-18 23:54:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-18 23:54:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-18 23:54:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-18 23:54:54 [scrapy.core.engine] INFO: Spider opened
2025-02-18 23:54:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-18 23:54:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-18 23:54:59 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-18 23:55:17 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-18 23:55:18 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-18 23:55:19 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b8083e7d6a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/09184ee85afa1779ca46b560821d2da3/element
2025-02-18 23:55:19 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b80828dc290>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/09184ee85afa1779ca46b560821d2da3/element
2025-02-18 23:55:19 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b80828dc170>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/09184ee85afa1779ca46b560821d2da3/element
2025-02-18 23:55:19 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b80828dc7a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/09184ee85afa1779ca46b560821d2da3/execute/sync
2025-02-18 23:55:19 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b80828dc9e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/09184ee85afa1779ca46b560821d2da3/execute/sync
2025-02-18 23:55:19 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b80828dcc20>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/09184ee85afa1779ca46b560821d2da3/execute/sync
2025-02-18 23:55:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/search/results/content/?keywords=%22python%20developer%22%2C%20%22jobs%22%2C%20%22remote%22&origin=GLOBAL_SEARCH_HEADER> (referer: None)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7b80828dce60>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_post.py", line 83, in parse
    self.scroll_to_load_posts(driver, max_scrolls=2)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_post.py", line 136, in scroll_to_load_posts
    driver.execute_script("window.scrollBy(0, 1000);")
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=58981): Max retries exceeded with url: /session/09184ee85afa1779ca46b560821d2da3/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b80828dce60>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-18 23:55:19 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-19 01:08:12 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-19 01:08:12 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-19 01:08:12 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-19 01:08:12 [scrapy.extensions.telnet] INFO: Telnet Password: 13b6a8d8e657b8dd
2025-02-19 01:08:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-19 01:08:12 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-19 01:08:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-19 01:08:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-19 01:08:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-19 01:08:12 [scrapy.core.engine] INFO: Spider opened
2025-02-19 01:08:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-19 01:08:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-19 01:08:13 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-19 01:08:20 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-19 01:08:25 [google_jobs] INFO: No location not now button
2025-02-19 01:09:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 7 items (at 7 items/min)
2025-02-19 01:10:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 19 items (at 12 items/min)
2025-02-19 01:10:52 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-19 01:10:52 [scrapy.extensions.feedexport] INFO: Stored json feed (27 items) in: data/google_jobs_2025-02-18T20-08-12+00-00.json
2025-02-19 01:10:52 [scrapy.extensions.feedexport] INFO: Stored csv feed (27 items) in: data/google_jobs_2025-02-18T20-08-12+00-00.csv
2025-02-19 01:10:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 160.379454,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 18, 20, 10, 52, 494209, tzinfo=datetime.timezone.utc),
 'item_scraped_count': 27,
 'items_per_minute': None,
 'log_count/INFO': 17,
 'memusage/max': 113643520,
 'memusage/startup': 73334784,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 18, 20, 8, 12, 114755, tzinfo=datetime.timezone.utc)}
2025-02-19 01:10:52 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-19 21:08:08 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-19 21:08:08 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-19 21:08:08 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-19 21:08:08 [scrapy.extensions.telnet] INFO: Telnet Password: 042e38e4c656e853
2025-02-19 21:08:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-19 21:08:08 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-19 21:08:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-19 21:08:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-19 21:08:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-19 21:08:08 [scrapy.core.engine] INFO: Spider opened
2025-02-19 21:08:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-19 21:08:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-19 21:08:10 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-19 21:08:19 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-19 21:08:25 [google_jobs] INFO: No location not now button
2025-02-19 21:08:29 [google_jobs] INFO: No load more button
2025-02-19 21:08:40 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-19 21:08:40 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-19 21:08:41 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bf74ee698e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/707666fba335069332bb2852ed5dceac/element
2025-02-19 21:08:41 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bf74ee69640>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/707666fba335069332bb2852ed5dceac/element
2025-02-19 21:08:41 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bf74ee68a10>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/707666fba335069332bb2852ed5dceac/element
2025-02-19 21:08:41 [google_jobs] INFO: Timeout: No jobs found within 15 seconds.
2025-02-19 21:08:41 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bf74ee6a150>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/707666fba335069332bb2852ed5dceac/elements
2025-02-19 21:08:41 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bf74ee6a330>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/707666fba335069332bb2852ed5dceac/elements
2025-02-19 21:08:41 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bf74ee6a570>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/707666fba335069332bb2852ed5dceac/elements
2025-02-19 21:08:41 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7bf74ee6a7b0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 61, in start_requests
    yield from self.parse(response)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 103, in parse
    job_items = driver.find_elements(By.XPATH, '//a[@class="MQUd2b"]')
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 926, in find_elements
    return self.execute(Command.FIND_ELEMENTS, {"using": by, "value": value})["value"] or []
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=50803): Max retries exceeded with url: /session/707666fba335069332bb2852ed5dceac/elements (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bf74ee6a7b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-19 21:08:41 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-19 21:08:41 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-19T16-08-09+00-00.json
2025-02-19 21:08:41 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-19T16-08-09+00-00.csv
2025-02-19 21:09:37 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-19 21:09:37 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-19 21:09:37 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-19 21:09:37 [scrapy.extensions.telnet] INFO: Telnet Password: b4eed8ea0af4892f
2025-02-19 21:09:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-19 21:09:37 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-19 21:09:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-19 21:09:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-19 21:09:37 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-19 21:09:37 [scrapy.core.engine] INFO: Spider opened
2025-02-19 21:09:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-19 21:09:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-19 21:09:44 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-19 21:09:56 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-19 21:10:05 [google_jobs] INFO: No location not now button
2025-02-19 21:10:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 5 items (at 5 items/min)
2025-02-19 21:11:02 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-19 21:11:02 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-19T16-09-37+00-00.json
2025-02-19 21:11:02 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-19T16-09-37+00-00.csv
2025-02-19 21:11:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 85.348583,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 19, 16, 11, 2, 447204, tzinfo=datetime.timezone.utc),
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 16,
 'memusage/max': 119222272,
 'memusage/startup': 72962048,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 19, 16, 9, 37, 98621, tzinfo=datetime.timezone.utc)}
2025-02-19 21:11:02 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-19 21:35:38 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-19 21:35:38 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-19 21:35:38 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-19 21:35:38 [scrapy.extensions.telnet] INFO: Telnet Password: d47f68f1316178c4
2025-02-19 21:35:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-19 21:35:38 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-19 21:35:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-19 21:35:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-19 21:35:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-19 21:35:38 [scrapy.core.engine] INFO: Spider opened
2025-02-19 21:35:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-19 21:35:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-19 21:35:39 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-19 21:35:47 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-19 21:35:47 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-19 21:35:47 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-19 21:35:53 [google_jobs] INFO: No location not now button
2025-02-19 21:35:57 [google_jobs] INFO: No load more button
2025-02-19 21:36:01 [google_jobs] INFO: Timeout: No jobs found within 15 seconds.
2025-02-19 21:36:01 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 61, in start_requests
    yield from self.parse(response)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 103, in parse
    job_items = driver.find_elements(By.XPATH, '//a[@class="MQUd2b"]')
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 926, in find_elements
    return self.execute(Command.FIND_ELEMENTS, {"using": by, "value": value})["value"] or []
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 429, in execute
    self.error_handler.check_response(response)
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py", line 232, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: invalid session id
Stacktrace:
#0 0x5d3600b17bea <unknown>
#1 0x5d36005b5623 <unknown>
#2 0x5d36005f7d7f <unknown>
#3 0x5d360062c996 <unknown>
#4 0x5d36006271f2 <unknown>
#5 0x5d3600626325 <unknown>
#6 0x5d360057e328 <unknown>
#7 0x5d3600ae118b <unknown>
#8 0x5d3600ae5112 <unknown>
#9 0x5d3600ace04c <unknown>
#10 0x5d3600ae5d04 <unknown>
#11 0x5d3600ab24bf <unknown>
#12 0x5d360057cd72 <unknown>
#13 0x751ac442a1ca <unknown>
#14 0x751ac442a28b __libc_start_main
#15 0x5d360054576a _start

2025-02-19 21:36:01 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-19 21:36:01 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-19T16-35-38+00-00.json
2025-02-19 21:36:01 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-19T16-35-38+00-00.csv
2025-02-19 21:46:51 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-19 21:46:51 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-19 21:46:51 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-19 21:46:51 [scrapy.extensions.telnet] INFO: Telnet Password: b41c5edcca518762
2025-02-19 21:46:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-19 21:46:51 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-19 21:46:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-19 21:46:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-19 21:46:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-19 21:46:51 [scrapy.core.engine] INFO: Spider opened
2025-02-19 21:46:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-19 21:46:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-19 21:46:53 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-19 21:46:58 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-19 21:46:58 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-19 21:47:01 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e50bbb84d70>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/a871da0dbcbe3104d09b0efba44ecdd5/source
2025-02-19 21:47:01 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e50bbb84770>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/a871da0dbcbe3104d09b0efba44ecdd5/source
2025-02-19 21:47:01 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e50bbb845c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/a871da0dbcbe3104d09b0efba44ecdd5/source
2025-02-19 21:47:01 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7e50bbb84800>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 57, in start_requests
    page_html = driver.page_source
                ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=57453): Max retries exceeded with url: /session/a871da0dbcbe3104d09b0efba44ecdd5/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e50bbb84800>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-19 21:47:01 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-19 21:47:01 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-19T16-46-51+00-00.json
2025-02-19 21:47:01 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-19T16-46-51+00-00.csv
2025-02-19 21:47:47 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-19 21:47:47 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-19 21:47:47 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-19 21:47:47 [scrapy.extensions.telnet] INFO: Telnet Password: 4dc8e04de9b85184
2025-02-19 21:47:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-19 21:47:47 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-19 21:47:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-19 21:47:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-19 21:47:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-19 21:47:47 [scrapy.core.engine] INFO: Spider opened
2025-02-19 21:47:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-19 21:47:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-19 21:47:50 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-19 21:47:58 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-19 21:48:04 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-19 21:48:04 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-19 21:48:04 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7157becbddf0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e643b8c7edee5bccd4a08519884bb372/element
2025-02-19 21:48:04 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7157becbcbf0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e643b8c7edee5bccd4a08519884bb372/element
2025-02-19 21:48:04 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7157becbcb30>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e643b8c7edee5bccd4a08519884bb372/element
2025-02-19 21:48:04 [google_jobs] INFO: No location not now button
2025-02-19 21:48:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7157becbe2a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e643b8c7edee5bccd4a08519884bb372/element
2025-02-19 21:48:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7157becbe480>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e643b8c7edee5bccd4a08519884bb372/element
2025-02-19 21:48:09 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7157becbe6c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e643b8c7edee5bccd4a08519884bb372/element
2025-02-19 21:48:09 [google_jobs] INFO: No load more button
2025-02-19 21:48:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7157becbec90>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e643b8c7edee5bccd4a08519884bb372/element
2025-02-19 21:48:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7157becbeed0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e643b8c7edee5bccd4a08519884bb372/element
2025-02-19 21:48:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7157becbf110>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e643b8c7edee5bccd4a08519884bb372/element
2025-02-19 21:48:13 [google_jobs] INFO: Timeout: No jobs found within 15 seconds.
2025-02-19 21:48:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7157becbe8d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e643b8c7edee5bccd4a08519884bb372/elements
2025-02-19 21:48:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7157becbe4b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e643b8c7edee5bccd4a08519884bb372/elements
2025-02-19 21:48:13 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7157becbe300>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e643b8c7edee5bccd4a08519884bb372/elements
2025-02-19 21:48:13 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7157becbdaf0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 61, in start_requests
    yield from self.parse(response)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 103, in parse
    job_items = driver.find_elements(By.XPATH, '//a[@class="MQUd2b"]')
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 926, in find_elements
    return self.execute(Command.FIND_ELEMENTS, {"using": by, "value": value})["value"] or []
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=44837): Max retries exceeded with url: /session/e643b8c7edee5bccd4a08519884bb372/elements (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7157becbdaf0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-19 21:48:13 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-19 21:48:13 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-19T16-47-47+00-00.json
2025-02-19 21:48:13 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-19T16-47-47+00-00.csv
2025-02-19 22:06:06 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-19 22:06:06 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-19 22:06:06 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-19 22:06:06 [scrapy.extensions.telnet] INFO: Telnet Password: 3204dcf42e21d913
2025-02-19 22:06:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-19 22:06:06 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-19 22:06:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-19 22:06:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-19 22:06:06 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-19 22:06:06 [scrapy.core.engine] INFO: Spider opened
2025-02-19 22:06:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-19 22:06:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-19 22:06:09 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-19 22:06:21 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-19 22:06:21 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-19 22:06:22 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b6a7ef896a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e33f2e60f0b89c744f9b1f11a5245a66/source
2025-02-19 22:06:22 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b6a7ef890a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e33f2e60f0b89c744f9b1f11a5245a66/source
2025-02-19 22:06:22 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b6a7ef88cb0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e33f2e60f0b89c744f9b1f11a5245a66/source
2025-02-19 22:06:22 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7b6a7ef88b30>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 61, in start_requests
    page_html = driver.page_source
                ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=60003): Max retries exceeded with url: /session/e33f2e60f0b89c744f9b1f11a5245a66/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b6a7ef88b30>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-19 22:06:22 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-19 22:06:22 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-19T17-06-06+00-00.json
2025-02-19 22:06:22 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-19T17-06-06+00-00.csv
2025-02-19 22:11:44 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-19 22:11:44 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-19 22:11:44 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-19 22:11:44 [scrapy.extensions.telnet] INFO: Telnet Password: f708b57cbf962861
2025-02-19 22:11:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-19 22:11:44 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-19 22:11:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-19 22:11:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-19 22:11:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-19 22:11:44 [scrapy.core.engine] INFO: Spider opened
2025-02-19 22:11:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-19 22:11:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-19 22:11:47 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-19 22:11:56 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-19 22:12:02 [google_jobs] INFO: No location not now button
2025-02-19 22:12:04 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-19 22:12:04 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-19 22:12:07 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7225bcd81d30>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f4b46211f0ab3f2b3de4c4db372db07b/element
2025-02-19 22:12:07 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7225bcd81a60>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f4b46211f0ab3f2b3de4c4db372db07b/element
2025-02-19 22:12:07 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7225bcd80d70>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f4b46211f0ab3f2b3de4c4db372db07b/element
2025-02-19 22:12:07 [google_jobs] INFO: No load more button
2025-02-19 22:12:11 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7225bcd82450>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f4b46211f0ab3f2b3de4c4db372db07b/element
2025-02-19 22:12:11 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7225bcd82690>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f4b46211f0ab3f2b3de4c4db372db07b/element
2025-02-19 22:12:11 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7225bcd828d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f4b46211f0ab3f2b3de4c4db372db07b/element
2025-02-19 22:12:11 [google_jobs] INFO: Timeout: No jobs found within 15 seconds.
2025-02-19 22:12:11 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7225bcd82e70>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f4b46211f0ab3f2b3de4c4db372db07b/elements
2025-02-19 22:12:11 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7225bcd83050>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f4b46211f0ab3f2b3de4c4db372db07b/elements
2025-02-19 22:12:11 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7225bcd83290>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f4b46211f0ab3f2b3de4c4db372db07b/elements
2025-02-19 22:12:11 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7225bcd82570>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 64, in start_requests
    yield from self.parse(response)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 106, in parse
    job_items = driver.find_elements(By.XPATH, '//a[@class="MQUd2b"]')
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 926, in find_elements
    return self.execute(Command.FIND_ELEMENTS, {"using": by, "value": value})["value"] or []
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=60791): Max retries exceeded with url: /session/f4b46211f0ab3f2b3de4c4db372db07b/elements (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7225bcd82570>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-19 22:12:11 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-19 22:12:11 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-19T17-11-44+00-00.json
2025-02-19 22:12:11 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-19T17-11-44+00-00.csv
2025-02-24 21:08:01 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-24 21:08:01 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-24 21:08:01 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-24 21:08:01 [scrapy.extensions.telnet] INFO: Telnet Password: 6cf12a58e9e5f665
2025-02-24 21:08:01 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-24 21:08:01 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-24 21:08:01 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-24 21:08:01 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-24 21:08:01 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-24 21:08:01 [scrapy.core.engine] INFO: Spider opened
2025-02-24 21:08:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-24 21:08:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-24 21:08:03 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-24 21:08:10 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-24 21:08:15 [google_jobs] INFO: No location not now button
2025-02-24 21:09:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 9 items (at 9 items/min)
2025-02-24 21:09:07 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-24 21:09:07 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-24T16-08-01+00-00.json
2025-02-24 21:09:07 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-24T16-08-01+00-00.csv
2025-02-24 21:09:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 65.361908,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 24, 16, 9, 7, 249441, tzinfo=datetime.timezone.utc),
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 16,
 'memusage/max': 115331072,
 'memusage/startup': 73003008,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 24, 16, 8, 1, 887533, tzinfo=datetime.timezone.utc)}
2025-02-24 21:09:07 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 00:27:33 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 00:27:33 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 00:27:33 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 00:27:33 [scrapy.extensions.telnet] INFO: Telnet Password: b76399a49211cd0b
2025-02-25 00:27:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 00:27:33 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 00:27:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 00:27:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 00:27:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 00:27:33 [scrapy.core.engine] INFO: Spider opened
2025-02-25 00:27:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 00:27:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 00:27:34 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 00:27:41 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 00:27:47 [google_jobs] INFO: No location not now button
2025-02-25 00:28:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 7 items (at 7 items/min)
2025-02-25 00:29:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 19 items (at 12 items/min)
2025-02-25 00:30:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 31 items (at 12 items/min)
2025-02-25 00:31:08 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 00:31:08 [scrapy.extensions.feedexport] INFO: Stored json feed (38 items) in: data/google_jobs_2025-02-24T19-27-33+00-00.json
2025-02-25 00:31:08 [scrapy.extensions.feedexport] INFO: Stored csv feed (38 items) in: data/google_jobs_2025-02-24T19-27-33+00-00.csv
2025-02-25 00:31:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 215.575611,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 24, 19, 31, 8, 922773, tzinfo=datetime.timezone.utc),
 'item_scraped_count': 38,
 'items_per_minute': None,
 'log_count/INFO': 18,
 'memusage/max': 133484544,
 'memusage/startup': 133484544,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 24, 19, 27, 33, 347162, tzinfo=datetime.timezone.utc)}
2025-02-25 00:31:08 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 00:58:30 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 00:58:30 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 00:58:30 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 00:58:30 [scrapy.extensions.telnet] INFO: Telnet Password: e37201e9bdbb3b7a
2025-02-25 00:58:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 00:58:30 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 00:58:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 00:58:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 00:58:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 00:58:30 [scrapy.core.engine] INFO: Spider opened
2025-02-25 00:58:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 00:58:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 00:58:35 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 00:59:02 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 00:59:02 [scrapy.extensions.feedexport] INFO: Stored json feed (12 items) in: data/linkedin_post_2025-02-24T19-58-30+00-00.json
2025-02-25 00:59:02 [scrapy.extensions.feedexport] INFO: Stored csv feed (12 items) in: data/linkedin_post_2025-02-24T19-58-30+00-00.csv
2025-02-25 00:59:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 455,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 159822,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 32.124676,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 24, 19, 59, 2, 903348, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1410499,
 'httpcompression/response_count': 1,
 'item_scraped_count': 12,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 139407360,
 'memusage/startup': 139407360,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 24, 19, 58, 30, 778672, tzinfo=datetime.timezone.utc)}
2025-02-25 00:59:02 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 01:00:14 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 01:00:14 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 01:00:14 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 01:00:14 [scrapy.extensions.telnet] INFO: Telnet Password: ddb0e1e096d7e6b2
2025-02-25 01:00:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 01:00:14 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 01:00:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 01:00:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 01:00:14 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 01:00:14 [scrapy.core.engine] INFO: Spider opened
2025-02-25 01:00:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 01:00:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 01:00:17 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 01:00:50 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 01:00:50 [scrapy.extensions.feedexport] INFO: Stored json feed (15 items) in: data/linkedin_post_2025-02-24T20-00-14+00-00.json
2025-02-25 01:00:50 [scrapy.extensions.feedexport] INFO: Stored csv feed (15 items) in: data/linkedin_post_2025-02-24T20-00-14+00-00.csv
2025-02-25 01:00:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 455,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 159631,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 36.636888,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 24, 20, 0, 50, 773422, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1410535,
 'httpcompression/response_count': 1,
 'item_scraped_count': 15,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 140062720,
 'memusage/startup': 140062720,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 24, 20, 0, 14, 136534, tzinfo=datetime.timezone.utc)}
2025-02-25 01:00:50 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 01:01:27 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 01:01:27 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 01:01:27 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 01:01:27 [scrapy.extensions.telnet] INFO: Telnet Password: 4bc3679cf4d2c573
2025-02-25 01:01:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 01:01:27 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 01:01:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 01:01:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 01:01:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 01:01:27 [scrapy.core.engine] INFO: Spider opened
2025-02-25 01:01:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 01:01:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 01:01:33 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 01:02:16 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 01:02:16 [scrapy.extensions.feedexport] INFO: Stored json feed (21 items) in: data/linkedin_post_2025-02-24T20-01-27+00-00.json
2025-02-25 01:02:16 [scrapy.extensions.feedexport] INFO: Stored csv feed (21 items) in: data/linkedin_post_2025-02-24T20-01-27+00-00.csv
2025-02-25 01:02:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 455,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 159791,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 49.066921,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 24, 20, 2, 16, 173435, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1410583,
 'httpcompression/response_count': 1,
 'item_scraped_count': 21,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 140062720,
 'memusage/startup': 140062720,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 24, 20, 1, 27, 106514, tzinfo=datetime.timezone.utc)}
2025-02-25 01:02:16 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 01:21:38 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 01:21:38 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 01:21:38 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 01:21:38 [scrapy.extensions.telnet] INFO: Telnet Password: a6db9bffab74bec6
2025-02-25 01:21:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 01:21:38 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 01:21:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 01:21:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 01:21:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 01:21:38 [scrapy.core.engine] INFO: Spider opened
2025-02-25 01:21:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 01:21:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 01:21:42 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 01:22:01 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-25 01:22:53 [linkedin_job] INFO: Total unique job URLs collected so far: 22
2025-02-25 01:22:53 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 01:22:53 [linkedin_job] INFO: ✅ Final total job URLs: 22
2025-02-25 01:22:58 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 01:24:02 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 1 pages/min), scraped 1 items (at 1 items/min)
2025-02-25 01:24:10 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 01:25:06 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 2 pages/min), scraped 2 items (at 1 items/min)
2025-02-25 01:25:14 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 01:26:21 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 0 pages/min), scraped 3 items (at 1 items/min)
2025-02-25 01:26:29 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 01:27:34 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 2 pages/min), scraped 4 items (at 1 items/min)
2025-02-25 01:27:39 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 01:28:46 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-25 01:28:47 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x77edd4c6bc20>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/56dba7a3f06e54463f582e101a6be6f0/element
2025-02-25 01:28:47 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x77edd4c90890>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/56dba7a3f06e54463f582e101a6be6f0/element
2025-02-25 01:28:47 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x77edd4c908f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/56dba7a3f06e54463f582e101a6be6f0/element
2025-02-25 01:28:47 [linkedin_job] INFO: Timeout: No job title found.
2025-02-25 01:28:51 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x77edd4c90a70>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/56dba7a3f06e54463f582e101a6be6f0/source
2025-02-25 01:28:51 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x77edd4c693d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/56dba7a3f06e54463f582e101a6be6f0/source
2025-02-25 01:28:51 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x77edd4c68740>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/56dba7a3f06e54463f582e101a6be6f0/source
2025-02-25 01:28:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4162243554/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=8xxCq%2FsGYcOO7E922RRovA%3D%3D&trackingId=ykvlGUps3k4e0aI5aMctlg%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?keywords=Python%20Developer%2C%20%20Django&location=United%20States&f_TPR=r86400&f_WT=2)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x77edd4c905f0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 245, in parse_job_page
    job_page_html = driver.page_source
                    ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 806, in __getattribute__
    return super().__getattribute__(item)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 570, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 135, in request
    return self.request_encode_url(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 182, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=45297): Max retries exceeded with url: /session/56dba7a3f06e54463f582e101a6be6f0/source (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x77edd4c905f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-25 01:28:51 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 1 pages/min), scraped 4 items (at 0 items/min)
2025-02-25 01:28:51 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-25 01:28:53 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 01:28:54 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-25 01:28:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4162644961/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=8xxCq%2FsGYcOO7E922RRovA%3D%3D&trackingId=kASaKj9DQT88wiyC38a0Ww%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?keywords=Python%20Developer%2C%20%20Django&location=United%20States&f_TPR=r86400&f_WT=2)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 220, in parse_job_page
    driver = uc.Chrome(options=options)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 466, in __init__
    super(Chrome, self).__init__(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/chrome/webdriver.py", line 45, in __init__
    super().__init__(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/chromium/webdriver.py", line 66, in __init__
    super().__init__(command_executor=executor, options=options)
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 250, in __init__
    self.start_session(capabilities)
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 724, in start_session
    super(selenium.webdriver.chrome.webdriver.WebDriver, self).start_session(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 342, in start_session
    response = self.execute(Command.NEW_SESSION, caps)["value"]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/util.py", line 38, in reraise
    raise value.with_traceback(tb)
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2025-02-25 01:28:55 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://www.linkedin.com/jobs/search/?alertAction=viewjobs&currentJobId=4167091538&eBP=NON_CHARGEABLE_CHANNEL&f_TPR=r86400&f_WT=2&keywords=Python%20Developer%2C%20%20Django&location=United%20States&origin=JOBS_HOME_JOB_ALERTS&refId=In5XXQobqhy9cFxwxcFx4A%3D%3D&savedSearchId=7657711378&trackingId=fPtMQVdBLcaGwfTSdVVCCw%3D%3D. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2025-02-25 01:43:40 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 01:43:40 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 01:43:40 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 01:43:40 [scrapy.extensions.telnet] INFO: Telnet Password: 1895a06d233549e0
2025-02-25 01:43:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 01:43:40 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 01:43:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 01:43:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 01:43:40 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 01:43:40 [scrapy.core.engine] INFO: Spider opened
2025-02-25 01:43:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 01:43:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 01:43:44 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 01:44:07 [linkedin_job] INFO: Extracting job listings from page 1
2025-02-25 01:44:35 [linkedin_job] INFO: Total unique job URLs collected so far: 10
2025-02-25 01:44:35 [linkedin_job] INFO: ✅ Final total job URLs: 10
2025-02-25 01:44:39 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 01:44:44 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-25 01:44:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4160790383/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=742nBEtiSdt9UM1mpl19CQ%3D%3D&trackingId=zaMwpWfWyU8BOrYNfucQuw%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?keywords=Python%20Developer%2C%20%20Django&location=United%20States&f_TPR=r86400&f_E=2&f_WT=2)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 226, in parse_job_page
    driver.add_cookie({"name": name, "value": value})
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 767, in add_cookie
    self.execute(Command.ADD_COOKIE, {"cookie": cookie_dict})
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/util.py", line 38, in reraise
    raise value.with_traceback(tb)
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2025-02-25 01:44:44 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 01:44:44 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-02-25 01:44:47 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 01:44:57 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-25 01:44:58 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79f8e68dfc80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/73f9e9242a8df5d04cb33aa47072dff0/url
2025-02-25 01:44:58 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79f8e68dfaa0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/73f9e9242a8df5d04cb33aa47072dff0/url
2025-02-25 01:44:58 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79f8e68ddfd0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/73f9e9242a8df5d04cb33aa47072dff0/url
2025-02-25 01:44:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.linkedin.com/jobs/view/4162269495/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=742nBEtiSdt9UM1mpl19CQ%3D%3D&trackingId=1Z2AgbnT%2BKGsVxSeI4RVUQ%3D%3D&trk=flagship3_search_srp_jobs> (referer: https://www.linkedin.com/jobs/search/?keywords=Python%20Developer%2C%20%20Django&location=United%20States&f_TPR=r86400&f_E=2&f_WT=2)
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x79f8e68ddd60>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/defer.py", line 327, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/utils/python.py", line 368, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/referer.py", line 379, in <genexpr>
    return (self._set_referer(r, response) for r in result)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/urllength.py", line 57, in <genexpr>
    return (r for r in result if self._filter(r, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result if self._filter(r, response, spider))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/spidermw.py", line 106, in process_sync
    yield from iterable
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/linkedin_jobs.py", line 230, in parse_job_page
    driver.get(response.url)
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 665, in get
    return super().get(url)
           ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 454, in get
    self.execute(Command.GET, {"url": url})
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=54189): Max retries exceeded with url: /session/73f9e9242a8df5d04cb33aa47072dff0/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79f8e68ddd60>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-25 18:22:24 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 18:22:24 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 18:22:24 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 18:22:24 [scrapy.extensions.telnet] INFO: Telnet Password: 8bb32714be8f09de
2025-02-25 18:22:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 18:22:24 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 18:22:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 18:22:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 18:22:24 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 18:22:24 [scrapy.core.engine] INFO: Spider opened
2025-02-25 18:22:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 18:22:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 18:22:27 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 18:22:46 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 18:22:56 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-25 18:22:56 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 53, in start_requests
    yield from self.parse(response)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 67, in parse
    driver.get(response.url)
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/undetected_chromedriver/__init__.py", line 665, in get
    return super().get(url)
           ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 454, in get
    self.execute(Command.GET, {"url": url})
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/util.py", line 38, in reraise
    raise value.with_traceback(tb)
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2025-02-25 18:22:56 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 18:22:56 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-25T13-22-24+00-00.json
2025-02-25 18:22:56 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-25T13-22-24+00-00.csv
2025-02-25 18:22:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 31.66507,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 25, 13, 22, 56, 213017, tzinfo=datetime.timezone.utc),
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 15,
 'memusage/max': 131887104,
 'memusage/startup': 131887104,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 25, 13, 22, 24, 547947, tzinfo=datetime.timezone.utc)}
2025-02-25 18:22:56 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 18:32:49 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 18:32:49 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 18:32:49 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 18:32:49 [scrapy.extensions.telnet] INFO: Telnet Password: d0c5bcc12c12d546
2025-02-25 18:32:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 18:32:49 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 18:32:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 18:32:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 18:32:49 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 18:32:49 [scrapy.core.engine] INFO: Spider opened
2025-02-25 18:32:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 18:32:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 18:32:52 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 18:33:00 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 18:33:06 [google_jobs] INFO: No location not now button
2025-02-25 18:33:10 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-25 18:33:10 [google_jobs] INFO: No load more button
2025-02-25 18:33:15 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x757781f7d880>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/6c2914ed44ab8cabcb5ee4100c2760c8/execute/sync
2025-02-25 18:33:15 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x757781f7d280>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/6c2914ed44ab8cabcb5ee4100c2760c8/execute/sync
2025-02-25 18:33:15 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x757781f7d4f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/6c2914ed44ab8cabcb5ee4100c2760c8/execute/sync
2025-02-25 18:33:15 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x757781f7e870>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 53, in start_requests
    yield from self.parse(response)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 86, in parse
    self.scroll_to_load_jobs(driver, max_scrolls=self.SCROLLS)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 135, in scroll_to_load_jobs
    driver.execute_script("window.scrollBy(0, 1000);")
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=42589): Max retries exceeded with url: /session/6c2914ed44ab8cabcb5ee4100c2760c8/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x757781f7e870>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-25 18:33:15 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 18:33:15 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-25T13-32-49+00-00.json
2025-02-25 18:33:15 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-25T13-32-49+00-00.csv
2025-02-25 18:33:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 25.964397,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 25, 13, 33, 15, 56762, tzinfo=datetime.timezone.utc),
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 17,
 'log_count/WARNING': 3,
 'memusage/max': 132792320,
 'memusage/startup': 132792320,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 25, 13, 32, 49, 92365, tzinfo=datetime.timezone.utc)}
2025-02-25 18:33:15 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 18:37:08 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 18:37:08 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 18:37:08 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 18:37:08 [scrapy.extensions.telnet] INFO: Telnet Password: 042d7c88eadb5b4f
2025-02-25 18:37:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 18:37:08 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 18:37:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 18:37:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 18:37:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 18:37:08 [scrapy.core.engine] INFO: Spider opened
2025-02-25 18:37:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 18:37:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 18:37:11 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 18:37:20 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 18:37:27 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-25 18:37:27 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79c17067a600>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/a845e0f8f825213cc6b89bbfaea128ee/element
2025-02-25 18:37:27 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79c17067a180>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/a845e0f8f825213cc6b89bbfaea128ee/element
2025-02-25 18:37:27 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79c170678e30>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/a845e0f8f825213cc6b89bbfaea128ee/element
2025-02-25 18:37:27 [google_jobs] INFO: No location not now button
2025-02-25 18:37:30 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79c170679670>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/a845e0f8f825213cc6b89bbfaea128ee/element
2025-02-25 18:37:30 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79c17067ab40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/a845e0f8f825213cc6b89bbfaea128ee/element
2025-02-25 18:37:30 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79c17067adb0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/a845e0f8f825213cc6b89bbfaea128ee/element
2025-02-25 18:37:30 [google_jobs] INFO: No load more button
2025-02-25 18:37:32 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-25 18:37:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79c17067b440>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/a845e0f8f825213cc6b89bbfaea128ee/execute/sync
2025-02-25 18:37:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79c17067b620>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/a845e0f8f825213cc6b89bbfaea128ee/execute/sync
2025-02-25 18:37:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79c17067b860>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/a845e0f8f825213cc6b89bbfaea128ee/execute/sync
2025-02-25 18:37:33 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x79c1706797c0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 53, in start_requests
    yield from self.parse(response)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 86, in parse
    self.scroll_to_load_jobs(driver, max_scrolls=self.SCROLLS)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 135, in scroll_to_load_jobs
    driver.execute_script("window.scrollBy(0, 1000);")
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 528, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=50137): Max retries exceeded with url: /session/a845e0f8f825213cc6b89bbfaea128ee/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79c1706797c0>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-25 18:37:33 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 18:37:33 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-25T13-37-08+00-00.json
2025-02-25 18:37:33 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-25T13-37-08+00-00.csv
2025-02-25 18:43:03 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 18:43:03 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 18:43:03 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 18:43:03 [scrapy.extensions.telnet] INFO: Telnet Password: 7459234aea404ed8
2025-02-25 18:43:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 18:43:03 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 18:43:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 18:43:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 18:43:03 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 18:43:03 [scrapy.core.engine] INFO: Spider opened
2025-02-25 18:43:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 18:43:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 18:43:06 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 18:43:17 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 18:43:21 [google_jobs] INFO: No location not now button
2025-02-25 18:43:32 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-25 18:43:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76d5998860c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/90ebfbf46bd6a4fb09c4feb2d4e5a5e2/element
2025-02-25 18:43:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76d599885100>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/90ebfbf46bd6a4fb09c4feb2d4e5a5e2/element
2025-02-25 18:43:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76d599884fe0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/90ebfbf46bd6a4fb09c4feb2d4e5a5e2/element
2025-02-25 18:43:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76d5998866c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/90ebfbf46bd6a4fb09c4feb2d4e5a5e2/element
2025-02-25 18:43:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76d599886900>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/90ebfbf46bd6a4fb09c4feb2d4e5a5e2/element
2025-02-25 18:43:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76d599886b40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/90ebfbf46bd6a4fb09c4feb2d4e5a5e2/element
2025-02-25 18:43:33 [google_jobs] INFO: Timeout: No jobs found within 15 seconds.
2025-02-25 18:43:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76d5998870e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/90ebfbf46bd6a4fb09c4feb2d4e5a5e2/elements
2025-02-25 18:43:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76d5998872c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/90ebfbf46bd6a4fb09c4feb2d4e5a5e2/elements
2025-02-25 18:43:33 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76d599887500>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/90ebfbf46bd6a4fb09c4feb2d4e5a5e2/elements
2025-02-25 18:43:33 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x76d599886630>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 53, in start_requests
    yield from self.parse(response)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 94, in parse
    job_items = driver.find_elements(By.XPATH, '//a[@class="MQUd2b"]')
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 926, in find_elements
    return self.execute(Command.FIND_ELEMENTS, {"using": by, "value": value})["value"] or []
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=60905): Max retries exceeded with url: /session/90ebfbf46bd6a4fb09c4feb2d4e5a5e2/elements (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76d599886630>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-25 18:43:33 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 18:43:33 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-25T13-43-03+00-00.json
2025-02-25 18:43:33 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-25T13-43-03+00-00.csv
2025-02-25 18:43:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 30.140339,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 25, 13, 43, 33, 602867, tzinfo=datetime.timezone.utc),
 'items_per_minute': None,
 'log_count/ERROR': 1,
 'log_count/INFO': 17,
 'log_count/WARNING': 9,
 'memusage/max': 131616768,
 'memusage/startup': 131616768,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 25, 13, 43, 3, 462528, tzinfo=datetime.timezone.utc)}
2025-02-25 18:43:33 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 18:45:53 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 18:45:53 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 18:45:53 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 18:45:53 [scrapy.extensions.telnet] INFO: Telnet Password: 9bcec9a6d44a26ca
2025-02-25 18:45:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 18:45:53 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 18:45:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 18:45:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 18:45:53 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 18:45:53 [scrapy.core.engine] INFO: Spider opened
2025-02-25 18:45:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 18:45:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 18:45:55 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 18:46:05 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 18:46:11 [google_jobs] INFO: No location not now button
2025-02-25 18:46:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 7 items (at 7 items/min)
2025-02-25 18:47:08 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 18:47:08 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-25T13-45-53+00-00.json
2025-02-25 18:47:08 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-25T13-45-53+00-00.csv
2025-02-25 18:47:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 75.366833,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 25, 13, 47, 8, 460690, tzinfo=datetime.timezone.utc),
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 16,
 'memusage/max': 131948544,
 'memusage/startup': 131948544,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 25, 13, 45, 53, 93857, tzinfo=datetime.timezone.utc)}
2025-02-25 18:47:08 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 18:50:11 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 18:50:11 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 18:50:11 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 18:50:11 [scrapy.extensions.telnet] INFO: Telnet Password: fe23ada7305653ef
2025-02-25 18:50:11 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 18:50:11 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 18:50:11 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 18:50:11 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 18:50:11 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 18:50:11 [scrapy.core.engine] INFO: Spider opened
2025-02-25 18:50:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 18:50:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 18:50:14 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 18:50:16 [scrapy.crawler] INFO: Received SIGTERM, shutting down gracefully. Send again to force 
2025-02-25 18:50:22 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 18:50:27 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2025-02-25 18:50:28 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72c0cb07a180>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/cc1e9ca3c35b41e1a8b65c9ef0f28773/element
2025-02-25 18:50:28 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72c0cb0795b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/cc1e9ca3c35b41e1a8b65c9ef0f28773/element
2025-02-25 18:50:28 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72c0cb079970>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/cc1e9ca3c35b41e1a8b65c9ef0f28773/element
2025-02-25 18:50:28 [google_jobs] INFO: No location not now button
2025-02-25 18:50:32 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72c0cb079400>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/cc1e9ca3c35b41e1a8b65c9ef0f28773/element
2025-02-25 18:50:32 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72c0cb07a8d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/cc1e9ca3c35b41e1a8b65c9ef0f28773/element
2025-02-25 18:50:32 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72c0cb07ab40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/cc1e9ca3c35b41e1a8b65c9ef0f28773/element
2025-02-25 18:50:32 [google_jobs] INFO: No load more button
2025-02-25 18:50:36 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72c0cb07b110>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/cc1e9ca3c35b41e1a8b65c9ef0f28773/element
2025-02-25 18:50:36 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72c0cb07b350>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/cc1e9ca3c35b41e1a8b65c9ef0f28773/element
2025-02-25 18:50:36 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72c0cb07b590>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/cc1e9ca3c35b41e1a8b65c9ef0f28773/element
2025-02-25 18:50:36 [google_jobs] INFO: Timeout: No jobs found within 15 seconds.
2025-02-25 18:50:36 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72c0cb07ad50>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/cc1e9ca3c35b41e1a8b65c9ef0f28773/elements
2025-02-25 18:50:36 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72c0cb07aa20>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/cc1e9ca3c35b41e1a8b65c9ef0f28773/elements
2025-02-25 18:50:36 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72c0cb079400>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/cc1e9ca3c35b41e1a8b65c9ef0f28773/elements
2025-02-25 18:50:36 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 493, in _make_request
    conn.request(
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 445, in request
    self.endheaders()
  File "/usr/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 276, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x72c0cb079a30>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/scrapy/core/engine.py", line 185, in _next_request
    request_or_item = next(self.slot.start_requests)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 53, in start_requests
    yield from self.parse(response)
  File "/home/user/scrapers/linkedin_scraper/linkedin_scraper/linkedin_scraper/spiders/google_jobs.py", line 94, in parse
    job_items = driver.find_elements(By.XPATH, '//a[@class="MQUd2b"]')
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 926, in find_elements
    return self.execute(Command.FIND_ELEMENTS, {"using": by, "value": value})["value"] or []
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 404, in execute
    return self._request(command_info[0], url, body=data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/selenium/webdriver/remote/remote_connection.py", line 428, in _request
    response = self._conn.request(method, url, body=body, headers=headers, timeout=self._client_config.timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 143, in request
    return self.request_encode_body(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/_request_methods.py", line 278, in request_encode_body
    return self.urlopen(method, url, **extra_kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/poolmanager.py", line 443, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 871, in urlopen
    return self.urlopen(
           ^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/user/scrapers/linkedin_scraper/venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=37985): Max retries exceeded with url: /session/cc1e9ca3c35b41e1a8b65c9ef0f28773/elements (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72c0cb079a30>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-25 18:50:36 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 18:50:36 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-25T13-50-11+00-00.json
2025-02-25 18:50:36 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-25T13-50-11+00-00.csv
2025-02-25 19:12:29 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 19:12:29 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 19:12:29 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 19:12:29 [scrapy.extensions.telnet] INFO: Telnet Password: 092a85e13ca9c21b
2025-02-25 19:12:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 19:12:29 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 19:12:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 19:12:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 19:12:29 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 19:12:29 [scrapy.core.engine] INFO: Spider opened
2025-02-25 19:12:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 19:12:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 19:12:33 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 19:12:41 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 19:12:47 [google_jobs] INFO: No location not now button
2025-02-25 19:13:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 7 items (at 7 items/min)
2025-02-25 19:13:35 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-02-25 19:13:36 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79ae69c76930>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e5eda4ca7e7d823a7584b957fb00390f/element
2025-02-25 19:13:36 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79ae69c763c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e5eda4ca7e7d823a7584b957fb00390f/element
2025-02-25 19:13:36 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79ae69c772f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e5eda4ca7e7d823a7584b957fb00390f/element
2025-02-25 19:13:36 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=40457): Max retries exceeded with url: /session/e5eda4ca7e7d823a7584b957fb00390f/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79ae69c77530>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-25 19:13:36 [urllib3.connectionpool] WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79ae69c77860>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e5eda4ca7e7d823a7584b957fb00390f/elements
2025-02-25 19:13:36 [urllib3.connectionpool] WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79ae69c77a40>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e5eda4ca7e7d823a7584b957fb00390f/elements
2025-02-25 19:13:36 [urllib3.connectionpool] WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79ae69c77c80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e5eda4ca7e7d823a7584b957fb00390f/elements
2025-02-25 19:13:36 [google_jobs] ERROR: Error extracting job details: HTTPConnectionPool(host='localhost', port=40457): Max retries exceeded with url: /session/e5eda4ca7e7d823a7584b957fb00390f/elements (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x79ae69c75a30>: Failed to establish a new connection: [Errno 111] Connection refused'))
2025-02-25 19:13:36 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 19:13:36 [scrapy.extensions.feedexport] INFO: Stored json feed (8 items) in: data/google_jobs_2025-02-25T14-12-29+00-00.json
2025-02-25 19:13:36 [scrapy.extensions.feedexport] INFO: Stored csv feed (8 items) in: data/google_jobs_2025-02-25T14-12-29+00-00.csv
2025-02-25 19:13:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 67.520862,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 25, 14, 13, 36, 875222, tzinfo=datetime.timezone.utc),
 'item_scraped_count': 8,
 'items_per_minute': None,
 'log_count/ERROR': 2,
 'log_count/INFO': 17,
 'log_count/WARNING': 6,
 'memusage/max': 132374528,
 'memusage/startup': 132374528,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 25, 14, 12, 29, 354360, tzinfo=datetime.timezone.utc)}
2025-02-25 19:13:36 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 19:22:22 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 19:22:22 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 19:22:22 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 19:22:22 [scrapy.extensions.telnet] INFO: Telnet Password: 93207ed5019d544f
2025-02-25 19:22:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 19:22:22 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 19:22:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 19:22:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 19:22:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 19:22:22 [scrapy.core.engine] INFO: Spider opened
2025-02-25 19:22:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 19:22:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 19:22:24 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 19:22:31 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 19:22:36 [google_jobs] INFO: No location not now button
2025-02-25 19:22:44 [google_jobs] INFO: Stop flag detected during scrolling. Stopping scraper.
2025-02-25 19:22:45 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 19:22:45 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-25T14-22-22+00-00.json
2025-02-25 19:22:45 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-25T14-22-22+00-00.csv
2025-02-25 19:22:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 22.819534,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 25, 14, 22, 45, 154189, tzinfo=datetime.timezone.utc),
 'items_per_minute': None,
 'log_count/INFO': 16,
 'memusage/max': 136167424,
 'memusage/startup': 136167424,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 25, 14, 22, 22, 334655, tzinfo=datetime.timezone.utc)}
2025-02-25 19:22:45 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 19:26:04 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 19:26:04 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 19:26:04 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 19:26:04 [scrapy.extensions.telnet] INFO: Telnet Password: b3f158e18e28f878
2025-02-25 19:26:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 19:26:04 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 19:26:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 19:26:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 19:26:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 19:26:04 [scrapy.core.engine] INFO: Spider opened
2025-02-25 19:26:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 19:26:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 19:26:07 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 19:26:15 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 19:26:20 [google_jobs] INFO: No location not now button
2025-02-25 19:27:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 8 items (at 8 items/min)
2025-02-25 19:27:14 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 19:27:14 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-25T14-26-04+00-00.json
2025-02-25 19:27:14 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-25T14-26-04+00-00.csv
2025-02-25 19:27:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 70.349388,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 25, 14, 27, 14, 570163, tzinfo=datetime.timezone.utc),
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 16,
 'memusage/max': 132354048,
 'memusage/startup': 132354048,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 25, 14, 26, 4, 220775, tzinfo=datetime.timezone.utc)}
2025-02-25 19:27:14 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 19:30:31 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 19:30:31 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 19:30:31 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 19:30:31 [scrapy.extensions.telnet] INFO: Telnet Password: 3f8a239881c5da79
2025-02-25 19:30:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 19:30:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 19:30:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 19:30:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 19:30:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 19:30:31 [scrapy.core.engine] INFO: Spider opened
2025-02-25 19:30:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 19:30:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 19:30:35 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 19:30:43 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 19:30:50 [google_jobs] INFO: No location not now button
2025-02-25 19:31:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 7 items (at 7 items/min)
2025-02-25 19:31:46 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 19:31:46 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-25T14-30-31+00-00.json
2025-02-25 19:31:46 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-25T14-30-31+00-00.csv
2025-02-25 19:31:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 75.348573,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 25, 14, 31, 46, 970915, tzinfo=datetime.timezone.utc),
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 16,
 'memusage/max': 132059136,
 'memusage/startup': 132059136,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 25, 14, 30, 31, 622342, tzinfo=datetime.timezone.utc)}
2025-02-25 19:31:46 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 19:36:56 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 19:36:56 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 19:36:56 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 19:36:56 [scrapy.extensions.telnet] INFO: Telnet Password: 7c1870829e0d203a
2025-02-25 19:36:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 19:36:56 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 19:36:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 19:36:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 19:36:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 19:36:56 [scrapy.core.engine] INFO: Spider opened
2025-02-25 19:36:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 19:36:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 19:36:56 [google_jobs] INFO: Stop flag detected. Stopping scraper.
2025-02-25 19:36:56 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 19:36:56 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: data/google_jobs_2025-02-25T14-36-56+00-00.json
2025-02-25 19:36:56 [scrapy.extensions.feedexport] INFO: Stored csv feed (0 items) in: data/google_jobs_2025-02-25T14-36-56+00-00.csv
2025-02-25 19:36:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 0.002282,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 25, 14, 36, 56, 928531, tzinfo=datetime.timezone.utc),
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 132182016,
 'memusage/startup': 132182016,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 25, 14, 36, 56, 926249, tzinfo=datetime.timezone.utc)}
2025-02-25 19:36:56 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 19:41:00 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 19:41:00 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 19:41:00 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 19:41:00 [scrapy.extensions.telnet] INFO: Telnet Password: f12f957693c007d8
2025-02-25 19:41:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 19:41:00 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 19:41:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 19:41:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 19:41:00 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 19:41:00 [scrapy.core.engine] INFO: Spider opened
2025-02-25 19:41:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 19:41:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 19:41:03 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 19:41:11 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 19:41:16 [google_jobs] INFO: No location not now button
2025-02-25 19:42:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 8 items (at 8 items/min)
2025-02-25 19:42:10 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 19:42:10 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-25T14-41-00+00-00.json
2025-02-25 19:42:10 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-25T14-41-00+00-00.csv
2025-02-25 19:42:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 70.332256,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 25, 14, 42, 10, 552768, tzinfo=datetime.timezone.utc),
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 16,
 'memusage/max': 132411392,
 'memusage/startup': 132411392,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 25, 14, 41, 0, 220512, tzinfo=datetime.timezone.utc)}
2025-02-25 19:42:10 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 19:50:35 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 19:50:35 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 19:50:35 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 19:50:35 [scrapy.extensions.telnet] INFO: Telnet Password: c39e78fcc1cddbac
2025-02-25 19:50:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 19:50:35 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 19:50:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 19:50:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 19:50:35 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 19:50:35 [scrapy.core.engine] INFO: Spider opened
2025-02-25 19:50:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 19:50:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 19:50:40 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 19:51:13 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 19:51:13 [scrapy.extensions.feedexport] INFO: Stored json feed (9 items) in: data/linkedin_post_2025-02-25T14-50-35+00-00.json
2025-02-25 19:51:13 [scrapy.extensions.feedexport] INFO: Stored csv feed (9 items) in: data/linkedin_post_2025-02-25T14-50-35+00-00.csv
2025-02-25 19:51:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 456,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 164334,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 38.32127,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 25, 14, 51, 13, 408852, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1439174,
 'httpcompression/response_count': 1,
 'item_scraped_count': 9,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 135950336,
 'memusage/startup': 135950336,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 25, 14, 50, 35, 87582, tzinfo=datetime.timezone.utc)}
2025-02-25 19:51:13 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 19:52:48 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 19:52:48 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 19:52:48 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 19:52:48 [scrapy.extensions.telnet] INFO: Telnet Password: 3989b0e92787f361
2025-02-25 19:52:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 19:52:48 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 19:52:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 19:52:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 19:52:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 19:52:48 [scrapy.core.engine] INFO: Spider opened
2025-02-25 19:52:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 19:52:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 19:52:53 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 19:53:25 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 19:53:25 [scrapy.extensions.feedexport] INFO: Stored json feed (12 items) in: data/linkedin_post_2025-02-25T14-52-48+00-00.json
2025-02-25 19:53:25 [scrapy.extensions.feedexport] INFO: Stored csv feed (12 items) in: data/linkedin_post_2025-02-25T14-52-48+00-00.csv
2025-02-25 19:53:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 483,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 161807,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 36.699939,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 25, 14, 53, 25, 291309, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1446779,
 'httpcompression/response_count': 1,
 'item_scraped_count': 12,
 'items_per_minute': None,
 'log_count/INFO': 13,
 'memusage/max': 135950336,
 'memusage/startup': 135950336,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 2, 25, 14, 52, 48, 591370, tzinfo=datetime.timezone.utc)}
2025-02-25 19:53:25 [scrapy.core.engine] INFO: Spider closed (finished)
2025-02-25 20:31:49 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: linkedin_scraper)
2025-02-25 20:31:49 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Linux-6.11.0-17-generic-x86_64-with-glibc2.39
2025-02-25 20:31:49 [scrapy.addons] INFO: Enabled addons:
[]
2025-02-25 20:31:49 [scrapy.extensions.telnet] INFO: Telnet Password: 36e72b89399bd8f4
2025-02-25 20:31:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2025-02-25 20:31:49 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'linkedin_scraper',
 'CONCURRENT_REQUESTS': 10,
 'DOWNLOAD_DELAY': 3,
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy_logs.txt',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'linkedin_scraper.spiders',
 'RETRY_HTTP_CODES': [429, 500, 502, 503, 504, 522, 524, 408, 403],
 'RETRY_TIMES': 5,
 'SPIDER_MODULES': ['linkedin_scraper.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
 'USER_AGENT': None}
2025-02-25 20:31:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'linkedin_scraper.middlewares.CustomRetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-02-25 20:31:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-02-25 20:31:49 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-02-25 20:31:49 [scrapy.core.engine] INFO: Spider opened
2025-02-25 20:31:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-02-25 20:31:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-02-25 20:31:52 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 20:32:01 [undetected_chromedriver.patcher] INFO: patching driver executable /home/user/.local/share/undetected_chromedriver/undetected_chromedriver
2025-02-25 20:32:08 [google_jobs] INFO: No location not now button
2025-02-25 20:32:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 7 items (at 7 items/min)
2025-02-25 20:33:05 [scrapy.core.engine] INFO: Closing spider (finished)
2025-02-25 20:33:05 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: data/google_jobs_2025-02-25T15-31-49+00-00.json
2025-02-25 20:33:05 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: data/google_jobs_2025-02-25T15-31-49+00-00.csv
2025-02-25 20:33:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'elapsed_time_seconds': 75.587673,
 'feedexport/success_count/FileFeedStorage': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 2, 25, 15, 33, 5, 165862, tzinfo=datetime.timezone.utc),
 'item_scraped_count': 10,
 'items_per_minute': None,
 'log_count/INFO': 16,
 'memusage/max': 132222976,
 'memusage/startup': 132222976,
 'responses_per_minute': None,
 'start_time': datetime.datetime(2025, 2, 25, 15, 31, 49, 578189, tzinfo=datetime.timezone.utc)}
2025-02-25 20:33:05 [scrapy.core.engine] INFO: Spider closed (finished)
